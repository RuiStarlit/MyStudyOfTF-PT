{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import glob\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "from itertools import islice\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# import tensorflow as tf\n",
    "# import keras.backend as K\n",
    "\n",
    "# from tensorflow.keras.layers import Dense, Lambda, Dot, Activation, Concatenate\n",
    "# from tensorflow.keras.layers import Layer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NTHREADS = 8\n",
    "SEED = 42\n",
    "TRAIN_BATCH_SIZE = 256\n",
    "TEST_BATCH_SIZE = 256\n",
    "BUCKET_WINDOWS2 = [(0, 100), (100, 200), (200, 300), (300, 400), (400, 500), (500, 600)]\n",
    "\n",
    "DATA_PATH = 'input/optiver-realized-volatility-prediction'\n",
    "BOOK_TRAIN_PATH = 'input/optiver-realized-volatility-prediction/book_train.parquet'\n",
    "TRADE_TRAIN_PATH = 'input/optiver-realized-volatility-prediction/trade_train.parquet'\n",
    "BOOK_TEST_PATH = 'input/optiver-realized-volatility-prediction/book_test.parquet'\n",
    "TRADE_TEST_PATH = 'input/optiver-realized-volatility-prediction/trade_test.parquet'\n",
    "CHECKPOINT = 'model_checkpoint/model_01'\n",
    "\n",
    "book_skip_columns = trade_skip_columns = ['time_id', 'row_id', 'target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import IntProgress\n",
    "def get_path_dict(f, v):\n",
    "\n",
    "    f_dict = {}\n",
    "    for i in tqdm(v):\n",
    "        fpath = f'{f}/stock_id={i}'\n",
    "        flist = glob.glob(os.path.join(fpath, '*.parquet'))\n",
    "    \n",
    "        if len(flist) > 0:\n",
    "            f_dict[i] = flist[0]\n",
    "    \n",
    "    return f_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ds shape: (428932, 3)\n",
      "Test ds shape: (3, 3)\n"
     ]
    }
   ],
   "source": [
    "train_ds = pd.read_csv(os.path.join(DATA_PATH, 'train.csv'))\n",
    "test_ds = pd.read_csv(os.path.join(DATA_PATH, 'test.csv'))\n",
    "print(f'Train ds shape: {train_ds.shape}')\n",
    "print(f'Test ds shape: {test_ds.shape}')\n",
    "train_ds['row_id'] = train_ds['stock_id'].astype(str) + '-' + train_ds['time_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6588371ba1734bb1a3cb48fc4240c75e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70b09b8c07d441a6959faca9f0f22666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39a7c1bcd939485db82f1bb4fcad0654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a80e6abe7a84a38be76f7b46767ee6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "book_train_dict = get_path_dict(BOOK_TRAIN_PATH, train_ds['stock_id'].unique())\n",
    "trade_train_dict = get_path_dict(TRADE_TRAIN_PATH, train_ds['stock_id'].unique())\n",
    "\n",
    "book_test_dict = get_path_dict(BOOK_TEST_PATH, test_ds['stock_id'].unique())\n",
    "trade_test_dict = get_path_dict(TRADE_TEST_PATH, test_ds['stock_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_wap1(df):\n",
    "    # Function to calculate first WAP\n",
    "    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "    return wap\n",
    "\n",
    "def calc_wap2(df):\n",
    "    # Function to calculate second WAP\n",
    "    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "    return wap\n",
    "\n",
    "def calc_wap3(df):\n",
    "    wap = (df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "    return wap\n",
    "\n",
    "def calc_wap4(df):\n",
    "    wap = (df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "    return wap\n",
    "\n",
    "def calc_ma(df, colname, window_size):\n",
    "    a = df[colname].rolling(window=window_size).mean()\n",
    "    return a\n",
    "\n",
    "def calc_mstd(df, colname, window_size):\n",
    "    a = df[colname].rolling(window=window_size).std()\n",
    "    return a\n",
    "\n",
    "def calc_memw(df, colname, window_size):\n",
    "    a = df[colname].ewm(span=10).mean()\n",
    "    return a\n",
    "\n",
    "def log_return(series):\n",
    "    # Function to calculate the log of the return\n",
    "    return np.log(series).diff()\n",
    "\n",
    "def realized_volatility(series):\n",
    "    # Calculate the realized volatility\n",
    "    return np.sqrt(np.sum(series**2))\n",
    "\n",
    "def count_unique(series):\n",
    "    # Function to count unique elements of a series\n",
    "    return len(np.unique(series))\n",
    "\n",
    "def book_ds_fe(df):\n",
    "    # Calculate Wap\n",
    "    df['wap1'] = calc_wap1(df)\n",
    "    df['wap2'] = calc_wap2(df)\n",
    "    \n",
    "    # Calculate log returns\n",
    "    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n",
    "    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n",
    "    \n",
    "    # Calculate wap balance\n",
    "    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n",
    "    \n",
    "    # Calculate spread\n",
    "    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n",
    "    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) / ((df['ask_price2'] + df['bid_price2']) / 2)\n",
    "    \n",
    "    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n",
    "    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n",
    "    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n",
    "    \n",
    "    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n",
    "    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n",
    "    \n",
    "    #####\n",
    "    win_size = 10\n",
    "    bid_price1_ma = calc_ma(df, 'bid_price1', win_size)\n",
    "    ask_size1_ma = calc_ma(df, 'ask_size1', win_size)\n",
    "    ask_price1_ma = calc_ma(df, 'ask_price1', win_size)\n",
    "    bid_size1_ma = calc_ma(df, 'bid_size1', win_size)\n",
    "    \n",
    "    bid_price2_ma = calc_ma(df, 'bid_price2', win_size)\n",
    "    ask_size2_ma = calc_ma(df, 'ask_size2', win_size)\n",
    "    ask_price2_ma = calc_ma(df, 'ask_price2', win_size)\n",
    "    bid_size2_ma = calc_ma(df, 'bid_size2', win_size)\n",
    "    \n",
    "    df['wap1_ma'] = (bid_price1_ma * ask_size1_ma + ask_price1_ma * bid_size1_ma) / (bid_size1_ma + ask_size1_ma)\n",
    "    df['wap2_ma'] = (bid_price2_ma * ask_size2_ma + ask_price2_ma * bid_size2_ma) / (bid_size2_ma + ask_size2_ma)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def trade_ds_fe(df):\n",
    "    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n",
    "    df['amount'] = df['price'] * df['size']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "缺少库sndfile，无法安装。\n",
    "这个特征获取不了\n",
    "import librosa\n",
    "\n",
    "''' MFCC coefficients contain information about the rate changes in the different spectrum bands '''\n",
    "def get_mfcc(a):\n",
    "    r = np.zeros((1, a.shape[1]))\n",
    "    for i in range(a.shape[1]):\n",
    "        mfcc = librosa.feature.mfcc(a[:, i])\n",
    "        mfcc_mean = mfcc.mean(axis=1)\n",
    "        #print(mfcc_mean)\n",
    "        #r[:, i] = np.mean(mfcc_mean)\n",
    "        r[:, i] = mfcc_mean[1]\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom tsfresh.feature_extraction import feature_calculators\\n\\n''' Number of peaks '''\\ndef get_number_peaks(a):\\n    r = np.zeros((1, a.shape[1]))\\n    for i in range(a.shape[1]):\\n        r[:, i] = feature_calculators.number_peaks(a[:, i], 2)\\n    return r\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from tsfresh.feature_extraction import feature_calculators\n",
    "\n",
    "''' Number of peaks '''\n",
    "def get_number_peaks(a):\n",
    "    r = np.zeros((1, a.shape[1]))\n",
    "    for i in range(a.shape[1]):\n",
    "        r[:, i] = feature_calculators.number_peaks(a[:, i], 2)\n",
    "    return r\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_seq_stat(a, s):\n",
    "    ''' a - array, s - seconds_in_bucket'''\n",
    "    \n",
    "    r = []\n",
    "    for w in BUCKET_WINDOWS2:\n",
    "        \n",
    "        idx = np.where(np.logical_and(s >= w[0], s < w[1]))[0]\n",
    "       \n",
    "        s_min = np.zeros((1, a.shape[1]))\n",
    "        s_max = np.zeros((1, a.shape[1]))\n",
    "        s_mean = np.zeros((1, a.shape[1]))\n",
    "        s_std = np.zeros((1, a.shape[1]))\n",
    "        s_median = np.zeros((1, a.shape[1]))\n",
    "        s_sum = np.zeros((1, a.shape[1]))\n",
    "        #s_mfcc = np.zeros((1, a.shape[1]))\n",
    "        s_peaks = np.zeros((1, a.shape[1]))\n",
    "        \n",
    "        if a[idx].shape[0] > 0:\n",
    "            s_min = np.min(a[idx], axis=0, keepdims=True)\n",
    "            s_max = np.max(a[idx], axis=0, keepdims=True)\n",
    "            s_mean = np.mean(a[idx], axis=0, keepdims=True)\n",
    "            s_std = np.std(a[idx], axis=0, keepdims=True)\n",
    "            s_median = np.median(a[idx], axis=0, keepdims=True)\n",
    "            s_sum = np.sum(a[idx], axis=0, keepdims=True)\n",
    "            \n",
    "            s_peaks = get_number_peaks(a[idx]) # <- it gives small boost\n",
    "            #s_mfcc = get_mfcc(a[idx])\n",
    "            \n",
    "        r.append(np.concatenate((s_min, s_max, s_mean, s_std, s_median, s_sum), axis=0))\n",
    "        \n",
    "    return np.nan_to_num(np.concatenate(r, axis=0).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_optiver_ds(ds, f_dict, fe_func, skip_cols, train_flg=True):\n",
    "    \n",
    "    x = []\n",
    "    y = []\n",
    "    \n",
    "    for stock_id, stock_fnmame in tqdm(f_dict.items()):\n",
    "\n",
    "        optiver_ds = pd.read_parquet(stock_fnmame)\n",
    "        optiver_ds['row_id'] = str(stock_id) + '-' + optiver_ds['time_id'].astype(str)\n",
    "\n",
    "        sds = ds[ds['stock_id'] == stock_id]\n",
    "\n",
    "        cols = ['time_id', 'target']\n",
    "        if train_flg == False:\n",
    "            cols = ['time_id']\n",
    "            \n",
    "        merge_ds = pd.merge(sds[cols], optiver_ds, on='time_id', how='left')\n",
    "        merge_ds = fe_func(merge_ds).fillna(0)\n",
    "        \n",
    "        cols = [c for c in merge_ds.columns if c not in skip_cols]\n",
    "\n",
    "        np_ds = merge_ds[cols].to_numpy(dtype=np.float16)\n",
    "        seconds_in_bucket = merge_ds['seconds_in_bucket'].to_numpy()\n",
    "        g_idx = merge_ds[['time_id']].to_numpy()\n",
    "        \n",
    "        l = np.unique(g_idx, return_index=True)[1][1:]        \n",
    "        a_list = np.split(np_ds, l)\n",
    "        s_list = np.split(seconds_in_bucket, l)\n",
    "\n",
    "        stat = list(map(np_seq_stat, a_list, s_list))\n",
    "        b = np.transpose(np.dstack(stat), (2, 1, 0))\n",
    "        b = b.astype(np.float16)\n",
    "        \n",
    "        r = []\n",
    "        if train_flg:\n",
    "            targets = merge_ds[['target']].to_numpy(dtype=np.float16)\n",
    "            t_list = np.split(targets, l)\n",
    "            r = [t[0][0] for t in t_list]\n",
    "        \n",
    "        x.append(b)\n",
    "        y.append(r)\n",
    "        #break\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(data, SIZE=10000):\n",
    "    it = iter(data)\n",
    "    for i in range(0, len(data), SIZE):\n",
    "        yield {k:data[k] for k in islice(it, SIZE)}\n",
    "        \n",
    "def process_book_train_chunk(chunk_ds):\n",
    "    return process_optiver_ds(train_ds, chunk_ds, book_ds_fe, book_skip_columns)\n",
    "def process_trade_train_chunk(chunk_ds):\n",
    "    return process_optiver_ds(train_ds, chunk_ds, trade_ds_fe, trade_skip_columns)\n",
    "def process_book_test_chunk(chunk_ds):\n",
    "    return process_optiver_ds(test_ds, chunk_ds, book_ds_fe, book_skip_columns, False)\n",
    "def process_trade_test_chunk(chunk_ds):\n",
    "    return process_optiver_ds(test_ds, chunk_ds, trade_ds_fe, trade_skip_columns, False)\n",
    "\n",
    "book_train_chunks = [i for i in chunks(book_train_dict, int(len(book_train_dict)/NTHREADS))]\n",
    "trade_train_chunks = [i for i in chunks(trade_train_dict, int(len(trade_train_dict)/NTHREADS))]\n",
    "\n",
    "z = 1 if len(book_test_dict) < NTHREADS else NTHREADS\n",
    "book_test_chunks = [i for i in chunks(book_test_dict, int(len(book_test_dict)/z))]\n",
    "trade_test_chunks = [i for i in chunks(trade_test_dict, int(len(trade_test_dict)/z))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pool = Pool(14)\n",
    "r = pool.map(process_book_train_chunk, book_train_chunks)\n",
    "pool.close()\n",
    "print(1)\n",
    "a1, a2 = zip(*r)\n",
    "np_books = [np.concatenate(a1[i], axis=0) for i in range(len(a1))]\n",
    "np_books = np.concatenate(np_books, axis=0)\n",
    "print(2)\n",
    "targets = [np.concatenate(a2[i], axis=0) for i in range(len(a2))]\n",
    "targets = np.concatenate(targets, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pool = Pool(8)\n",
    "r = pool.map(process_trade_train_chunk, trade_train_chunks)\n",
    "pool.close()\n",
    "\n",
    "a1, _ = zip(*r)\n",
    "np_trades = [np.concatenate(a1[i], axis=0) for i in range(len(a1))]\n",
    "np_trades = np.concatenate(np_trades, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np_books.shape, np_trades.shape, targets.shape)\n",
    "np_train = np.concatenate((np_books, np_trades), axis=2)\n",
    "print(np_train.shape, targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "data_output = open('LSTMtemp/np_train.pkl','wb')\n",
    "pickle.dump(np_train,data_output)\n",
    "data_output.close()\n",
    "\n",
    "data_output = open('LSTMtemp/targets.pkl','wb')\n",
    "pickle.dump(targets,data_output)\n",
    "data_output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "data_input = open('LSTMtemp/np_train.pkl','rb')\n",
    "np_train = pickle.load(data_input)\n",
    "data_input.close()\n",
    "\n",
    "data_input = open('LSTMtemp/targets.pkl','rb')\n",
    "targets = pickle.load(data_input)\n",
    "data_input.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(428932, 36, 29) (428932,)\n"
     ]
    }
   ],
   "source": [
    "print(np_train.shape, targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.arange(np_train.shape[0])\n",
    "train_idx, valid_idx = train_test_split(idx, shuffle=False, test_size=0.1, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5717b6cfdf7f4da7a5551ad1477388a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scaler\n",
    "transformers = []\n",
    "for i in tqdm(range(np_train.shape[1])):\n",
    "    a = np.nan_to_num(np_train[train_idx, i, :])\n",
    "    b = np.nan_to_num(np_train[valid_idx, i, :])\n",
    "\n",
    "    transformer = StandardScaler() # StandardScaler is very useful! 标准化\n",
    "    np_train[train_idx, i, :] = transformer.fit_transform(a)\n",
    "    np_train[valid_idx, i, :] = transformer.transform(b)\n",
    "    transformers.append(transformer) # Save Scalers for the inference stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  对缺失值处理\n",
    "np_train = np.nan_to_num(np_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "def rmspe(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square((y_true - y_pred) / y_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, ds:np.array, targets:np.array):\n",
    "        super().__init__()\n",
    "        self.targets =targets\n",
    "        self.ds = ds\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.ds.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.ds[idx]\n",
    "        y = self.targets[idx]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length=36\n",
    "features = 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_attn(nn.Module):\n",
    "    def __init__(self,batch_size, hidden_size=50, n_layers=1, featrues_dim=29, LSTM_layer = 2):\n",
    "        super(LSTM_attn, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.featrues_dim = featrues_dim\n",
    "        self.dropout = 0\n",
    "        self.batch_size = batch_size\n",
    "        self.LSTM_layer = LSTM_layer\n",
    "\n",
    "        self.n_layers = n_layers\n",
    "        self.lstm1 = nn.LSTM(input_size = featrues_dim,\n",
    "                            hidden_size = self.hidden_size,\n",
    "                            num_layers = self.n_layers,\n",
    "                            )\n",
    "        self.lstm2 = nn.LSTM(input_size = self.hidden_size,\n",
    "                            hidden_size = self.hidden_size,\n",
    "                            num_layers = self.n_layers,\n",
    "                            )\n",
    "\n",
    "#         self.w_omega = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
    "#         self.u_omega = nn.Parameter(torch.Tensor(hidden_size, 1))\n",
    "\n",
    "#         nn.init.uniform_(self.w_omega, -0.1, 0.1)\n",
    "#         nn.init.uniform_(self.u_omega, -0.1, 0.1)\n",
    "#         self.fc1 = nn.Linear(hidden_size + hidden_size*n_layers, 128)\n",
    "#         self.fc2 = nn.Linear(128, 1)\n",
    "    \n",
    "#     def attention_net(self, x):\n",
    "#         # x:[batch, seq_len, hidden_size*2]\n",
    "# #         hidden_states = x\n",
    "# #         hidden_size = int(hidden_states.shape[2])\n",
    "\n",
    "#         u = torch.tanh(torch.matmul(x, self.w_omega))\n",
    "#         # [batch, seq_len, hidden_size*2]\n",
    "#         attn = torch.matmul(u, self.u_omega)\n",
    "#         # [batch, seq_len, 1]\n",
    "#         attn_score = F.softmax(attn, dim=1)\n",
    "\n",
    "#         scored_x = x*attn_score\n",
    "#         # [batch, seq_len, hidden_size*2]\n",
    "\n",
    "#         context = torch.sum(scored_x, dim=1)\n",
    "\n",
    "#         return context\n",
    "    \n",
    "    def forward(self, x):\n",
    "#         assert x.shape==(self.batch_size,seq_length,self.featrues_dim)\n",
    "        x.transpose_(1, 0)\n",
    "#         assert x.shape==(seq_length, self.batch_size, self.featrues_dim)\n",
    "        lstm1, (h_t, final_cell_state) = self.lstm1(x)\n",
    "#         print('lstm1.shape',lstm1.shape)\n",
    "#         lstm1.transpose_(1, 0)\n",
    "#         print('lstm1T.shape',lstm1.shape)\n",
    "#         print(seq_length, self.batch_size, self.featrues_dim)\n",
    "#         print(seq_length, self.batch_size, self.hidden_size)\n",
    "        \n",
    "#         assert lstm1.shape==(seq_length, self.batch_size, self.hidden_size)\n",
    "        \n",
    "        if self.LSTM_layer == 1:\n",
    "            output = lstm1.permute(1, 0 ,2)\n",
    "            attn_output = self.attention_net(output)\n",
    "            logit = self.fc(attn_output)\n",
    "        elif self.LSTM_layer == 2:\n",
    "            lstm2, (h_t, final_cell_state) = self.lstm2(h_t)\n",
    "            output = lstm2.permute(1, 0 ,2)\n",
    "            attn_output = self.attention_net(output)\n",
    "            print('output', attn_output.shape)\n",
    "            h_t = h_t.permute(1, 0 ,2)\n",
    "#             ht2 = torch.cat((h_t[:,0,:], h_t[:,1,:]), 1)\n",
    "#             print('ht2',ht2.shape)\n",
    "#             x = torch.cat((attn_output, ht2),0)\n",
    "#             print('x',x.shape)\n",
    "#             x = self.fc1(x)\n",
    "#             x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_attn(nn.Module):\n",
    "    def __init__(self,batch_size, hidden_size=50, n_layers=1, featrues_dim=29, LSTM_layer = 2):\n",
    "        super(LSTM_attn, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.featrues_dim = featrues_dim\n",
    "        self.dropout = 0\n",
    "        self.batch_size = batch_size\n",
    "        self.LSTM_layer = LSTM_layer\n",
    "\n",
    "        self.n_layers = n_layers\n",
    "        self.lstm1 = nn.LSTM(input_size = featrues_dim,\n",
    "                            hidden_size = self.hidden_size,\n",
    "                            num_layers = self.n_layers,\n",
    "                            )\n",
    "        self.lstm2 = nn.LSTM(input_size = self.hidden_size,\n",
    "                            hidden_size = self.hidden_size,\n",
    "                            num_layers = self.n_layers,\n",
    "                            )\n",
    "        self.flat = nn.Flatten()\n",
    "        slef.fc = nn.Linear()\n",
    "    \n",
    "    def forward(self, x):\n",
    "#         assert x.shape==(self.batch_size,seq_length,self.featrues_dim)\n",
    "        x.transpose_(1, 0)\n",
    "#         assert x.shape==(seq_length, self.batch_size, self.featrues_dim)\n",
    "        lstm1, (h_t, final_cell_state) = self.lstm1(x)\n",
    "#         lstm1.transpose_(1, 0)    \n",
    "#         assert lstm1.shape==(seq_length, self.batch_size, self.hidden_size)\n",
    "        \n",
    "        if self.LSTM_layer == 1:\n",
    "            output = lstm1.permute(1, 0 ,2)\n",
    "            attn_output = self.attention_net(output)\n",
    "            logit = self.fc(attn_output)\n",
    "        elif self.LSTM_layer == 2:\n",
    "            lstm2, (h_t, final_cell_state) = self.lstm2(h_t)\n",
    "            output = lstm2.permute(1, 0 ,2)\n",
    "            x = self.flat(output)\n",
    "            x = self.fc(x)\n",
    "            \n",
    "        return x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, y_true, y_pred):\n",
    "        return torch.sqrt(torch.mean(torch.pow(((y_true-y_pred)/y_pred),2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM_attn(\n",
      "  (lstm1): LSTM(29, 50, num_layers=2)\n",
      "  (lstm2): LSTM(50, 50, num_layers=2)\n",
      "  (fc1): Linear(in_features=150, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "TRAIN_BATCH_SIZE = 1024 * 5\n",
    "TEST_BATCH_SIZE  = 1024 * 5\n",
    "\n",
    "train_dataset = myDataset(np_train[train_idx, :, :], targets[train_idx])\n",
    "test_dataset =  myDataset(np_train[valid_idx, :, :], targets[valid_idx])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = TRAIN_BATCH_SIZE,\n",
    "                                           num_workers = 0)\n",
    "test_loader  = torch.utils.data.DataLoader(test_dataset , batch_size = TRAIN_BATCH_SIZE,\n",
    "                                           num_workers = 0)\n",
    "\n",
    "assert torch.cuda.is_available()\n",
    "device = 'cuda'\n",
    "\n",
    "hidden_size=50\n",
    "n_layers=2\n",
    "LSTM_layer = 2\n",
    "\n",
    "\n",
    "model = LSTM_attn(TRAIN_BATCH_SIZE, hidden_size=hidden_size, n_layers=n_layers, featrues_dim=29, LSTM_layer = LSTM_layer)\n",
    "model.to(device)\n",
    "\n",
    "criterion = My_loss().to(device)\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = 0.006)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88d63fec39404a19a1db41b979bd220e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output torch.Size([5120, 2, 100])\n",
      "final_hidden_state torch.Size([5120, 2, 100])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (20480x100 and 200x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\RUISAV~1\\AppData\\Local\\Temp/ipykernel_7692/2321105954.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Users\\RuiSavior\\anaconda3\\envs\\optiver_kaggle\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\RUISAV~1\\AppData\\Local\\Temp/ipykernel_7692/3606273460.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'final_hidden_state'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfinal_hidden_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal_hidden_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Users\\RuiSavior\\anaconda3\\envs\\optiver_kaggle\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Users\\RuiSavior\\anaconda3\\envs\\optiver_kaggle\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Users\\RuiSavior\\anaconda3\\envs\\optiver_kaggle\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1845\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1846\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1847\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1848\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (20480x100 and 200x128)"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(epochs)):\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    model.train()\n",
    "    avg_loss = []\n",
    "    val_avg_loss= []\n",
    "    for data,target in train_loader:\n",
    "        data = data.float()\n",
    "        \n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "            \n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        avg_loss.append(loss.item())\n",
    "    avg_loss = np.array(avg_loss).mean()\n",
    "    train_loss.append(avg_loss)\n",
    "        \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data,target in test_loader:\n",
    "            data = data.float()\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            val_avg_loss.append(loss.item())\n",
    "        val_avg_loss = np.array(avg_loss).mean()\n",
    "        train_loss.append(avg_loss)\n",
    "        val_loss.append(val_avg_loss)\n",
    "    print('Epoch:  {}   |Train_Loss: {:.6f} |Val_Loss: {:.6f}'.format(epoch + 1,avg_loss,val_avg_loss))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Oct 11 11:22:21 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 472.12       Driver Version: 472.12       CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "| N/A   35C    P8    12W /  N/A |   1508MiB /  6144MiB |      6%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      3728    C+G   ...7pnf6hceqser\\snipaste.exe    N/A      |\n",
      "|    0   N/A  N/A      4332    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A      8732    C+G   ...b3d8bbwe\\WinStore.App.exe    N/A      |\n",
      "|    0   N/A  N/A     14188    C+G   ...ekyb3d8bbwe\\YourPhone.exe    N/A      |\n",
      "|    0   N/A  N/A     14364    C+G   ...ram Files\\LGHUB\\lghub.exe    N/A      |\n",
      "|    0   N/A  N/A     14680    C+G   ...nputApp\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     15176    C+G   ...wekyb3d8bbwe\\Video.UI.exe    N/A      |\n",
      "|    0   N/A  N/A     17516    C+G   ...ekyb3d8bbwe\\onenoteim.exe    N/A      |\n",
      "|    0   N/A  N/A     19432    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A     22628    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     26532    C+G   ...lPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A     28900    C+G   ...ne\\bin\\webwallpaper32.exe    N/A      |\n",
      "|    0   N/A  N/A     29876    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A     30672    C+G   ...perience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A     32120    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A     32296    C+G   ...me\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A     32604    C+G   ...6bftszj\\TranslucentTB.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f44236101734719a3f6ec31edc2af89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output torch.Size([1024, 50])\n",
      "final_hidden_state torch.Size([1024, 2, 50])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Got 100 and 50 (The offending index is 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\RUISAV~1\\AppData\\Local\\Temp/ipykernel_7692/1302146709.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Users\\RuiSavior\\anaconda3\\envs\\optiver_kaggle\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\RUISAV~1\\AppData\\Local\\Temp/ipykernel_7692/928735777.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;31m#             print(h_t[:,0,:].shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[0mht2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh_t\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh_t\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattn_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mht2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'x'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Got 100 and 50 (The offending index is 0)"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "kfold = KFold(n_splits = 2, random_state = 2021, shuffle = True)\n",
    "fold_loss = []\n",
    "# Iterate through each fold\n",
    "for fold, (trn_ind, val_ind) in enumerate(kfold.split(np_train)):\n",
    "    TRAIN_BATCH_SIZE = 1024 * 5\n",
    "    TEST_BATCH_SIZE  = 1024 * 5\n",
    "\n",
    "    train_dataset = myDataset(np_train[trn_ind, :, :], targets[trn_ind])\n",
    "    test_dataset =  myDataset(np_train[val_ind, :, :], targets[val_ind])\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = 1024,\n",
    "                                           num_workers = 0)\n",
    "    test_loader  = torch.utils.data.DataLoader(test_dataset , batch_size = 1024,\n",
    "                                           num_workers = 0)\n",
    "    \n",
    "    print(f'Training fold {fold + 1}')\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    best_val_loss = float('+inf')\n",
    "    if hasattr(torch.cuda, 'empty_cache'):\n",
    "        torch.cuda.empty_cache()\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        model.train()\n",
    "        avg_loss = []\n",
    "        val_avg_loss= []\n",
    "        for data,target in train_loader:\n",
    "            data = data.float()\n",
    "\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            avg_loss.append(loss.item())\n",
    "        avg_loss = np.array(avg_loss).mean()\n",
    "        train_loss.append(avg_loss)\n",
    "        scheduler.step(avg_loss)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for data,target in test_loader:\n",
    "                data = data.float()\n",
    "                data = data.to(device)\n",
    "                target = target.to(device)\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                val_avg_loss.append(loss.item())\n",
    "            val_avg_loss = np.array(val_avg_loss).mean()\n",
    "            val_loss.append(val_avg_loss)\n",
    "\n",
    "        if val_avg_loss < best_val_loss:\n",
    "            best_val_loss = val_avg_loss\n",
    "            best_epoch = epoch\n",
    "        torch.save(model.state_dict(), 'LSTM_attn-model.pt')\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print('Epoch:  {}   |Train_Loss: {:.6f} |Val_Loss: {:.6f}'.format(epoch + 1,avg_loss,val_avg_loss))\n",
    "    print('best_val_loss at '+str(best_epoch)+'epoch')\n",
    "    print('Fold'+str(fold)+'\\'s val_loss is'+str(best_val_loss))\n",
    "    fold_loss.append(best_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:optiver_kaggle]",
   "language": "python",
   "name": "conda-env-optiver_kaggle-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
