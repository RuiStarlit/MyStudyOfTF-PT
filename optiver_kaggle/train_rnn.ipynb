{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "north-quarter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from utils import *\n",
    "from Network import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import time\n",
    "\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "extended-bulletin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 20, 64]), torch.Size([5, 32, 64]), torch.Size([5, 32, 64]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = nn.LSTM(input_size = 15, hidden_size =64, num_layers =5, bidirectional=False, dropout =0.2, batch_first=True)(torch.Tensor(32,20,15))\n",
    "out[0].shape, out[1][0].shape, out[1][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capable-castle",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "pacific-wells",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n"
     ]
    }
   ],
   "source": [
    "# hyperparams\n",
    "enc_seq_len = 20\n",
    "dec_seq_len = 3\n",
    "output_sequence_length = 0\n",
    "\n",
    "dim_val = 32\n",
    "dim_attn = 5\n",
    "lr = 0.05\n",
    "epochs = 20\n",
    "\n",
    "batch_size = 32\n",
    "input_size = 15\n",
    "\n",
    "n_heads = 3\n",
    "# init network and optimizer\n",
    "# dim_val*2\n",
    "\n",
    "class BoostRNN(torch.nn.Module):\n",
    "    def __init__(self, dim_val, dim_attn, input_size, n_layers, n_heads):\n",
    "        super(BoostRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dim_val = dim_val\n",
    "        \n",
    "        self.rnn = nn.LSTM(input_size = input_size, hidden_size =dim_val, num_layers =n_layers, bidirectional=False, dropout =0.2, batch_first=True)\n",
    "        self.rnn2 = nn.LSTM(input_size = 7, hidden_size =dim_val, num_layers =n_layers, bidirectional=False, dropout =0.2, batch_first=True)\n",
    "\n",
    "        self.fc0 = nn.Linear(input_size, dim_val*2)\n",
    "        self.fc1 = nn.Linear(dim_val*2, dim_val*2)\n",
    "        self.fc2 = nn.Linear(dim_val*2, 1)\n",
    "\n",
    "        self.fc3 = nn.Linear(dim_val*2, dim_val*2)\n",
    "        self.fc4 = nn.Linear(dim_val*2, 1)\n",
    "        \n",
    "        self.alpha = torch.nn.parameter.Parameter(torch.Tensor(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        xx = x[:, :, [0, 2, 10, 11, 12, 13, 14]]\n",
    "        \n",
    "        # x = self.fc0(x)\n",
    "        x, (hn, cn) = self.rnn(x)\n",
    "        hn = torch.cat((hn[-2,:,:], hn[-1,:,:]), dim = 1) \n",
    "        x = self.fc2(F.elu(self.fc1(hn)))\n",
    "        \n",
    "        xx, (hn, cn) = self.rnn2(xx)\n",
    "        hn = torch.cat((hn[-2,:,:], hn[-1,:,:]), dim = 1) \n",
    "        xx = self.fc4(F.elu(self.fc3(hn)))\n",
    "\n",
    "        x = torch.sigmoid(self.alpha) * x + (1 - torch.sigmoid(self.alpha)) * xx\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self, dim_val, dim_attn, input_size, n_layers, n_heads):\n",
    "        super(RNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dim_val = dim_val\n",
    "        \n",
    "        self.rnn = nn.LSTM(input_size = input_size, hidden_size =dim_val, num_layers =n_layers, bidirectional=False, dropout =0.2, batch_first=True)\n",
    "\n",
    "        self.fc1 = nn.Linear(dim_val*2, dim_val*2)\n",
    "        self.fc2 = nn.Linear(dim_val*2, 1)\n",
    "\n",
    "        self.fc3 = nn.Linear(dim_val*2, dim_val*2)\n",
    "        self.fc4 = nn.Linear(dim_val*2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.fc0(x)\n",
    "        x, (hn, cn) = self.rnn(x)\n",
    "        hn = torch.cat((hn[-2,:,:], hn[-1,:,:]), dim = 1) \n",
    "        x = self.fc2(F.elu(self.fc1(hn)))\n",
    "        xx = self.fc4(F.elu(self.fc3(hn)))\n",
    "        return x, xx\n",
    "\n",
    "    \n",
    "class SelectRNN(torch.nn.Module):\n",
    "    def __init__(self, dim_val, dim_attn, input_size, n_layers, n_heads):\n",
    "        super(SelectRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dim_val = dim_val\n",
    "        \n",
    "        self.rnn = nn.LSTM(input_size = input_size, hidden_size =dim_val, num_layers =n_layers, bidirectional=False, dropout =0.2, batch_first=True)\n",
    "        \n",
    "        self.fc1 = nn.Linear(dim_val*2, dim_val*2)\n",
    "        self.fc2 = nn.Linear(dim_val*2, 1)\n",
    "\n",
    "        self.key1 = Key(input_size, input_size)\n",
    "        self.query1 = Query(input_size, input_size)\n",
    "        \n",
    "        self.key2 = Key(input_size, input_size)\n",
    "        self.query2 = Query(input_size, input_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        m1 = torch.matmul(self.query1(x), self.key1(x).transpose(2,1).float())\n",
    "        m2 = torch.matmul(self.query2(x).transpose(2,1).float(), self.key2(x))\n",
    "        p1 = torch.softmax(torch.mean(m1, -1), -1).unsqueeze(2)\n",
    "        p2 = torch.softmax(torch.mean(m2, -1), -1).unsqueeze(1)\n",
    "\n",
    "        x = x * p2\n",
    "        x, (hn, cn) = self.rnn(x)\n",
    "        hn = torch.cat((hn[-2,:,:], hn[-1,:,:]), dim = 1) \n",
    "        x = self.fc2(F.elu(self.fc1(hn)))\n",
    "        return x, (p1, p2)\n",
    "                \n",
    "class AttentionRNN(torch.nn.Module):\n",
    "    def __init__(self, dim_val, dim_attn, input_size, n_layers, n_heads):\n",
    "        super(AttentionRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dim_val = dim_val\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, dim_val)\n",
    "\n",
    "        self.attn = MultiHeadAttentionBlock(dim_val, dim_attn, n_heads)\n",
    "        self.norm = nn.LayerNorm(dim_val)\n",
    "        self.rnn = nn.LSTM(input_size = dim_val, hidden_size =dim_val, num_layers =n_layers, bidirectional=False, dropout =0.2, batch_first=True)\n",
    "\n",
    "        self.fc2 = nn.Linear(dim_val*2, dim_val*2)\n",
    "        self.fc3 = nn.Linear(dim_val*2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        a = self.attn(x)\n",
    "        x = self.norm(a + x)\n",
    "        \n",
    "        x, (hn, cn) = self.rnn(x)\n",
    "        hn = torch.cat((hn[-2,:,:], hn[-1,:,:]), dim = 1) \n",
    "        x = self.fc3(F.elu(self.fc2(hn)))\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self, dim_val, dim_attn, input_size, n_layers, n_heads):\n",
    "        super(CNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dim_val = dim_val\n",
    "\n",
    "        self.conv = nn.Conv2d(input_size, dim_val, (1, 1))\n",
    "\n",
    "        self.conv1 = nn.Conv2d(input_size, dim_val, (3, 1), padding=(1, 0))\n",
    "        self.conv2 = nn.Conv2d(input_size, dim_val, (5, 1), padding=(2, 0))\n",
    "        self.conv3 = nn.Conv2d(input_size, dim_val, (7, 1), padding=(3, 0))\n",
    "   \n",
    "        self.pool = nn.AdaptiveMaxPool2d((1 ,1))\n",
    "\n",
    "        self.pool1 = nn.AdaptiveMaxPool2d((1 ,1))\n",
    "        self.pool2 = nn.AdaptiveMaxPool2d((1 ,1))\n",
    "        self.pool3 = nn.AdaptiveMaxPool2d((1 ,1))\n",
    "        \n",
    "        self.bn = nn.BatchNorm2d(dim_val)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(dim_val)\n",
    "        self.bn2 = nn.BatchNorm2d(dim_val)\n",
    "        self.bn3 = nn.BatchNorm2d(dim_val)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(dim_val*3, 1)\n",
    "        self.fc1 = nn.Linear(dim_val*3, dim_val)\n",
    "        self.fc2 = nn.Linear(dim_val, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(3)\n",
    "        x = x.transpose(2, 1)\n",
    "        \n",
    "        x1 = F.elu(self.conv1(x))\n",
    "        x2 = F.elu(self.conv2(x))\n",
    "        x3 = F.elu(self.conv3(x))\n",
    "        \n",
    "        x = torch.cat([x1, x2, x3], dim=1)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = x.flatten(start_dim=1)\n",
    "        x = self.dropout(x)\n",
    "        # x = self.fc2(F.tanh(self.fc1(x)))\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class SimpleCNN(torch.nn.Module):\n",
    "    def __init__(self, dim_val, dim_attn, input_size, n_layers, n_heads):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dim_val = dim_val\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, dim_val, (3, 3))\n",
    "        self.conv2 = nn.Conv2d(dim_val, dim_val*2, (3, 3))\n",
    "        self.conv3 = nn.Conv2d(dim_val*2, dim_val*2, (3, 3))\n",
    "        \n",
    "        self.pool = nn.AdaptiveAvgPool2d((1 ,1))\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(dim_val)\n",
    "        self.bn2 = nn.BatchNorm2d(dim_val*2)\n",
    "        self.bn3 = nn.BatchNorm2d(dim_val*2)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(dim_val*2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.bn1(F.elu(self.conv1(x)))\n",
    "        x = self.bn2(F.elu(self.conv2(x)))\n",
    "        x = self.bn3(F.elu(self.conv3(x)))\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x.flatten(start_dim=1))\n",
    "        return x\n",
    "\n",
    "    \n",
    "t1 = RNN(dim_val, dim_attn, input_size, n_layers=2, n_heads=n_heads)\n",
    "t1.cuda()\n",
    "\n",
    "# t2 = RNN(dim_val, dim_attn, input_size, n_layers=2, n_heads=n_heads)\n",
    "optimizer1 = torch.optim.SGD(t1.parameters(), lr=lr, weight_decay=0.01)\n",
    "scheduler1 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer1, \n",
    "                                                        mode='min', \n",
    "                                                        factor=0.9, \n",
    "                                                        patience=10, \n",
    "                                                        verbose=False, \n",
    "                                                        threshold=0.0001, \n",
    "                                                        threshold_mode='rel', \n",
    "                                                        cooldown=0, \n",
    "                                                        min_lr=0, \n",
    "                                                        eps=1e-08)\n",
    "# optimizer2 = torch.optim.SGD(t1.parameters(), lr=lr)\n",
    "# t2.cuda()\n",
    "\n",
    "print('success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "first-ethernet",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "class FileIter():\n",
    "    def __init__(self, f, enc_seq_len=20):\n",
    "        self.name = f\n",
    "        self.f = h5py.File(f, 'r')\n",
    "        self.n = len(self.f['timestamp'])\n",
    "        self.indexes = np.arange(self.n)\n",
    "        self.batch_size = batch_size\n",
    "        self.index = 0\n",
    "        self.mask = list(range(15)) #[1, 3, 4, 5, 6, 7, 8, 9]# [0, 2, 10, 11, 12, 13, 14]\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        batch_index = self.indexes[i * self.batch_size : (i + 1) * self.batch_size]\n",
    "        X = self.f['x'][batch_index, -enc_seq_len:, :]\n",
    "        Y = self.f['y'][batch_index]\n",
    "        X = torch.tensor(X[:, :, np.array(self.mask)]).cuda()  \n",
    "        # X = torch.tensor(X).cuda() \n",
    "        Y = torch.tensor(Y).cuda()\n",
    "        return X, Y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.n / self.batch_size))\n",
    "        \n",
    "    def next_item(self):\n",
    "        if self.index >= self.__len__():\n",
    "            self.index = 0\n",
    "        X, Y = self.__getitem__(self.index)\n",
    "        self.index += 1\n",
    "        return X, Y\n",
    "        \n",
    "tmp_l =  glob.glob('../stockprice/train/*.hdf')\n",
    "tmp_l.remove('../stockprice/train/20190425_20190510.hdf')\n",
    "train_fs = [FileIter(f) for f in tmp_l]\n",
    "test_fs = [FileIter(f) for f in glob.glob('../stockprice/test/*.hdf')]\n",
    "sum_n = np.sum([f.n for f in train_fs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "narrow-horror",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9375, device='cuda:0')\n",
      "torch.Size([16, 1])\n",
      "0.9875233769416809\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1.1535197701434576"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tst_X, tst_Y = test_fs[np.random.randint(len(test_fs))].next_item()\n",
    "t1.eval()\n",
    "# net_out, a = t1(tst_X)\n",
    "# plt.plot(a[0][0][:,0].cpu().detach().numpy())\n",
    "# plt.show()\n",
    "\n",
    "# plt.plot(a[1][0][0].cpu().detach().numpy())\n",
    "# plt.show()\n",
    "\n",
    "# plt.imshow((a[0][0] * a[1][0]).cpu().detach().numpy())\n",
    "# plt.show()\n",
    "net_out = t1(tst_X)[0]\n",
    "\n",
    "print(torch.mean(torch.gt(net_out * tst_Y, 0).float()))\n",
    "print(net_out.shape)\n",
    "net_out = np.squeeze(net_out.detach().cpu().numpy())\n",
    "tst_Y = np.squeeze(tst_Y.cpu().numpy())\n",
    "np.sum((tst_Y - net_out)**2) / batch_size\n",
    "plt.plot(net_out)\n",
    "plt.plot(tst_Y)\n",
    "print(np.sum((net_out - tst_Y)**2)/batch_size)\n",
    "r2 = 1 - np.sum((net_out - tst_Y)**2) /(np.sum((tst_Y - np.mean(tst_Y))**2) + 1e-4)\n",
    "r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "certified-gamma",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "251172\n"
     ]
    }
   ],
   "source": [
    "print('='*10)\n",
    "# keep track of loss for graph\n",
    "losses = []\n",
    "r2_list = []\n",
    "rate_list = []\n",
    "\n",
    "train_loss = None\n",
    "train_r2 = None\n",
    "train_rate = None\n",
    "train_acc = None\n",
    "\n",
    "metrics_loss = None\n",
    "metrics_r2 = None\n",
    "metrics_rate = None\n",
    "metrics_acc = None\n",
    "\n",
    "train_file_index = 0\n",
    "test_file_index = 0\n",
    "\n",
    "f_r2 = lambda x, y: 1 - torch.sum((x - y)**2) /(torch.sum((y - torch.mean(y))**2) + 1e-4)\n",
    "f_rate = lambda x, y: torch.mean(torch.gt(x * y, 0).float())\n",
    "f_average = lambda x, y : x.cpu() # if y is None else 0.8*y + 0.2 *x.cpu()\n",
    "f_acc = lambda x, y: torch.mean(torch.eq(torch.gt(x, 0.5).int(), y).float())\n",
    "\n",
    "print(sum_n//batch_size - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convenient-verse",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "northern-bangladesh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2526, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.2971, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(1.2258, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0057, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(2.0917, device='cuda:0', grad_fn=<AddBackward0>) tensor(-2.3840, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(1.3398, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.4464, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(5.9556, device='cuda:0', grad_fn=<AddBackward0>) tensor(-1.8125, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(15.7481, device='cuda:0', grad_fn=<AddBackward0>) tensor(-18.6500, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(3.8053, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0410, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(9.9928, device='cuda:0', grad_fn=<AddBackward0>) tensor(-5.3689, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(6.1717, device='cuda:0', grad_fn=<AddBackward0>) tensor(-1.0906, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(1.5996, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0017, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(7.0988, device='cuda:0', grad_fn=<AddBackward0>) tensor(-4.3747, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(3.1664, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.1123, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(4.2036, device='cuda:0', grad_fn=<AddBackward0>) tensor(-2.1956, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(4.7215, device='cuda:0', grad_fn=<AddBackward0>) tensor(-1.4968, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(2.7892, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.4698, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(1.1913, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.1376, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(2.9913, device='cuda:0', grad_fn=<AddBackward0>) tensor(-2.7509, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(1.6918, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.3282, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(1.2741, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0313, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(2.0711, device='cuda:0', grad_fn=<AddBackward0>) tensor(-1.0950, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(1.3275, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0875, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(1.3354, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0008, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(3.3873, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0375, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(4.9255, device='cuda:0', grad_fn=<AddBackward0>) tensor(-5.9711, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(4.5861, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0594, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(7.8204, device='cuda:0', grad_fn=<AddBackward0>) tensor(-1.6546, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(3.0126, device='cuda:0', grad_fn=<AddBackward0>) tensor(-3.4996, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(3.6141, device='cuda:0', grad_fn=<AddBackward0>) tensor(-1.8118, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(5.6709, device='cuda:0', grad_fn=<AddBackward0>) tensor(-2.4683, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(2.2562, device='cuda:0', grad_fn=<AddBackward0>) tensor(-1.8703, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(5.5474, device='cuda:0', grad_fn=<AddBackward0>) tensor(-2.1387, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(2.6697, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.3133, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(4.8726, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.5332, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(2.4405, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0102, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(3.8284, device='cuda:0', grad_fn=<AddBackward0>) tensor(-2.3238, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(1.8451, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0253, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(2.9774, device='cuda:0', grad_fn=<AddBackward0>) tensor(-2.2638, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(1.7682, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.2554, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(2.4863, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.1456, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(2.2500, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0055, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(4.4057, device='cuda:0', grad_fn=<AddBackward0>) tensor(-2.6319, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(6.2355, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.1703, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(1.8311, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.2460, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(1.7638, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.5179, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(3.5194, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.4352, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(3.6895, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0066, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(4.4049, device='cuda:0', grad_fn=<AddBackward0>) tensor(-6.0684, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(3.2473, device='cuda:0', grad_fn=<AddBackward0>) tensor(-2.1163, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(1.2689, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0923, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(2.9084, device='cuda:0', grad_fn=<AddBackward0>) tensor(-1.4463, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(2.2388, device='cuda:0', grad_fn=<AddBackward0>) tensor(-2.0077, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(1.3442, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.1214, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(3.0752, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.4275, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(5.7330, device='cuda:0', grad_fn=<AddBackward0>) tensor(-8.9011, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(2.3033, device='cuda:0', grad_fn=<AddBackward0>) tensor(-2.9841, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(1.2468, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.4317, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(2.1734, device='cuda:0', grad_fn=<AddBackward0>) tensor(-1.4503, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(2.5763, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0210, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(4.6884, device='cuda:0', grad_fn=<AddBackward0>) tensor(-1.0194, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(4.5894, device='cuda:0', grad_fn=<AddBackward0>) tensor(-2.8715, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(7.6524, device='cuda:0', grad_fn=<AddBackward0>) tensor(-5.3275, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(5.0560, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0843, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(1.9297, device='cuda:0', grad_fn=<AddBackward0>) tensor(-2.3240, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(1.7863, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.1611, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(2.7546, device='cuda:0', grad_fn=<AddBackward0>) tensor(-4.5558, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(2.0153, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.8617, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(3.1164, device='cuda:0', grad_fn=<AddBackward0>) tensor(-4.5109, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(5.5982, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.7643, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(13.8234, device='cuda:0', grad_fn=<AddBackward0>) tensor(-4.0904, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(2.2252, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0046, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(2.2971, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.5818, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(7.6599, device='cuda:0', grad_fn=<AddBackward0>) tensor(-2.1496, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(4.1491, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0395, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(5.3806, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0477, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(2.4272, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.1238, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(2.4779, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.2121, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(1.5381, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.2148, device='cuda:0', grad_fn=<RsubBackward1>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.5417, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.5742, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(1.6466, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0826, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(4.7765, device='cuda:0', grad_fn=<AddBackward0>) tensor(-2.1048, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(2.4520, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.8744, device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "tensor(1.9647, device='cuda:0', grad_fn=<AddBackward0>) tensor(-1.3506, device='cuda:0', grad_fn=<RsubBackward1>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-3587445bdbc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mnet_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mloss1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.3\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mloss1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0moptimizer1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_r2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 32-2\n",
    "# from Network import RNN\n",
    "t1 = RNN(64, dim_attn, input_size, n_layers=4, n_heads=n_heads)\n",
    "t1.cuda()\n",
    "optimizer1 = torch.optim.SGD(t1.parameters(), lr=lr, weight_decay=0.01)\n",
    "t1.train()\n",
    "\n",
    "while True:\n",
    "    X, Y = train_fs[train_file_index % len(train_fs)].next_item()\n",
    "    optimizer1.zero_grad()\n",
    "    net_out = t1(X)\n",
    "    loss1 = F.binary_cross_entropy_with_logits(net_out[1], torch.gt(Y, 0).float()) + torch.mean((net_out[0] - Y) ** 2) + 0.3 * torch.sum((net_out[0] - Y)**2) /(torch.sum((Y - torch.mean(Y))**2) + 1e-4) #\n",
    "    loss1.backward()\n",
    "    optimizer1.step()\n",
    "    print(loss1, f_r2(net_out[0], Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adapted-alexander",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:0 train loss: 3.070, r2: -0.022, rate:0.312, acc:0.688 val loss: 1.419, r2: -0.033, rate:0.375, acc:0.625 lr 0.05\n",
      "epoch:0 step:100 train loss: 0.817, r2: 0.193, rate:0.562, acc:0.562 val loss: 0.845, r2: 0.165, rate:0.500, acc:0.750 lr 0.05\n",
      "epoch:0 step:200 train loss: 3.631, r2: -0.844, rate:0.812, acc:0.188 val loss: 1.104, r2: 0.372, rate:0.625, acc:0.562 lr 0.05\n",
      "epoch:0 step:300 train loss: 0.816, r2: -0.207, rate:0.812, acc:0.562 val loss: 1.324, r2: -0.309, rate:0.562, acc:0.688 lr 0.05\n",
      "epoch:0 step:400 train loss: 1.632, r2: 0.026, rate:0.625, acc:0.688 val loss: 1.666, r2: -0.634, rate:0.812, acc:0.375 lr 0.05\n",
      "epoch:0 step:500 train loss: 0.937, r2: -1.394, rate:0.688, acc:0.438 val loss: 0.833, r2: 0.259, rate:0.812, acc:0.562 lr 0.05\n",
      "epoch:0 step:600 train loss: 1.475, r2: -0.262, rate:0.938, acc:0.438 val loss: 0.993, r2: 0.335, rate:0.875, acc:0.375 lr 0.05\n",
      "epoch:0 step:700 train loss: 1.235, r2: -2.019, rate:0.375, acc:0.750 val loss: 1.320, r2: -3.534, rate:0.312, acc:0.625 lr 0.05\n",
      "epoch:0 step:800 train loss: 0.791, r2: 0.370, rate:0.938, acc:0.812 val loss: 0.989, r2: -1.035, rate:0.562, acc:0.562 lr 0.05\n",
      "epoch:0 step:900 train loss: 0.516, r2: -0.664, rate:1.000, acc:1.000 val loss: 1.463, r2: -0.291, rate:0.562, acc:0.750 lr 0.05\n",
      "epoch:0 step:1000 train loss: 1.829, r2: 0.320, rate:0.688, acc:0.688 val loss: 1.494, r2: -1.174, rate:0.688, acc:0.438 lr 0.05\n",
      "epoch:0 step:1100 train loss: 1.348, r2: -0.857, rate:0.625, acc:0.625 val loss: 2.272, r2: -1.156, rate:0.562, acc:0.750 lr 0.05\n",
      "epoch:0 step:1200 train loss: 0.944, r2: -3.690, rate:0.688, acc:0.688 val loss: 1.328, r2: 0.117, rate:0.438, acc:0.438 lr 0.05\n",
      "epoch:0 step:1300 train loss: 2.014, r2: 0.311, rate:0.812, acc:0.562 val loss: 1.984, r2: -0.192, rate:0.438, acc:0.688 lr 0.05\n",
      "epoch:0 step:1400 train loss: 0.598, r2: 0.336, rate:0.938, acc:0.938 val loss: 1.579, r2: -4.907, rate:0.312, acc:0.125 lr 0.05\n",
      "epoch:0 step:1500 train loss: 1.136, r2: -5.198, rate:0.375, acc:0.375 val loss: 3.305, r2: -11.607, rate:0.000, acc:0.000 lr 0.05\n",
      "epoch:0 step:1600 train loss: 0.701, r2: -0.173, rate:0.625, acc:0.875 val loss: 2.202, r2: -0.800, rate:0.812, acc:0.625 lr 0.05\n",
      "epoch:0 step:1700 train loss: 0.796, r2: -0.813, rate:0.750, acc:0.750 val loss: 1.260, r2: -3.065, rate:0.688, acc:0.375 lr 0.05\n",
      "epoch:0 step:1800 train loss: 0.653, r2: 0.048, rate:0.812, acc:0.562 val loss: 3.452, r2: 0.311, rate:0.812, acc:0.938 lr 0.05\n",
      "epoch:0 step:1900 train loss: 0.584, r2: 0.202, rate:0.812, acc:0.938 val loss: 1.522, r2: -1.973, rate:0.500, acc:0.250 lr 0.05\n",
      "epoch:0 step:2000 train loss: 0.609, r2: -2.143, rate:0.938, acc:0.688 val loss: 0.904, r2: -0.792, rate:0.500, acc:0.625 lr 0.05\n",
      "epoch:0 step:2100 train loss: 0.769, r2: -2.103, rate:0.438, acc:0.562 val loss: 1.328, r2: 0.057, rate:0.562, acc:0.562 lr 0.05\n",
      "epoch:0 step:2200 train loss: 2.088, r2: -3.097, rate:0.875, acc:0.938 val loss: 1.476, r2: -0.270, rate:0.500, acc:0.375 lr 0.05\n",
      "epoch:0 step:2300 train loss: 1.417, r2: -2.011, rate:0.625, acc:0.750 val loss: 0.656, r2: -0.107, rate:0.750, acc:0.812 lr 0.05\n",
      "epoch:0 step:2400 train loss: 1.046, r2: -0.836, rate:0.625, acc:0.500 val loss: 1.308, r2: -5.064, rate:0.312, acc:0.188 lr 0.05\n",
      "epoch:0 step:2500 train loss: 1.025, r2: -0.221, rate:0.750, acc:0.375 val loss: 1.111, r2: 0.093, rate:0.750, acc:0.625 lr 0.05\n",
      "epoch:0 step:2600 train loss: 0.635, r2: -0.907, rate:0.875, acc:1.000 val loss: 1.227, r2: -1.116, rate:0.562, acc:0.562 lr 0.05\n",
      "epoch:0 step:2700 train loss: 0.918, r2: -2.793, rate:0.562, acc:0.688 val loss: 1.006, r2: 0.306, rate:0.625, acc:0.625 lr 0.045000000000000005\n",
      "epoch:0 step:2800 train loss: 0.952, r2: -0.377, rate:0.625, acc:0.500 val loss: 1.517, r2: -0.367, rate:0.562, acc:0.438 lr 0.045000000000000005\n",
      "epoch:0 step:2900 train loss: 3.477, r2: -0.884, rate:0.500, acc:0.500 val loss: 3.444, r2: -0.194, rate:1.000, acc:1.000 lr 0.045000000000000005\n",
      "epoch:0 step:3000 train loss: 0.926, r2: -0.893, rate:0.375, acc:0.562 val loss: 1.270, r2: -0.137, rate:0.562, acc:0.500 lr 0.045000000000000005\n",
      "epoch:0 step:3100 train loss: 2.903, r2: -10.533, rate:0.375, acc:0.250 val loss: 0.556, r2: 0.707, rate:0.875, acc:0.688 lr 0.045000000000000005\n",
      "epoch:0 step:3200 train loss: 0.866, r2: -0.545, rate:0.688, acc:0.750 val loss: 0.974, r2: 0.054, rate:0.750, acc:0.625 lr 0.045000000000000005\n",
      "epoch:0 step:3300 train loss: 1.042, r2: -1.635, rate:0.438, acc:0.500 val loss: 1.097, r2: -0.191, rate:0.688, acc:0.688 lr 0.045000000000000005\n",
      "epoch:0 step:3400 train loss: 1.508, r2: -3.130, rate:0.375, acc:0.188 val loss: 1.670, r2: -1.433, rate:0.625, acc:0.438 lr 0.045000000000000005\n",
      "epoch:0 step:3500 train loss: 7.922, r2: -6.449, rate:0.562, acc:0.438 val loss: 0.699, r2: 0.601, rate:0.812, acc:0.750 lr 0.045000000000000005\n",
      "epoch:0 step:3600 train loss: 1.156, r2: -1.026, rate:0.500, acc:0.625 val loss: 0.972, r2: 0.391, rate:0.812, acc:0.625 lr 0.045000000000000005\n",
      "epoch:0 step:3700 train loss: 0.920, r2: -2.875, rate:0.625, acc:0.562 val loss: 0.736, r2: -0.958, rate:0.750, acc:0.562 lr 0.045000000000000005\n",
      "epoch:0 step:3800 train loss: 1.099, r2: -0.821, rate:0.625, acc:0.812 val loss: 0.612, r2: 0.557, rate:0.812, acc:0.625 lr 0.04050000000000001\n",
      "epoch:0 step:3900 train loss: 1.797, r2: 0.311, rate:0.625, acc:0.812 val loss: 1.008, r2: 0.306, rate:0.812, acc:0.688 lr 0.04050000000000001\n",
      "epoch:0 step:4000 train loss: 4.296, r2: -12.568, rate:0.500, acc:0.875 val loss: 2.644, r2: -0.818, rate:0.500, acc:0.500 lr 0.04050000000000001\n",
      "epoch:0 step:4100 train loss: 1.352, r2: -0.728, rate:0.375, acc:0.812 val loss: 1.797, r2: -2.586, rate:0.312, acc:0.688 lr 0.04050000000000001\n",
      "epoch:0 step:4200 train loss: 1.014, r2: -0.195, rate:0.688, acc:0.500 val loss: 0.768, r2: -0.576, rate:0.625, acc:0.688 lr 0.04050000000000001\n",
      "epoch:0 step:4300 train loss: 0.887, r2: 0.138, rate:0.625, acc:0.438 val loss: 1.081, r2: -0.031, rate:0.625, acc:0.812 lr 0.04050000000000001\n",
      "epoch:0 step:4400 train loss: 0.478, r2: 0.425, rate:0.875, acc:0.875 val loss: 0.905, r2: -2.675, rate:0.625, acc:0.688 lr 0.04050000000000001\n",
      "epoch:0 step:4500 train loss: 0.838, r2: -3.669, rate:0.312, acc:0.562 val loss: 0.813, r2: -0.004, rate:0.625, acc:0.625 lr 0.04050000000000001\n",
      "epoch:0 step:4600 train loss: 0.928, r2: -2.726, rate:0.438, acc:0.562 val loss: 1.642, r2: -11.690, rate:0.188, acc:0.688 lr 0.04050000000000001\n",
      "epoch:0 step:4700 train loss: 1.498, r2: 0.341, rate:0.562, acc:0.500 val loss: 0.819, r2: 0.278, rate:0.875, acc:0.938 lr 0.04050000000000001\n",
      "epoch:0 step:4800 train loss: 1.684, r2: -0.417, rate:0.625, acc:0.562 val loss: 1.203, r2: -6.796, rate:0.688, acc:0.750 lr 0.04050000000000001\n",
      "epoch:0 step:4900 train loss: 0.985, r2: -1.283, rate:0.562, acc:0.625 val loss: 1.344, r2: -3.021, rate:0.500, acc:0.688 lr 0.03645000000000001\n",
      "epoch:0 step:5000 train loss: 0.689, r2: 0.391, rate:0.750, acc:0.812 val loss: 0.894, r2: 0.273, rate:0.688, acc:0.500 lr 0.03645000000000001\n",
      "epoch:0 step:5100 train loss: 0.862, r2: -2.029, rate:0.750, acc:0.500 val loss: 2.366, r2: -0.425, rate:0.438, acc:0.438 lr 0.03645000000000001\n",
      "epoch:0 step:5200 train loss: 0.887, r2: -0.129, rate:0.688, acc:0.750 val loss: 0.924, r2: -1.173, rate:0.562, acc:0.375 lr 0.03645000000000001\n",
      "epoch:0 step:5300 train loss: 1.227, r2: 0.636, rate:0.938, acc:0.938 val loss: 1.018, r2: -0.178, rate:0.500, acc:0.625 lr 0.03645000000000001\n",
      "epoch:0 step:5400 train loss: 0.760, r2: 0.460, rate:0.875, acc:0.250 val loss: 0.748, r2: 0.298, rate:0.812, acc:0.625 lr 0.03645000000000001\n",
      "epoch:0 step:5500 train loss: 1.474, r2: -1.834, rate:0.312, acc:0.250 val loss: 1.839, r2: 0.009, rate:0.750, acc:0.688 lr 0.03645000000000001\n",
      "epoch:0 step:5600 train loss: 1.379, r2: 0.099, rate:0.312, acc:0.375 val loss: 3.035, r2: -0.091, rate:0.500, acc:0.625 lr 0.03645000000000001\n",
      "epoch:0 step:5700 train loss: 0.956, r2: 0.355, rate:0.688, acc:0.875 val loss: 0.955, r2: -1.562, rate:0.688, acc:0.312 lr 0.03645000000000001\n",
      "epoch:0 step:5800 train loss: 0.949, r2: 0.259, rate:0.875, acc:0.625 val loss: 0.791, r2: -1.051, rate:0.562, acc:0.500 lr 0.03645000000000001\n",
      "epoch:0 step:5900 train loss: 1.082, r2: 0.353, rate:0.938, acc:0.812 val loss: 2.238, r2: -0.537, rate:0.875, acc:0.750 lr 0.03645000000000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:6000 train loss: 1.549, r2: 0.437, rate:0.688, acc:0.750 val loss: 1.858, r2: -0.115, rate:0.438, acc:0.500 lr 0.03280500000000001\n",
      "epoch:0 step:6100 train loss: 0.774, r2: 0.530, rate:0.750, acc:0.750 val loss: 1.269, r2: -0.461, rate:0.688, acc:0.438 lr 0.03280500000000001\n",
      "epoch:0 step:6200 train loss: 0.825, r2: -1.654, rate:0.625, acc:1.000 val loss: 5.185, r2: -0.362, rate:0.312, acc:0.500 lr 0.03280500000000001\n",
      "epoch:0 step:6300 train loss: 0.801, r2: -1.491, rate:0.688, acc:0.812 val loss: 1.013, r2: 0.051, rate:0.562, acc:0.938 lr 0.03280500000000001\n",
      "epoch:0 step:6400 train loss: 1.537, r2: -1.288, rate:0.625, acc:1.000 val loss: 0.935, r2: 0.095, rate:0.562, acc:0.688 lr 0.03280500000000001\n",
      "epoch:0 step:6500 train loss: 1.220, r2: 0.108, rate:0.688, acc:0.625 val loss: 0.588, r2: -0.846, rate:0.750, acc:0.938 lr 0.03280500000000001\n",
      "epoch:0 step:6600 train loss: 0.705, r2: 0.598, rate:0.875, acc:0.750 val loss: 1.417, r2: -0.588, rate:0.375, acc:0.375 lr 0.03280500000000001\n",
      "epoch:0 step:6700 train loss: 0.959, r2: -2.151, rate:0.625, acc:0.562 val loss: 1.540, r2: -0.748, rate:0.562, acc:0.812 lr 0.03280500000000001\n",
      "epoch:0 step:6800 train loss: 0.739, r2: 0.254, rate:0.812, acc:0.562 val loss: 0.740, r2: -1.575, rate:0.625, acc:0.500 lr 0.03280500000000001\n",
      "epoch:0 step:6900 train loss: 0.664, r2: 0.047, rate:0.812, acc:0.812 val loss: 1.008, r2: -0.168, rate:0.625, acc:0.625 lr 0.03280500000000001\n",
      "epoch:0 step:7000 train loss: 0.961, r2: -1.276, rate:0.500, acc:0.688 val loss: 1.675, r2: 0.150, rate:0.688, acc:0.625 lr 0.03280500000000001\n",
      "epoch:0 step:7100 train loss: 0.869, r2: -0.334, rate:0.375, acc:0.312 val loss: 0.614, r2: 0.433, rate:0.812, acc:0.750 lr 0.02952450000000001\n",
      "epoch:0 step:7200 train loss: 0.656, r2: 0.007, rate:0.750, acc:0.625 val loss: 0.790, r2: 0.572, rate:0.875, acc:0.688 lr 0.02952450000000001\n",
      "epoch:0 step:7300 train loss: 0.659, r2: 0.449, rate:0.625, acc:0.938 val loss: 0.672, r2: -0.379, rate:0.812, acc:0.812 lr 0.02952450000000001\n",
      "epoch:0 step:7400 train loss: 0.902, r2: 0.162, rate:0.688, acc:0.500 val loss: 0.677, r2: 0.442, rate:0.750, acc:0.625 lr 0.02952450000000001\n",
      "epoch:0 step:7500 train loss: 0.812, r2: 0.513, rate:0.625, acc:0.625 val loss: 1.157, r2: -0.794, rate:0.375, acc:0.312 lr 0.02952450000000001\n",
      "epoch:0 step:7600 train loss: 0.633, r2: 0.253, rate:0.875, acc:0.875 val loss: 0.962, r2: -0.202, rate:0.562, acc:0.562 lr 0.02952450000000001\n",
      "epoch:0 step:7700 train loss: 1.850, r2: -5.335, rate:0.062, acc:0.062 val loss: 0.976, r2: -0.839, rate:0.562, acc:0.250 lr 0.02952450000000001\n",
      "epoch:0 step:7800 train loss: 0.849, r2: 0.052, rate:0.625, acc:0.562 val loss: 1.120, r2: 0.007, rate:0.625, acc:0.375 lr 0.02952450000000001\n",
      "epoch:0 step:0.208 lr 0.029525\n",
      "epoch:1 step:0 train loss: 0.705, r2: -0.060, rate:0.750, acc:0.375 val loss: 0.958, r2: 0.243, rate:0.625, acc:0.688 lr 0.02952450000000001\n",
      "epoch:1 step:100 train loss: 1.259, r2: 0.453, rate:0.562, acc:0.375 val loss: 0.952, r2: 0.351, rate:0.812, acc:0.688 lr 0.02952450000000001\n",
      "epoch:1 step:200 train loss: 0.644, r2: -1.304, rate:0.875, acc:0.625 val loss: 2.002, r2: 0.055, rate:0.812, acc:0.812 lr 0.02952450000000001\n",
      "epoch:1 step:300 train loss: 0.660, r2: 0.643, rate:0.875, acc:0.812 val loss: 1.390, r2: 0.159, rate:0.688, acc:0.562 lr 0.02657205000000001\n",
      "epoch:1 step:400 train loss: 4.884, r2: -2.129, rate:0.500, acc:0.250 val loss: 1.036, r2: 0.541, rate:0.812, acc:0.938 lr 0.02657205000000001\n",
      "epoch:1 step:500 train loss: 0.698, r2: 0.386, rate:0.438, acc:0.688 val loss: 1.937, r2: -0.338, rate:0.688, acc:0.750 lr 0.02657205000000001\n",
      "epoch:1 step:600 train loss: 3.172, r2: 0.333, rate:0.688, acc:0.750 val loss: 0.997, r2: 0.436, rate:0.750, acc:0.562 lr 0.02657205000000001\n",
      "epoch:1 step:700 train loss: 0.751, r2: -4.334, rate:0.375, acc:0.500 val loss: 0.947, r2: 0.403, rate:0.625, acc:0.625 lr 0.02657205000000001\n",
      "epoch:1 step:800 train loss: 1.583, r2: 0.400, rate:0.812, acc:0.688 val loss: 0.722, r2: 0.154, rate:0.750, acc:0.750 lr 0.02657205000000001\n",
      "epoch:1 step:900 train loss: 0.693, r2: 0.243, rate:0.750, acc:0.750 val loss: 0.888, r2: -10.996, rate:0.562, acc:0.688 lr 0.02657205000000001\n",
      "epoch:1 step:1000 train loss: 0.776, r2: 0.431, rate:0.688, acc:0.688 val loss: 0.732, r2: 0.442, rate:0.750, acc:0.750 lr 0.02657205000000001\n",
      "epoch:1 step:1100 train loss: 0.981, r2: 0.086, rate:0.750, acc:0.500 val loss: 1.748, r2: -0.587, rate:0.625, acc:0.688 lr 0.02657205000000001\n",
      "epoch:1 step:1200 train loss: 0.977, r2: 0.191, rate:0.562, acc:0.688 val loss: 0.706, r2: -1.023, rate:0.875, acc:0.500 lr 0.02657205000000001\n",
      "epoch:1 step:1300 train loss: 1.608, r2: 0.508, rate:0.750, acc:0.750 val loss: 1.081, r2: 0.039, rate:0.750, acc:0.625 lr 0.02657205000000001\n",
      "epoch:1 step:1400 train loss: 0.892, r2: -2.707, rate:0.625, acc:0.750 val loss: 1.068, r2: -0.015, rate:0.812, acc:0.625 lr 0.02391484500000001\n",
      "epoch:1 step:1500 train loss: 0.959, r2: 0.397, rate:0.562, acc:0.625 val loss: 1.311, r2: 0.531, rate:0.875, acc:0.938 lr 0.02391484500000001\n",
      "epoch:1 step:1600 train loss: 0.532, r2: 0.449, rate:0.812, acc:0.812 val loss: 1.213, r2: 0.009, rate:0.938, acc:0.875 lr 0.02391484500000001\n",
      "epoch:1 step:1700 train loss: 2.637, r2: 0.067, rate:0.875, acc:0.875 val loss: 1.122, r2: 0.080, rate:0.500, acc:0.500 lr 0.02391484500000001\n",
      "epoch:1 step:1800 train loss: 0.814, r2: -2.271, rate:0.625, acc:0.688 val loss: 0.786, r2: 0.264, rate:0.625, acc:0.688 lr 0.02391484500000001\n",
      "epoch:1 step:1900 train loss: 1.365, r2: 0.516, rate:0.688, acc:0.812 val loss: 1.566, r2: 0.024, rate:0.562, acc:0.500 lr 0.02391484500000001\n",
      "epoch:1 step:2000 train loss: 0.852, r2: -0.547, rate:0.812, acc:0.875 val loss: 0.849, r2: 0.143, rate:0.688, acc:0.625 lr 0.02391484500000001\n",
      "epoch:1 step:2100 train loss: 0.686, r2: -0.249, rate:0.688, acc:0.812 val loss: 1.731, r2: -3.441, rate:0.562, acc:0.812 lr 0.02391484500000001\n",
      "epoch:1 step:2200 train loss: 0.926, r2: -0.839, rate:0.438, acc:0.312 val loss: 0.783, r2: 0.096, rate:0.750, acc:0.750 lr 0.02391484500000001\n",
      "epoch:1 step:2300 train loss: 0.677, r2: -0.106, rate:0.688, acc:0.500 val loss: 0.773, r2: 0.444, rate:0.875, acc:0.625 lr 0.02391484500000001\n",
      "epoch:1 step:2400 train loss: 1.240, r2: -0.281, rate:0.500, acc:0.812 val loss: 1.152, r2: 0.208, rate:0.812, acc:0.750 lr 0.02391484500000001\n",
      "epoch:1 step:2500 train loss: 0.713, r2: 0.133, rate:0.812, acc:0.438 val loss: 0.701, r2: 0.574, rate:0.750, acc:0.562 lr 0.021523360500000012\n",
      "epoch:1 step:2600 train loss: 0.935, r2: 0.310, rate:0.688, acc:0.812 val loss: 1.257, r2: 0.213, rate:0.688, acc:0.750 lr 0.021523360500000012\n",
      "epoch:1 step:2700 train loss: 0.805, r2: -0.185, rate:0.625, acc:1.000 val loss: 1.036, r2: -2.202, rate:0.500, acc:0.438 lr 0.021523360500000012\n",
      "epoch:1 step:2800 train loss: 1.500, r2: -1.799, rate:0.500, acc:1.000 val loss: 1.447, r2: -0.684, rate:0.562, acc:0.688 lr 0.021523360500000012\n",
      "epoch:1 step:2900 train loss: 2.240, r2: 0.237, rate:0.625, acc:0.500 val loss: 1.302, r2: -0.837, rate:0.625, acc:1.000 lr 0.021523360500000012\n",
      "epoch:1 step:3000 train loss: 0.832, r2: -0.468, rate:0.625, acc:0.875 val loss: 0.686, r2: -0.121, rate:0.750, acc:0.812 lr 0.021523360500000012\n",
      "epoch:1 step:3100 train loss: 1.204, r2: -1.128, rate:0.500, acc:0.688 val loss: 0.700, r2: 0.415, rate:0.750, acc:0.812 lr 0.021523360500000012\n",
      "epoch:1 step:3200 train loss: 0.752, r2: 0.327, rate:0.625, acc:0.438 val loss: 1.192, r2: -2.693, rate:0.938, acc:0.750 lr 0.021523360500000012\n",
      "epoch:1 step:3300 train loss: 0.600, r2: 0.785, rate:0.875, acc:0.750 val loss: 1.096, r2: -0.488, rate:0.500, acc:0.688 lr 0.021523360500000012\n",
      "epoch:1 step:3400 train loss: 1.002, r2: -1.262, rate:0.438, acc:0.625 val loss: 0.793, r2: 0.338, rate:0.750, acc:0.562 lr 0.021523360500000012\n",
      "epoch:1 step:3500 train loss: 0.766, r2: 0.161, rate:0.688, acc:0.688 val loss: 1.413, r2: 0.228, rate:0.812, acc:0.875 lr 0.021523360500000012\n",
      "epoch:1 step:3600 train loss: 0.811, r2: -2.801, rate:0.312, acc:0.375 val loss: 1.838, r2: -0.365, rate:0.688, acc:0.438 lr 0.01937102445000001\n",
      "epoch:1 step:3700 train loss: 0.401, r2: -0.741, rate:1.000, acc:1.000 val loss: 0.880, r2: 0.164, rate:0.688, acc:0.500 lr 0.01937102445000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:3800 train loss: 1.317, r2: 0.353, rate:0.438, acc:0.562 val loss: 0.641, r2: 0.357, rate:0.688, acc:0.750 lr 0.01937102445000001\n",
      "epoch:1 step:3900 train loss: 0.689, r2: -0.097, rate:0.812, acc:0.375 val loss: 0.851, r2: 0.402, rate:0.688, acc:0.625 lr 0.01937102445000001\n",
      "epoch:1 step:4000 train loss: 1.244, r2: -0.565, rate:0.562, acc:0.562 val loss: 0.840, r2: -0.423, rate:0.750, acc:0.625 lr 0.01937102445000001\n",
      "epoch:1 step:4100 train loss: 0.635, r2: -0.548, rate:0.812, acc:0.812 val loss: 2.640, r2: 0.394, rate:0.750, acc:0.750 lr 0.01937102445000001\n",
      "epoch:1 step:4200 train loss: 2.356, r2: -0.180, rate:0.938, acc:0.812 val loss: 0.876, r2: 0.305, rate:0.625, acc:0.562 lr 0.01937102445000001\n",
      "epoch:1 step:4300 train loss: 1.154, r2: -0.920, rate:0.375, acc:0.562 val loss: 1.275, r2: 0.042, rate:0.500, acc:0.500 lr 0.01937102445000001\n",
      "epoch:1 step:4400 train loss: 0.754, r2: -0.082, rate:0.812, acc:0.812 val loss: 1.037, r2: -0.719, rate:0.500, acc:0.438 lr 0.01937102445000001\n",
      "epoch:1 step:4500 train loss: 0.853, r2: -0.657, rate:0.562, acc:0.438 val loss: 0.810, r2: 0.510, rate:0.562, acc:0.750 lr 0.01937102445000001\n",
      "epoch:1 step:4600 train loss: 1.560, r2: -1.887, rate:0.500, acc:0.500 val loss: 1.963, r2: -7.066, rate:0.562, acc:0.562 lr 0.01937102445000001\n",
      "epoch:1 step:4700 train loss: 0.865, r2: -0.835, rate:0.562, acc:0.562 val loss: 1.249, r2: 0.295, rate:0.688, acc:0.812 lr 0.01743392200500001\n",
      "epoch:1 step:4800 train loss: 0.789, r2: -0.279, rate:0.625, acc:0.812 val loss: 0.946, r2: -0.685, rate:0.562, acc:1.000 lr 0.01743392200500001\n",
      "epoch:1 step:4900 train loss: 0.769, r2: 0.014, rate:0.562, acc:0.375 val loss: 2.265, r2: -7.010, rate:0.438, acc:0.812 lr 0.01743392200500001\n",
      "epoch:1 step:5000 train loss: 0.859, r2: -0.099, rate:0.500, acc:0.875 val loss: 0.941, r2: 0.491, rate:0.812, acc:0.750 lr 0.01743392200500001\n",
      "epoch:1 step:5100 train loss: 0.705, r2: -2.463, rate:0.500, acc:0.875 val loss: 0.819, r2: 0.453, rate:0.812, acc:0.688 lr 0.01743392200500001\n",
      "epoch:1 step:5200 train loss: 1.062, r2: 0.435, rate:0.688, acc:0.625 val loss: 0.894, r2: 0.555, rate:0.938, acc:0.562 lr 0.01743392200500001\n",
      "epoch:1 step:5300 train loss: 0.888, r2: 0.107, rate:0.750, acc:0.688 val loss: 1.736, r2: -5.420, rate:0.625, acc:0.625 lr 0.01743392200500001\n",
      "epoch:1 step:5400 train loss: 1.783, r2: -2.156, rate:0.375, acc:0.750 val loss: 1.192, r2: -4.920, rate:0.312, acc:0.375 lr 0.01743392200500001\n",
      "epoch:1 step:5500 train loss: 2.108, r2: 0.379, rate:0.688, acc:0.562 val loss: 1.213, r2: -0.395, rate:0.688, acc:0.938 lr 0.01743392200500001\n",
      "epoch:1 step:5600 train loss: 1.087, r2: 0.530, rate:0.750, acc:0.625 val loss: 1.082, r2: 0.116, rate:0.688, acc:0.688 lr 0.01743392200500001\n",
      "epoch:1 step:5700 train loss: 0.893, r2: 0.491, rate:0.500, acc:0.750 val loss: 0.937, r2: 0.119, rate:0.562, acc:0.625 lr 0.01743392200500001\n",
      "epoch:1 step:5800 train loss: 0.708, r2: 0.013, rate:0.812, acc:0.688 val loss: 0.839, r2: 0.282, rate:0.500, acc:0.438 lr 0.015690529804500006\n",
      "epoch:1 step:5900 train loss: 0.725, r2: -1.186, rate:0.625, acc:0.812 val loss: 1.026, r2: -5.773, rate:0.688, acc:0.688 lr 0.015690529804500006\n",
      "epoch:1 step:6000 train loss: 0.952, r2: -1.071, rate:0.562, acc:0.625 val loss: 0.986, r2: 0.239, rate:0.562, acc:0.500 lr 0.015690529804500006\n",
      "epoch:1 step:6100 train loss: 0.492, r2: -0.033, rate:1.000, acc:1.000 val loss: 1.200, r2: 0.219, rate:0.688, acc:0.750 lr 0.015690529804500006\n",
      "epoch:1 step:6200 train loss: 0.529, r2: 0.518, rate:1.000, acc:0.625 val loss: 1.117, r2: 0.142, rate:0.750, acc:0.625 lr 0.015690529804500006\n",
      "epoch:1 step:6300 train loss: 0.966, r2: -10.167, rate:0.250, acc:0.438 val loss: 0.890, r2: 0.269, rate:0.750, acc:0.688 lr 0.015690529804500006\n",
      "epoch:1 step:6400 train loss: 1.235, r2: -4.459, rate:0.562, acc:0.625 val loss: 0.589, r2: 0.609, rate:0.812, acc:0.750 lr 0.015690529804500006\n",
      "epoch:1 step:6500 train loss: 1.748, r2: 0.496, rate:0.875, acc:0.875 val loss: 0.702, r2: 0.252, rate:0.875, acc:0.438 lr 0.015690529804500006\n",
      "epoch:1 step:6600 train loss: 0.699, r2: -0.296, rate:0.625, acc:0.500 val loss: 2.630, r2: -7.776, rate:0.812, acc:0.875 lr 0.015690529804500006\n",
      "epoch:1 step:6700 train loss: 0.715, r2: -0.415, rate:0.812, acc:0.812 val loss: 0.546, r2: 0.272, rate:0.875, acc:0.875 lr 0.015690529804500006\n",
      "epoch:1 step:6800 train loss: 0.541, r2: 0.787, rate:0.750, acc:0.875 val loss: 5.550, r2: -1.025, rate:0.625, acc:0.750 lr 0.015690529804500006\n",
      "epoch:1 step:6900 train loss: 1.217, r2: -1.549, rate:0.250, acc:0.688 val loss: 1.359, r2: -0.994, rate:0.188, acc:0.750 lr 0.014121476824050006\n",
      "epoch:1 step:7000 train loss: 0.627, r2: 0.381, rate:0.812, acc:0.438 val loss: 0.844, r2: -0.070, rate:0.625, acc:0.562 lr 0.014121476824050006\n",
      "epoch:1 step:7100 train loss: 0.651, r2: -0.462, rate:0.750, acc:0.688 val loss: 0.378, r2: 0.747, rate:1.000, acc:0.938 lr 0.014121476824050006\n",
      "epoch:1 step:7200 train loss: 0.820, r2: -1.511, rate:0.500, acc:0.688 val loss: 1.858, r2: 0.066, rate:0.625, acc:0.688 lr 0.014121476824050006\n",
      "epoch:1 step:7300 train loss: 0.610, r2: 0.229, rate:0.875, acc:0.938 val loss: 1.126, r2: -0.640, rate:0.625, acc:0.500 lr 0.014121476824050006\n",
      "epoch:1 step:7400 train loss: 0.941, r2: -1.613, rate:0.562, acc:0.812 val loss: 0.828, r2: -0.691, rate:0.812, acc:0.875 lr 0.014121476824050006\n",
      "epoch:1 step:7500 train loss: 1.088, r2: -0.746, rate:0.688, acc:0.938 val loss: 1.424, r2: 0.150, rate:0.812, acc:0.812 lr 0.014121476824050006\n",
      "epoch:1 step:7600 train loss: 1.346, r2: -1.038, rate:0.750, acc:0.625 val loss: 1.077, r2: 0.088, rate:0.625, acc:0.875 lr 0.014121476824050006\n",
      "epoch:1 step:7700 train loss: 0.832, r2: -0.896, rate:0.500, acc:0.500 val loss: 1.211, r2: -0.207, rate:0.438, acc:0.375 lr 0.014121476824050006\n",
      "epoch:1 step:7800 train loss: 1.146, r2: -0.296, rate:0.500, acc:0.438 val loss: 0.649, r2: 0.439, rate:0.812, acc:0.812 lr 0.014121476824050006\n",
      "epoch:1 step:0.296 lr 0.014121\n",
      "epoch:2 step:0 train loss: 0.633, r2: 0.030, rate:0.875, acc:0.812 val loss: 1.048, r2: -0.051, rate:0.562, acc:0.438 lr 0.014121476824050006\n",
      "epoch:2 step:100 train loss: 0.750, r2: -0.730, rate:0.750, acc:0.875 val loss: 1.312, r2: 0.062, rate:0.750, acc:0.625 lr 0.012709329141645007\n",
      "epoch:2 step:200 train loss: 1.360, r2: -0.684, rate:0.375, acc:0.250 val loss: 1.002, r2: -0.128, rate:0.750, acc:0.688 lr 0.012709329141645007\n",
      "epoch:2 step:300 train loss: 0.867, r2: -1.516, rate:0.562, acc:0.438 val loss: 1.418, r2: -1.979, rate:0.438, acc:0.375 lr 0.012709329141645007\n",
      "epoch:2 step:400 train loss: 1.011, r2: -1.619, rate:0.438, acc:0.500 val loss: 0.911, r2: 0.320, rate:0.562, acc:0.625 lr 0.012709329141645007\n",
      "epoch:2 step:500 train loss: 0.853, r2: -0.190, rate:0.750, acc:0.562 val loss: 1.533, r2: -0.501, rate:0.438, acc:0.562 lr 0.012709329141645007\n",
      "epoch:2 step:600 train loss: 0.760, r2: 0.631, rate:0.500, acc:0.500 val loss: 2.418, r2: -1.470, rate:0.625, acc:0.750 lr 0.012709329141645007\n",
      "epoch:2 step:700 train loss: 0.659, r2: -0.121, rate:0.750, acc:0.688 val loss: 1.035, r2: -0.008, rate:0.688, acc:0.688 lr 0.012709329141645007\n",
      "epoch:2 step:800 train loss: 1.166, r2: 0.426, rate:0.812, acc:0.812 val loss: 0.591, r2: 0.534, rate:0.688, acc:0.875 lr 0.012709329141645007\n",
      "epoch:2 step:900 train loss: 0.669, r2: 0.293, rate:0.750, acc:0.438 val loss: 0.717, r2: 0.493, rate:0.750, acc:0.812 lr 0.012709329141645007\n",
      "epoch:2 step:1000 train loss: 0.697, r2: 0.443, rate:0.625, acc:0.875 val loss: 0.910, r2: -1.260, rate:0.438, acc:0.312 lr 0.012709329141645007\n",
      "epoch:2 step:1100 train loss: 0.799, r2: 0.479, rate:0.812, acc:0.625 val loss: 1.513, r2: -0.899, rate:0.438, acc:0.688 lr 0.012709329141645007\n",
      "epoch:2 step:1200 train loss: 1.118, r2: -0.132, rate:0.688, acc:0.562 val loss: 0.781, r2: 0.107, rate:0.562, acc:0.938 lr 0.011438396227480507\n",
      "epoch:2 step:1300 train loss: 0.974, r2: 0.048, rate:0.750, acc:0.625 val loss: 0.787, r2: 0.492, rate:0.750, acc:0.688 lr 0.011438396227480507\n",
      "epoch:2 step:1400 train loss: 0.682, r2: -0.037, rate:0.625, acc:0.938 val loss: 0.775, r2: 0.309, rate:0.625, acc:0.750 lr 0.011438396227480507\n",
      "epoch:2 step:1500 train loss: 0.691, r2: 0.040, rate:0.812, acc:0.812 val loss: 1.012, r2: -0.130, rate:0.500, acc:0.625 lr 0.011438396227480507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:1600 train loss: 1.093, r2: -19.296, rate:0.062, acc:0.875 val loss: 1.009, r2: -1.246, rate:0.625, acc:0.688 lr 0.011438396227480507\n",
      "epoch:2 step:1700 train loss: 0.747, r2: -3.706, rate:0.688, acc:0.500 val loss: 0.769, r2: 0.200, rate:0.750, acc:0.875 lr 0.011438396227480507\n",
      "epoch:2 step:1800 train loss: 0.668, r2: 0.258, rate:0.625, acc:0.750 val loss: 1.932, r2: -0.493, rate:0.500, acc:0.500 lr 0.011438396227480507\n",
      "epoch:2 step:1900 train loss: 0.610, r2: 0.083, rate:0.750, acc:0.625 val loss: 1.490, r2: -0.606, rate:0.625, acc:0.812 lr 0.011438396227480507\n",
      "epoch:2 step:2000 train loss: 1.043, r2: -1.146, rate:0.625, acc:0.250 val loss: 0.982, r2: 0.413, rate:0.938, acc:0.875 lr 0.011438396227480507\n",
      "epoch:2 step:2100 train loss: 1.303, r2: -0.272, rate:0.312, acc:0.750 val loss: 1.249, r2: -0.224, rate:0.438, acc:0.500 lr 0.011438396227480507\n",
      "epoch:2 step:2200 train loss: 0.654, r2: 0.305, rate:0.750, acc:0.875 val loss: 1.062, r2: -0.556, rate:0.750, acc:0.688 lr 0.011438396227480507\n",
      "epoch:2 step:2300 train loss: 1.203, r2: -0.033, rate:0.438, acc:0.562 val loss: 0.643, r2: 0.659, rate:0.875, acc:0.812 lr 0.010294556604732457\n",
      "epoch:2 step:2400 train loss: 0.884, r2: 0.118, rate:0.812, acc:0.750 val loss: 0.588, r2: 0.457, rate:0.812, acc:0.875 lr 0.010294556604732457\n",
      "epoch:2 step:2500 train loss: 0.557, r2: 0.395, rate:0.875, acc:0.812 val loss: 0.776, r2: 0.553, rate:0.812, acc:0.750 lr 0.010294556604732457\n",
      "epoch:2 step:2600 train loss: 0.830, r2: 0.472, rate:0.812, acc:0.875 val loss: 0.898, r2: 0.266, rate:0.688, acc:0.688 lr 0.010294556604732457\n",
      "epoch:2 step:2700 train loss: 0.810, r2: 0.250, rate:0.688, acc:0.562 val loss: 1.274, r2: -0.140, rate:0.500, acc:0.562 lr 0.010294556604732457\n",
      "epoch:2 step:2800 train loss: 1.040, r2: -2.251, rate:0.438, acc:0.125 val loss: 0.471, r2: 0.702, rate:1.000, acc:0.688 lr 0.010294556604732457\n",
      "epoch:2 step:2900 train loss: 1.187, r2: -5.842, rate:0.188, acc:0.188 val loss: 1.783, r2: -2.740, rate:0.250, acc:0.250 lr 0.010294556604732457\n",
      "epoch:2 step:3000 train loss: 0.818, r2: 0.004, rate:0.625, acc:0.438 val loss: 0.897, r2: -0.116, rate:0.688, acc:0.438 lr 0.010294556604732457\n",
      "epoch:2 step:3100 train loss: 1.215, r2: 0.028, rate:0.500, acc:0.500 val loss: 1.410, r2: 0.227, rate:0.625, acc:0.688 lr 0.010294556604732457\n",
      "epoch:2 step:3200 train loss: 1.247, r2: -0.642, rate:0.438, acc:0.500 val loss: 3.979, r2: -1.357, rate:0.625, acc:0.375 lr 0.010294556604732457\n",
      "epoch:2 step:3300 train loss: 0.832, r2: 0.134, rate:0.875, acc:0.812 val loss: 0.999, r2: 0.068, rate:0.812, acc:0.500 lr 0.010294556604732457\n",
      "epoch:2 step:3400 train loss: 0.562, r2: 0.559, rate:0.938, acc:0.875 val loss: 2.000, r2: 0.379, rate:0.500, acc:0.688 lr 0.00926510094425921\n",
      "epoch:2 step:3500 train loss: 2.579, r2: -4.008, rate:0.938, acc:0.938 val loss: 0.788, r2: -0.020, rate:0.688, acc:0.688 lr 0.00926510094425921\n",
      "epoch:2 step:3600 train loss: 0.738, r2: 0.295, rate:0.625, acc:0.750 val loss: 1.042, r2: -1.596, rate:0.562, acc:0.625 lr 0.00926510094425921\n",
      "epoch:2 step:3700 train loss: 1.266, r2: 0.398, rate:0.438, acc:0.688 val loss: 0.517, r2: -0.163, rate:0.938, acc:1.000 lr 0.00926510094425921\n",
      "epoch:2 step:3800 train loss: 0.954, r2: 0.402, rate:0.562, acc:0.812 val loss: 1.270, r2: 0.287, rate:0.750, acc:0.688 lr 0.00926510094425921\n",
      "epoch:2 step:3900 train loss: 0.650, r2: 0.182, rate:0.812, acc:0.875 val loss: 0.731, r2: 0.411, rate:0.625, acc:0.750 lr 0.00926510094425921\n",
      "epoch:2 step:4000 train loss: 0.940, r2: -0.123, rate:0.438, acc:0.938 val loss: 0.862, r2: -5.010, rate:0.375, acc:0.750 lr 0.00926510094425921\n",
      "epoch:2 step:4100 train loss: 2.234, r2: -0.465, rate:0.500, acc:0.312 val loss: 0.883, r2: 0.001, rate:0.438, acc:0.500 lr 0.00926510094425921\n",
      "epoch:2 step:4200 train loss: 1.545, r2: -0.406, rate:0.312, acc:0.250 val loss: 1.458, r2: -0.166, rate:0.688, acc:0.438 lr 0.00926510094425921\n",
      "epoch:2 step:4300 train loss: 1.516, r2: -0.496, rate:0.562, acc:0.625 val loss: 0.840, r2: 0.378, rate:0.625, acc:0.688 lr 0.00926510094425921\n",
      "epoch:2 step:4400 train loss: 2.539, r2: -11.416, rate:0.438, acc:0.312 val loss: 1.172, r2: -0.813, rate:0.562, acc:0.438 lr 0.00926510094425921\n",
      "epoch:2 step:4500 train loss: 0.663, r2: -0.058, rate:0.688, acc:0.750 val loss: 1.714, r2: -0.052, rate:0.562, acc:0.812 lr 0.00833859084983329\n",
      "epoch:2 step:4600 train loss: 0.652, r2: 0.001, rate:0.625, acc:0.750 val loss: 1.440, r2: 0.174, rate:0.625, acc:0.438 lr 0.00833859084983329\n",
      "epoch:2 step:4700 train loss: 2.839, r2: 0.143, rate:0.875, acc:0.750 val loss: 0.622, r2: -1.215, rate:0.750, acc:1.000 lr 0.00833859084983329\n",
      "epoch:2 step:4800 train loss: 0.948, r2: -0.020, rate:0.688, acc:0.500 val loss: 0.892, r2: 0.268, rate:0.750, acc:0.562 lr 0.00833859084983329\n",
      "epoch:2 step:4900 train loss: 1.334, r2: -0.123, rate:0.688, acc:0.812 val loss: 1.031, r2: -3.330, rate:0.438, acc:0.500 lr 0.00833859084983329\n",
      "epoch:2 step:5000 train loss: 0.633, r2: 0.272, rate:0.812, acc:0.875 val loss: 2.551, r2: -1.611, rate:0.562, acc:0.312 lr 0.00833859084983329\n",
      "epoch:2 step:5100 train loss: 0.721, r2: -1.469, rate:0.438, acc:0.688 val loss: 0.670, r2: -3.351, rate:0.500, acc:0.812 lr 0.00833859084983329\n",
      "epoch:2 step:5200 train loss: 0.581, r2: 0.351, rate:0.875, acc:0.938 val loss: 1.029, r2: -0.723, rate:0.688, acc:0.438 lr 0.00833859084983329\n",
      "epoch:2 step:5300 train loss: 3.372, r2: -2.213, rate:0.562, acc:1.000 val loss: 2.770, r2: -2.360, rate:0.625, acc:0.500 lr 0.00833859084983329\n",
      "epoch:2 step:5400 train loss: 0.839, r2: -0.182, rate:0.750, acc:0.812 val loss: 0.689, r2: 0.430, rate:0.875, acc:0.875 lr 0.00833859084983329\n",
      "epoch:2 step:5500 train loss: 0.746, r2: 0.561, rate:0.688, acc:0.750 val loss: 2.833, r2: -0.079, rate:0.562, acc:0.438 lr 0.00833859084983329\n",
      "epoch:2 step:5600 train loss: 0.634, r2: -0.963, rate:0.812, acc:0.938 val loss: 1.799, r2: 0.267, rate:0.812, acc:0.688 lr 0.007504731764849962\n",
      "epoch:2 step:5700 train loss: 1.119, r2: -4.665, rate:0.500, acc:0.688 val loss: 1.169, r2: -0.223, rate:0.312, acc:0.500 lr 0.007504731764849962\n",
      "epoch:2 step:5800 train loss: 0.892, r2: 0.265, rate:0.688, acc:0.812 val loss: 2.565, r2: -0.350, rate:0.688, acc:0.438 lr 0.007504731764849962\n",
      "epoch:2 step:5900 train loss: 1.113, r2: -1.327, rate:0.625, acc:0.438 val loss: 0.902, r2: -1.141, rate:0.500, acc:0.562 lr 0.007504731764849962\n",
      "epoch:2 step:6000 train loss: 0.949, r2: -0.550, rate:0.500, acc:0.625 val loss: 2.074, r2: -6.627, rate:0.438, acc:0.312 lr 0.007504731764849962\n",
      "epoch:2 step:6100 train loss: 0.647, r2: 0.610, rate:0.625, acc:0.812 val loss: 0.890, r2: 0.260, rate:0.500, acc:0.375 lr 0.007504731764849962\n",
      "epoch:2 step:6200 train loss: 2.632, r2: -0.988, rate:0.750, acc:0.562 val loss: 1.307, r2: 0.126, rate:0.750, acc:0.438 lr 0.007504731764849962\n",
      "epoch:2 step:6300 train loss: 1.125, r2: 0.018, rate:0.500, acc:0.562 val loss: 1.956, r2: -0.051, rate:0.500, acc:0.500 lr 0.007504731764849962\n",
      "epoch:2 step:6400 train loss: 1.043, r2: -0.439, rate:0.688, acc:0.500 val loss: 1.062, r2: -0.259, rate:0.625, acc:0.625 lr 0.007504731764849962\n",
      "epoch:2 step:6500 train loss: 1.019, r2: -0.771, rate:0.938, acc:0.938 val loss: 1.398, r2: 0.128, rate:0.750, acc:0.625 lr 0.007504731764849962\n",
      "epoch:2 step:6600 train loss: 0.603, r2: 0.548, rate:0.812, acc:0.875 val loss: 0.525, r2: 0.755, rate:0.812, acc:0.625 lr 0.007504731764849962\n",
      "epoch:2 step:6700 train loss: 3.338, r2: -2.113, rate:0.438, acc:0.688 val loss: 1.593, r2: -1.340, rate:0.625, acc:0.812 lr 0.006754258588364966\n",
      "epoch:2 step:6800 train loss: 1.970, r2: -2.148, rate:0.375, acc:0.188 val loss: 0.590, r2: -0.089, rate:0.875, acc:0.812 lr 0.006754258588364966\n",
      "epoch:2 step:6900 train loss: 0.825, r2: -1.540, rate:0.500, acc:0.250 val loss: 1.064, r2: -0.317, rate:0.500, acc:0.438 lr 0.006754258588364966\n",
      "epoch:2 step:7000 train loss: 1.088, r2: -0.468, rate:0.875, acc:0.688 val loss: 1.802, r2: -0.455, rate:0.625, acc:0.688 lr 0.006754258588364966\n",
      "epoch:2 step:7100 train loss: 0.718, r2: 0.464, rate:0.812, acc:0.812 val loss: 1.452, r2: -2.469, rate:0.562, acc:0.875 lr 0.006754258588364966\n",
      "epoch:2 step:7200 train loss: 0.760, r2: -0.538, rate:0.688, acc:0.375 val loss: 1.310, r2: 0.565, rate:0.875, acc:0.688 lr 0.006754258588364966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:7300 train loss: 0.577, r2: 0.204, rate:0.938, acc:0.688 val loss: 0.555, r2: 0.767, rate:0.938, acc:0.688 lr 0.006754258588364966\n",
      "epoch:2 step:7400 train loss: 2.076, r2: -0.093, rate:0.812, acc:0.750 val loss: 0.941, r2: -0.978, rate:0.562, acc:0.562 lr 0.006754258588364966\n",
      "epoch:2 step:7500 train loss: 0.716, r2: 0.261, rate:0.688, acc:0.938 val loss: 4.000, r2: -3.688, rate:0.562, acc:0.375 lr 0.006754258588364966\n",
      "epoch:2 step:7600 train loss: 0.787, r2: -0.641, rate:0.625, acc:0.562 val loss: 0.897, r2: 0.395, rate:0.688, acc:0.625 lr 0.006754258588364966\n",
      "epoch:2 step:7700 train loss: 0.907, r2: -0.757, rate:0.688, acc:0.688 val loss: 0.522, r2: 0.538, rate:0.812, acc:0.875 lr 0.006754258588364966\n",
      "epoch:2 step:7800 train loss: 2.292, r2: -3.393, rate:0.375, acc:0.875 val loss: 3.179, r2: -0.257, rate:0.375, acc:0.688 lr 0.00607883272952847\n",
      "epoch:2 step:0.308 lr 0.006079\n",
      "epoch:3 step:0 train loss: 0.738, r2: -0.541, rate:0.562, acc:0.750 val loss: 1.164, r2: 0.375, rate:0.812, acc:0.688 lr 0.00607883272952847\n",
      "epoch:3 step:100 train loss: 0.735, r2: 0.643, rate:0.625, acc:0.375 val loss: 0.863, r2: 0.410, rate:0.875, acc:0.688 lr 0.00607883272952847\n",
      "epoch:3 step:200 train loss: 1.390, r2: -1.991, rate:0.438, acc:0.688 val loss: 1.278, r2: 0.610, rate:0.875, acc:0.875 lr 0.00607883272952847\n",
      "epoch:3 step:300 train loss: 0.553, r2: 0.357, rate:0.812, acc:0.750 val loss: 1.300, r2: -0.150, rate:0.500, acc:0.375 lr 0.00607883272952847\n",
      "epoch:3 step:400 train loss: 0.743, r2: -3.165, rate:0.562, acc:0.188 val loss: 1.770, r2: 0.467, rate:0.750, acc:0.750 lr 0.00607883272952847\n",
      "epoch:3 step:500 train loss: 1.626, r2: -5.668, rate:0.438, acc:0.250 val loss: 0.591, r2: -1.208, rate:0.812, acc:0.812 lr 0.00607883272952847\n",
      "epoch:3 step:600 train loss: 0.800, r2: -0.253, rate:0.625, acc:0.250 val loss: 0.841, r2: -0.094, rate:0.500, acc:0.500 lr 0.00607883272952847\n",
      "epoch:3 step:700 train loss: 0.939, r2: 0.359, rate:0.625, acc:0.688 val loss: 0.945, r2: 0.150, rate:0.688, acc:0.688 lr 0.00607883272952847\n",
      "epoch:3 step:800 train loss: 4.910, r2: -2.841, rate:0.562, acc:0.312 val loss: 0.968, r2: -0.070, rate:0.750, acc:0.625 lr 0.00607883272952847\n",
      "epoch:3 step:900 train loss: 0.875, r2: 0.022, rate:0.562, acc:0.750 val loss: 1.498, r2: 0.182, rate:0.750, acc:0.750 lr 0.00607883272952847\n",
      "epoch:3 step:1000 train loss: 0.732, r2: 0.559, rate:0.750, acc:0.625 val loss: 1.105, r2: -0.924, rate:0.500, acc:0.625 lr 0.005470949456575623\n",
      "epoch:3 step:1100 train loss: 0.718, r2: 0.165, rate:0.500, acc:0.562 val loss: 0.987, r2: 0.413, rate:0.688, acc:0.812 lr 0.005470949456575623\n",
      "epoch:3 step:1200 train loss: 1.120, r2: -5.676, rate:0.625, acc:0.375 val loss: 1.842, r2: -2.898, rate:0.250, acc:0.062 lr 0.005470949456575623\n",
      "epoch:3 step:1300 train loss: 3.700, r2: -0.681, rate:0.875, acc:0.750 val loss: 1.090, r2: 0.423, rate:0.688, acc:0.688 lr 0.005470949456575623\n",
      "epoch:3 step:1400 train loss: 1.426, r2: -2.022, rate:0.938, acc:0.500 val loss: 0.670, r2: 0.504, rate:0.938, acc:0.688 lr 0.005470949456575623\n",
      "epoch:3 step:1500 train loss: 0.749, r2: -1.365, rate:0.625, acc:0.875 val loss: 0.840, r2: 0.366, rate:0.688, acc:0.812 lr 0.005470949456575623\n",
      "epoch:3 step:1600 train loss: 1.406, r2: -2.790, rate:0.562, acc:0.562 val loss: 1.437, r2: -0.260, rate:0.500, acc:0.500 lr 0.005470949456575623\n",
      "epoch:3 step:1700 train loss: 4.124, r2: -1.489, rate:0.688, acc:0.688 val loss: 1.772, r2: -0.816, rate:0.562, acc:0.375 lr 0.005470949456575623\n",
      "epoch:3 step:1800 train loss: 0.729, r2: 0.329, rate:0.875, acc:0.562 val loss: 0.958, r2: -0.039, rate:0.750, acc:0.625 lr 0.005470949456575623\n",
      "epoch:3 step:1900 train loss: 1.192, r2: -1.520, rate:0.875, acc:0.688 val loss: 0.577, r2: 0.582, rate:0.812, acc:0.750 lr 0.005470949456575623\n",
      "epoch:3 step:2000 train loss: 1.741, r2: -2.423, rate:0.688, acc:0.125 val loss: 0.866, r2: 0.001, rate:0.562, acc:0.875 lr 0.005470949456575623\n",
      "epoch:3 step:2100 train loss: 1.288, r2: -1.337, rate:0.812, acc:0.875 val loss: 0.586, r2: 0.264, rate:0.812, acc:0.812 lr 0.0049238545109180605\n",
      "epoch:3 step:2200 train loss: 1.576, r2: -1.521, rate:0.875, acc:0.938 val loss: 0.678, r2: 0.634, rate:0.812, acc:0.688 lr 0.0049238545109180605\n",
      "epoch:3 step:2300 train loss: 1.787, r2: -2.120, rate:0.812, acc:0.375 val loss: 0.856, r2: -2.220, rate:0.562, acc:0.750 lr 0.0049238545109180605\n",
      "epoch:3 step:2400 train loss: 1.610, r2: -0.303, rate:0.688, acc:0.500 val loss: 0.960, r2: -1.979, rate:0.562, acc:0.688 lr 0.0049238545109180605\n",
      "epoch:3 step:2500 train loss: 0.815, r2: -0.849, rate:0.438, acc:0.688 val loss: 0.625, r2: 0.222, rate:0.688, acc:0.625 lr 0.0049238545109180605\n",
      "epoch:3 step:2600 train loss: 0.803, r2: -0.278, rate:0.500, acc:0.750 val loss: 0.685, r2: 0.371, rate:0.750, acc:0.750 lr 0.0049238545109180605\n",
      "epoch:3 step:2700 train loss: 0.835, r2: -0.085, rate:0.812, acc:0.750 val loss: 0.536, r2: 0.607, rate:0.875, acc:0.750 lr 0.0049238545109180605\n",
      "epoch:3 step:2800 train loss: 0.805, r2: -0.204, rate:0.812, acc:0.562 val loss: 0.641, r2: 0.318, rate:0.812, acc:0.750 lr 0.0049238545109180605\n",
      "epoch:3 step:2900 train loss: 1.290, r2: -1.605, rate:0.625, acc:0.562 val loss: 1.190, r2: 0.239, rate:0.625, acc:0.750 lr 0.0049238545109180605\n",
      "epoch:3 step:3000 train loss: 2.885, r2: -0.412, rate:0.500, acc:0.375 val loss: 0.872, r2: 0.464, rate:0.750, acc:0.938 lr 0.0049238545109180605\n",
      "epoch:3 step:3100 train loss: 0.701, r2: 0.695, rate:0.688, acc:0.375 val loss: 0.541, r2: 0.699, rate:1.000, acc:0.812 lr 0.0049238545109180605\n",
      "epoch:3 step:3200 train loss: 1.778, r2: -9.576, rate:0.938, acc:1.000 val loss: 0.746, r2: -0.669, rate:0.562, acc:0.688 lr 0.004431469059826255\n",
      "epoch:3 step:3300 train loss: 0.461, r2: 0.829, rate:1.000, acc:0.875 val loss: 0.743, r2: 0.217, rate:0.625, acc:0.750 lr 0.004431469059826255\n",
      "epoch:3 step:3400 train loss: 0.910, r2: -3.471, rate:0.500, acc:0.125 val loss: 0.939, r2: 0.419, rate:0.625, acc:0.625 lr 0.004431469059826255\n",
      "epoch:3 step:3500 train loss: 0.762, r2: 0.661, rate:0.875, acc:0.750 val loss: 1.958, r2: -0.437, rate:0.438, acc:0.688 lr 0.004431469059826255\n",
      "epoch:3 step:3600 train loss: 0.837, r2: 0.129, rate:0.625, acc:0.625 val loss: 3.879, r2: -6.070, rate:0.312, acc:0.188 lr 0.004431469059826255\n",
      "epoch:3 step:3700 train loss: 0.860, r2: -0.681, rate:0.875, acc:0.750 val loss: 1.701, r2: -0.344, rate:0.562, acc:0.750 lr 0.004431469059826255\n",
      "epoch:3 step:3800 train loss: 1.016, r2: -2.303, rate:0.500, acc:0.500 val loss: 0.726, r2: 0.698, rate:0.938, acc:0.812 lr 0.004431469059826255\n",
      "epoch:3 step:3900 train loss: 0.815, r2: 0.083, rate:0.688, acc:0.625 val loss: 1.264, r2: -7.501, rate:0.625, acc:1.000 lr 0.004431469059826255\n",
      "epoch:3 step:4000 train loss: 0.843, r2: -1.223, rate:0.375, acc:0.125 val loss: 1.169, r2: -0.556, rate:0.625, acc:0.812 lr 0.004431469059826255\n",
      "epoch:3 step:4100 train loss: 0.988, r2: -0.109, rate:0.938, acc:0.812 val loss: 0.830, r2: -1.064, rate:0.688, acc:0.500 lr 0.004431469059826255\n",
      "epoch:3 step:4200 train loss: 0.930, r2: 0.368, rate:0.688, acc:0.625 val loss: 1.488, r2: -0.434, rate:0.312, acc:0.250 lr 0.004431469059826255\n",
      "epoch:3 step:4300 train loss: 0.801, r2: -1.352, rate:0.375, acc:0.812 val loss: 1.197, r2: -0.038, rate:0.562, acc:0.625 lr 0.0039883221538436296\n",
      "epoch:3 step:4400 train loss: 1.193, r2: -0.648, rate:0.688, acc:0.875 val loss: 2.636, r2: -2.891, rate:0.688, acc:0.875 lr 0.0039883221538436296\n",
      "epoch:3 step:4500 train loss: 2.623, r2: -2.305, rate:1.000, acc:0.938 val loss: 1.489, r2: 0.458, rate:0.750, acc:0.750 lr 0.0039883221538436296\n",
      "epoch:3 step:4600 train loss: 0.797, r2: -2.737, rate:0.625, acc:0.625 val loss: 0.886, r2: -0.152, rate:0.875, acc:0.875 lr 0.0039883221538436296\n",
      "epoch:3 step:4700 train loss: 11.297, r2: -10.151, rate:0.875, acc:1.000 val loss: 0.631, r2: 0.281, rate:0.938, acc:0.812 lr 0.0039883221538436296\n",
      "epoch:3 step:4800 train loss: 1.127, r2: -0.165, rate:0.375, acc:0.562 val loss: 1.209, r2: 0.363, rate:0.812, acc:0.812 lr 0.0039883221538436296\n",
      "epoch:3 step:4900 train loss: 0.726, r2: 0.513, rate:0.750, acc:0.812 val loss: 1.991, r2: -0.080, rate:0.500, acc:0.562 lr 0.0039883221538436296\n",
      "epoch:3 step:5000 train loss: 0.701, r2: 0.712, rate:0.688, acc:0.688 val loss: 1.115, r2: 0.006, rate:0.562, acc:0.625 lr 0.0039883221538436296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:5100 train loss: 0.607, r2: 0.395, rate:0.875, acc:0.812 val loss: 0.750, r2: 0.439, rate:0.875, acc:0.875 lr 0.0039883221538436296\n",
      "epoch:3 step:5200 train loss: 0.904, r2: -1.784, rate:0.562, acc:0.562 val loss: 0.915, r2: 0.266, rate:0.750, acc:0.688 lr 0.0039883221538436296\n",
      "epoch:3 step:5300 train loss: 0.920, r2: 0.642, rate:0.750, acc:0.688 val loss: 0.821, r2: -0.371, rate:0.625, acc:0.438 lr 0.0039883221538436296\n",
      "epoch:3 step:5400 train loss: 0.716, r2: 0.668, rate:0.750, acc:0.938 val loss: 1.621, r2: -7.940, rate:0.188, acc:0.625 lr 0.003589489938459267\n",
      "epoch:3 step:5500 train loss: 0.656, r2: -0.364, rate:0.562, acc:0.750 val loss: 0.703, r2: 0.334, rate:0.812, acc:0.812 lr 0.003589489938459267\n",
      "epoch:3 step:5600 train loss: 3.486, r2: -0.187, rate:0.812, acc:0.875 val loss: 0.997, r2: 0.055, rate:0.625, acc:0.875 lr 0.003589489938459267\n",
      "epoch:3 step:5700 train loss: 1.178, r2: -0.433, rate:0.688, acc:0.562 val loss: 0.724, r2: 0.575, rate:0.875, acc:0.688 lr 0.003589489938459267\n",
      "epoch:3 step:5800 train loss: 1.915, r2: -0.605, rate:0.875, acc:0.625 val loss: 0.980, r2: 0.211, rate:0.562, acc:0.562 lr 0.003589489938459267\n",
      "epoch:3 step:5900 train loss: 1.410, r2: 0.220, rate:0.812, acc:0.750 val loss: 1.915, r2: -9.422, rate:0.562, acc:0.625 lr 0.003589489938459267\n",
      "epoch:3 step:6000 train loss: 0.889, r2: -2.984, rate:0.125, acc:0.625 val loss: 1.491, r2: -0.910, rate:0.438, acc:0.562 lr 0.003589489938459267\n",
      "epoch:3 step:6100 train loss: 0.712, r2: 0.661, rate:0.688, acc:0.500 val loss: 0.661, r2: -2.505, rate:0.688, acc:0.812 lr 0.003589489938459267\n",
      "epoch:3 step:6200 train loss: 0.944, r2: -0.194, rate:0.625, acc:0.750 val loss: 1.071, r2: -1.473, rate:0.812, acc:0.625 lr 0.003589489938459267\n",
      "epoch:3 step:6300 train loss: 1.017, r2: -0.741, rate:0.500, acc:0.438 val loss: 1.100, r2: 0.407, rate:0.875, acc:0.688 lr 0.003589489938459267\n",
      "epoch:3 step:6400 train loss: 0.798, r2: -0.680, rate:0.688, acc:0.750 val loss: 1.046, r2: -2.596, rate:0.375, acc:0.812 lr 0.003589489938459267\n",
      "epoch:3 step:6500 train loss: 1.333, r2: -0.569, rate:0.562, acc:0.688 val loss: 0.606, r2: 0.293, rate:0.812, acc:0.688 lr 0.0032305409446133403\n",
      "epoch:3 step:6600 train loss: 0.733, r2: -4.220, rate:0.812, acc:0.688 val loss: 1.424, r2: 0.057, rate:0.812, acc:0.875 lr 0.0032305409446133403\n",
      "epoch:3 step:6700 train loss: 0.648, r2: 0.634, rate:0.875, acc:0.688 val loss: 2.024, r2: -0.333, rate:0.625, acc:0.688 lr 0.0032305409446133403\n",
      "epoch:3 step:6800 train loss: 0.723, r2: -0.072, rate:0.688, acc:0.562 val loss: 0.687, r2: 0.540, rate:0.812, acc:0.562 lr 0.0032305409446133403\n",
      "epoch:3 step:6900 train loss: 0.627, r2: -0.034, rate:0.750, acc:0.688 val loss: 0.827, r2: -0.150, rate:0.875, acc:0.688 lr 0.0032305409446133403\n",
      "epoch:3 step:7000 train loss: 0.724, r2: 0.702, rate:0.875, acc:0.438 val loss: 1.175, r2: -0.694, rate:0.375, acc:0.438 lr 0.0032305409446133403\n",
      "epoch:3 step:7100 train loss: 0.773, r2: 0.649, rate:0.812, acc:0.812 val loss: 0.609, r2: 0.208, rate:0.875, acc:0.625 lr 0.0032305409446133403\n",
      "epoch:3 step:7200 train loss: 0.627, r2: 0.367, rate:0.812, acc:0.438 val loss: 0.714, r2: 0.212, rate:0.625, acc:0.625 lr 0.0032305409446133403\n",
      "epoch:3 step:7300 train loss: 0.764, r2: -1.523, rate:0.625, acc:0.938 val loss: 0.903, r2: -1.110, rate:0.562, acc:0.688 lr 0.0032305409446133403\n",
      "epoch:3 step:7400 train loss: 0.860, r2: -0.543, rate:0.188, acc:0.500 val loss: 0.658, r2: 0.674, rate:0.875, acc:0.875 lr 0.0032305409446133403\n",
      "epoch:3 step:7500 train loss: 1.208, r2: -1.909, rate:0.375, acc:0.375 val loss: 1.199, r2: -1.188, rate:0.625, acc:0.438 lr 0.0032305409446133403\n",
      "epoch:3 step:7600 train loss: 1.348, r2: 0.329, rate:0.750, acc:0.688 val loss: 0.721, r2: 0.549, rate:0.812, acc:0.688 lr 0.0029074868501520064\n",
      "epoch:3 step:7700 train loss: 1.434, r2: -1.460, rate:0.562, acc:0.312 val loss: 0.858, r2: -0.240, rate:0.625, acc:0.375 lr 0.0029074868501520064\n",
      "epoch:3 step:7800 train loss: 0.584, r2: -0.063, rate:0.938, acc:0.688 val loss: 0.844, r2: 0.661, rate:0.688, acc:0.625 lr 0.0029074868501520064\n",
      "epoch:3 step:0.305 lr 0.002907\n",
      "epoch:4 step:0 train loss: 0.852, r2: 0.483, rate:0.500, acc:0.438 val loss: 1.930, r2: 0.166, rate:0.938, acc:0.875 lr 0.0029074868501520064\n",
      "epoch:4 step:100 train loss: 1.046, r2: 0.050, rate:0.375, acc:0.750 val loss: 1.929, r2: 0.072, rate:0.562, acc:0.562 lr 0.0029074868501520064\n",
      "epoch:4 step:200 train loss: 1.645, r2: 0.110, rate:0.500, acc:0.562 val loss: 0.678, r2: 0.647, rate:0.750, acc:0.812 lr 0.0029074868501520064\n",
      "epoch:4 step:300 train loss: 0.647, r2: -1.309, rate:0.438, acc:1.000 val loss: 16.838, r2: -6.317, rate:0.625, acc:0.750 lr 0.0029074868501520064\n",
      "epoch:4 step:400 train loss: 1.435, r2: -1.614, rate:0.438, acc:0.188 val loss: 0.976, r2: 0.246, rate:0.562, acc:0.625 lr 0.0029074868501520064\n",
      "epoch:4 step:500 train loss: 0.654, r2: 0.499, rate:0.625, acc:0.875 val loss: 1.600, r2: -1.836, rate:0.688, acc:0.812 lr 0.0029074868501520064\n",
      "epoch:4 step:600 train loss: 1.081, r2: 0.027, rate:0.688, acc:0.812 val loss: 1.337, r2: -0.011, rate:0.750, acc:0.875 lr 0.0029074868501520064\n",
      "epoch:4 step:700 train loss: 0.807, r2: 0.333, rate:0.500, acc:0.438 val loss: 0.757, r2: -0.149, rate:0.750, acc:0.750 lr 0.0029074868501520064\n",
      "epoch:4 step:800 train loss: 2.242, r2: 0.041, rate:0.688, acc:0.500 val loss: 1.266, r2: -2.738, rate:0.625, acc:0.875 lr 0.0026167381651368057\n",
      "epoch:4 step:900 train loss: 0.763, r2: 0.262, rate:0.625, acc:0.375 val loss: 1.682, r2: 0.581, rate:0.938, acc:0.688 lr 0.0026167381651368057\n",
      "epoch:4 step:1000 train loss: 1.249, r2: -4.137, rate:0.375, acc:0.625 val loss: 0.883, r2: 0.530, rate:0.500, acc:0.562 lr 0.0026167381651368057\n",
      "epoch:4 step:1100 train loss: 4.149, r2: 0.443, rate:0.625, acc:0.812 val loss: 0.813, r2: 0.256, rate:0.688, acc:0.688 lr 0.0026167381651368057\n",
      "epoch:4 step:1200 train loss: 1.123, r2: -0.420, rate:0.562, acc:0.812 val loss: 0.954, r2: -0.185, rate:0.500, acc:0.562 lr 0.0026167381651368057\n",
      "epoch:4 step:1300 train loss: 1.539, r2: -0.186, rate:0.562, acc:0.250 val loss: 0.712, r2: -0.724, rate:0.750, acc:0.688 lr 0.0026167381651368057\n",
      "epoch:4 step:1400 train loss: 1.786, r2: -0.918, rate:0.375, acc:0.375 val loss: 0.575, r2: 0.486, rate:0.812, acc:1.000 lr 0.0026167381651368057\n",
      "epoch:4 step:1500 train loss: 0.667, r2: 0.346, rate:0.750, acc:0.438 val loss: 1.221, r2: -1.077, rate:0.938, acc:0.688 lr 0.0026167381651368057\n",
      "epoch:4 step:1600 train loss: 0.665, r2: -0.124, rate:0.500, acc:0.188 val loss: 1.202, r2: -2.396, rate:0.438, acc:0.688 lr 0.0026167381651368057\n",
      "epoch:4 step:1700 train loss: 0.702, r2: 0.634, rate:0.812, acc:0.688 val loss: 0.829, r2: 0.219, rate:0.625, acc:0.625 lr 0.0026167381651368057\n",
      "epoch:4 step:1800 train loss: 1.454, r2: -3.294, rate:0.438, acc:0.125 val loss: 1.039, r2: 0.714, rate:0.812, acc:0.688 lr 0.0026167381651368057\n",
      "epoch:4 step:1900 train loss: 1.745, r2: -0.511, rate:0.625, acc:0.625 val loss: 1.213, r2: -1.070, rate:0.562, acc:0.625 lr 0.002355064348623125\n",
      "epoch:4 step:2000 train loss: 3.048, r2: -0.748, rate:0.562, acc:0.750 val loss: 1.813, r2: -0.553, rate:0.500, acc:0.562 lr 0.002355064348623125\n",
      "epoch:4 step:2100 train loss: 0.628, r2: 0.338, rate:0.812, acc:0.562 val loss: 1.520, r2: -1.750, rate:0.625, acc:0.688 lr 0.002355064348623125\n",
      "epoch:4 step:2200 train loss: 0.768, r2: 0.574, rate:0.812, acc:0.438 val loss: 0.579, r2: -0.856, rate:0.875, acc:0.812 lr 0.002355064348623125\n",
      "epoch:4 step:2300 train loss: 0.506, r2: 0.505, rate:0.875, acc:0.812 val loss: 1.709, r2: -0.026, rate:0.812, acc:0.750 lr 0.002355064348623125\n",
      "epoch:4 step:2400 train loss: 0.632, r2: 0.439, rate:0.875, acc:0.812 val loss: 0.855, r2: 0.183, rate:0.500, acc:0.438 lr 0.002355064348623125\n",
      "epoch:4 step:2500 train loss: 0.513, r2: 0.208, rate:0.812, acc:0.875 val loss: 1.424, r2: -3.410, rate:0.562, acc:0.500 lr 0.002355064348623125\n",
      "epoch:4 step:2600 train loss: 0.692, r2: -0.123, rate:1.000, acc:0.938 val loss: 1.496, r2: -0.484, rate:0.750, acc:0.812 lr 0.002355064348623125\n",
      "epoch:4 step:2700 train loss: 2.130, r2: -1.175, rate:0.875, acc:0.375 val loss: 1.286, r2: -0.414, rate:0.438, acc:0.812 lr 0.002355064348623125\n",
      "epoch:4 step:2800 train loss: 0.800, r2: -0.220, rate:0.750, acc:0.875 val loss: 0.596, r2: 0.270, rate:0.750, acc:0.625 lr 0.002355064348623125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:2900 train loss: 0.911, r2: -0.879, rate:0.625, acc:0.750 val loss: 1.016, r2: 0.174, rate:0.688, acc:0.188 lr 0.002355064348623125\n",
      "epoch:4 step:3000 train loss: 0.655, r2: 0.383, rate:0.812, acc:0.625 val loss: 0.625, r2: 0.694, rate:0.750, acc:0.500 lr 0.002119557913760813\n",
      "epoch:4 step:3100 train loss: 0.802, r2: -0.045, rate:0.562, acc:0.562 val loss: 0.656, r2: -1.736, rate:0.812, acc:0.938 lr 0.002119557913760813\n",
      "epoch:4 step:3200 train loss: 0.806, r2: -1.659, rate:0.500, acc:0.688 val loss: 0.901, r2: -0.315, rate:0.625, acc:0.500 lr 0.002119557913760813\n",
      "epoch:4 step:3300 train loss: 1.101, r2: -2.898, rate:0.438, acc:0.375 val loss: 1.550, r2: -0.153, rate:0.688, acc:0.875 lr 0.002119557913760813\n",
      "epoch:4 step:3400 train loss: 0.720, r2: -0.455, rate:0.438, acc:0.188 val loss: 0.979, r2: 0.306, rate:0.438, acc:0.438 lr 0.002119557913760813\n",
      "epoch:4 step:3500 train loss: 1.081, r2: 0.458, rate:0.875, acc:0.938 val loss: 0.866, r2: -0.450, rate:0.625, acc:0.812 lr 0.002119557913760813\n",
      "epoch:4 step:3600 train loss: 0.707, r2: -0.296, rate:0.562, acc:0.438 val loss: 0.838, r2: 0.507, rate:0.500, acc:0.812 lr 0.002119557913760813\n",
      "epoch:4 step:3700 train loss: 0.749, r2: -0.038, rate:0.812, acc:0.500 val loss: 0.789, r2: 0.240, rate:0.625, acc:0.750 lr 0.002119557913760813\n",
      "epoch:4 step:3800 train loss: 2.170, r2: 0.326, rate:0.625, acc:0.625 val loss: 2.768, r2: 0.466, rate:0.938, acc:0.812 lr 0.002119557913760813\n",
      "epoch:4 step:3900 train loss: 1.106, r2: 0.161, rate:0.562, acc:0.625 val loss: 0.812, r2: -0.019, rate:0.750, acc:0.750 lr 0.002119557913760813\n",
      "epoch:4 step:4000 train loss: 0.784, r2: 0.237, rate:0.750, acc:0.688 val loss: 0.845, r2: -0.429, rate:0.750, acc:0.500 lr 0.002119557913760813\n",
      "epoch:4 step:4100 train loss: 1.248, r2: -0.519, rate:0.812, acc:0.812 val loss: 0.657, r2: 0.290, rate:0.875, acc:0.875 lr 0.0019076021223847317\n",
      "epoch:4 step:4200 train loss: 0.782, r2: -1.282, rate:0.562, acc:0.875 val loss: 0.765, r2: -0.526, rate:0.688, acc:0.688 lr 0.0019076021223847317\n",
      "epoch:4 step:4300 train loss: 0.549, r2: 0.597, rate:0.812, acc:0.875 val loss: 0.796, r2: 0.277, rate:0.688, acc:0.562 lr 0.0019076021223847317\n",
      "epoch:4 step:4400 train loss: 0.760, r2: -0.802, rate:0.625, acc:0.500 val loss: 0.893, r2: -0.069, rate:0.562, acc:0.562 lr 0.0019076021223847317\n",
      "epoch:4 step:4500 train loss: 1.750, r2: -0.996, rate:0.750, acc:0.688 val loss: 1.197, r2: -1.804, rate:0.500, acc:0.500 lr 0.0019076021223847317\n",
      "epoch:4 step:4600 train loss: 1.084, r2: -1.531, rate:0.625, acc:0.625 val loss: 1.102, r2: 0.433, rate:0.875, acc:0.688 lr 0.0019076021223847317\n",
      "epoch:4 step:4700 train loss: 2.964, r2: 0.342, rate:0.875, acc:0.812 val loss: 2.648, r2: 0.271, rate:0.938, acc:1.000 lr 0.0019076021223847317\n",
      "epoch:4 step:4800 train loss: 1.059, r2: -1.366, rate:0.688, acc:0.312 val loss: 3.523, r2: -1.172, rate:0.750, acc:0.875 lr 0.0019076021223847317\n",
      "epoch:4 step:4900 train loss: 0.936, r2: -0.692, rate:0.438, acc:0.688 val loss: 1.169, r2: -0.360, rate:0.750, acc:0.812 lr 0.0019076021223847317\n",
      "epoch:4 step:5000 train loss: 0.892, r2: 0.008, rate:0.438, acc:0.625 val loss: 2.162, r2: 0.442, rate:0.750, acc:0.812 lr 0.0019076021223847317\n",
      "epoch:4 step:5100 train loss: 1.210, r2: -2.153, rate:0.500, acc:0.188 val loss: 0.855, r2: -0.232, rate:0.562, acc:0.500 lr 0.0019076021223847317\n",
      "epoch:4 step:5200 train loss: 0.678, r2: 0.177, rate:0.750, acc:0.688 val loss: 0.796, r2: -0.132, rate:0.562, acc:0.562 lr 0.0017168419101462585\n",
      "epoch:4 step:5300 train loss: 0.807, r2: -0.004, rate:0.688, acc:0.875 val loss: 1.169, r2: 0.172, rate:0.500, acc:0.500 lr 0.0017168419101462585\n",
      "epoch:4 step:5400 train loss: 0.587, r2: 0.171, rate:0.938, acc:0.438 val loss: 0.802, r2: 0.599, rate:0.938, acc:0.562 lr 0.0017168419101462585\n",
      "epoch:4 step:5500 train loss: 0.706, r2: -0.641, rate:0.688, acc:0.875 val loss: 0.843, r2: -0.739, rate:0.875, acc:0.875 lr 0.0017168419101462585\n",
      "epoch:4 step:5600 train loss: 4.623, r2: -9.231, rate:0.625, acc:0.812 val loss: 0.747, r2: -0.081, rate:0.500, acc:0.812 lr 0.0017168419101462585\n",
      "epoch:4 step:5700 train loss: 0.683, r2: 0.031, rate:0.625, acc:0.750 val loss: 1.323, r2: -1.567, rate:0.375, acc:0.250 lr 0.0017168419101462585\n",
      "epoch:4 step:5800 train loss: 2.238, r2: -0.122, rate:0.562, acc:0.625 val loss: 1.022, r2: 0.109, rate:0.875, acc:0.812 lr 0.0017168419101462585\n",
      "epoch:4 step:5900 train loss: 2.155, r2: -0.923, rate:0.375, acc:0.812 val loss: 0.787, r2: 0.480, rate:0.750, acc:0.812 lr 0.0017168419101462585\n",
      "epoch:4 step:6000 train loss: 4.744, r2: -4.580, rate:0.312, acc:0.625 val loss: 1.647, r2: -1.253, rate:0.812, acc:0.500 lr 0.0017168419101462585\n",
      "epoch:4 step:6100 train loss: 0.728, r2: -2.379, rate:0.688, acc:0.562 val loss: 1.153, r2: 0.669, rate:0.688, acc:0.688 lr 0.0017168419101462585\n",
      "epoch:4 step:6200 train loss: 0.993, r2: -4.058, rate:0.312, acc:0.188 val loss: 1.238, r2: -1.182, rate:0.562, acc:0.250 lr 0.0017168419101462585\n",
      "epoch:4 step:6300 train loss: 0.786, r2: -1.214, rate:0.625, acc:0.500 val loss: 0.766, r2: 0.082, rate:0.688, acc:0.812 lr 0.0015451577191316326\n",
      "epoch:4 step:6400 train loss: 1.053, r2: -0.079, rate:0.562, acc:0.562 val loss: 0.708, r2: 0.467, rate:0.750, acc:0.750 lr 0.0015451577191316326\n",
      "epoch:4 step:6500 train loss: 1.969, r2: -0.088, rate:0.750, acc:0.625 val loss: 0.756, r2: -1.674, rate:0.562, acc:0.500 lr 0.0015451577191316326\n",
      "epoch:4 step:6600 train loss: 1.142, r2: 0.257, rate:0.750, acc:0.500 val loss: 1.435, r2: -0.331, rate:0.562, acc:0.375 lr 0.0015451577191316326\n",
      "epoch:4 step:6700 train loss: 4.347, r2: -5.128, rate:0.438, acc:0.250 val loss: 1.359, r2: -1.935, rate:0.375, acc:0.188 lr 0.0015451577191316326\n",
      "epoch:4 step:6800 train loss: 0.775, r2: 0.525, rate:0.562, acc:0.500 val loss: 0.964, r2: -0.503, rate:0.625, acc:0.750 lr 0.0015451577191316326\n",
      "epoch:4 step:6900 train loss: 0.737, r2: -1.022, rate:0.625, acc:0.812 val loss: 1.390, r2: 0.521, rate:0.812, acc:0.750 lr 0.0015451577191316326\n",
      "epoch:4 step:7000 train loss: 0.546, r2: 0.789, rate:0.875, acc:0.688 val loss: 1.219, r2: 0.282, rate:0.562, acc:0.625 lr 0.0015451577191316326\n",
      "epoch:4 step:7100 train loss: 3.553, r2: -1.061, rate:0.438, acc:0.625 val loss: 1.658, r2: -0.749, rate:0.562, acc:0.500 lr 0.0015451577191316326\n",
      "epoch:4 step:7200 train loss: 0.691, r2: -0.655, rate:0.562, acc:0.625 val loss: 1.744, r2: 0.189, rate:0.438, acc:0.438 lr 0.0015451577191316326\n",
      "epoch:4 step:7300 train loss: 0.909, r2: -0.111, rate:0.625, acc:0.562 val loss: 1.576, r2: -0.087, rate:0.688, acc:0.438 lr 0.0015451577191316326\n",
      "epoch:4 step:7400 train loss: 6.810, r2: -4.178, rate:0.750, acc:0.938 val loss: 1.690, r2: -5.382, rate:0.562, acc:0.312 lr 0.0013906419472184694\n",
      "epoch:4 step:7500 train loss: 0.749, r2: -0.447, rate:0.812, acc:0.875 val loss: 0.827, r2: 0.169, rate:0.688, acc:0.625 lr 0.0013906419472184694\n",
      "epoch:4 step:7600 train loss: 5.787, r2: -12.069, rate:0.750, acc:0.500 val loss: 3.652, r2: -1.603, rate:0.562, acc:0.438 lr 0.0013906419472184694\n",
      "epoch:4 step:7700 train loss: 0.654, r2: 0.059, rate:0.938, acc:0.875 val loss: 1.576, r2: -0.725, rate:0.625, acc:0.312 lr 0.0013906419472184694\n",
      "epoch:4 step:7800 train loss: 1.869, r2: 0.391, rate:0.688, acc:0.625 val loss: 0.640, r2: 0.357, rate:0.688, acc:0.438 lr 0.0013906419472184694\n",
      "epoch:4 step:0.308 lr 0.001391\n",
      "epoch:5 step:0 train loss: 0.684, r2: -0.053, rate:0.562, acc:0.562 val loss: 1.092, r2: 0.376, rate:0.625, acc:0.625 lr 0.0013906419472184694\n",
      "epoch:5 step:100 train loss: 0.573, r2: 0.446, rate:0.875, acc:0.750 val loss: 0.817, r2: -1.381, rate:0.625, acc:0.375 lr 0.0013906419472184694\n",
      "epoch:5 step:200 train loss: 1.115, r2: 0.198, rate:0.562, acc:0.625 val loss: 0.838, r2: 0.123, rate:0.688, acc:0.812 lr 0.0013906419472184694\n",
      "epoch:5 step:300 train loss: 0.738, r2: -0.220, rate:0.625, acc:0.562 val loss: 0.803, r2: -0.919, rate:0.812, acc:0.750 lr 0.0013906419472184694\n",
      "epoch:5 step:400 train loss: 0.823, r2: 0.128, rate:0.812, acc:0.750 val loss: 0.742, r2: 0.523, rate:0.812, acc:0.812 lr 0.0013906419472184694\n",
      "epoch:5 step:500 train loss: 0.653, r2: 0.398, rate:0.750, acc:0.750 val loss: 1.431, r2: -0.102, rate:0.438, acc:0.500 lr 0.0013906419472184694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:600 train loss: 1.161, r2: -3.243, rate:0.562, acc:0.875 val loss: 1.120, r2: -2.390, rate:0.688, acc:0.562 lr 0.0012515777524966226\n",
      "epoch:5 step:700 train loss: 0.607, r2: 0.655, rate:0.812, acc:0.438 val loss: 0.924, r2: -1.404, rate:0.562, acc:0.875 lr 0.0012515777524966226\n",
      "epoch:5 step:800 train loss: 1.065, r2: -0.091, rate:0.438, acc:0.062 val loss: 0.842, r2: 0.016, rate:0.500, acc:0.312 lr 0.0012515777524966226\n",
      "epoch:5 step:900 train loss: 0.825, r2: -0.779, rate:0.562, acc:0.375 val loss: 1.236, r2: 0.437, rate:0.562, acc:0.500 lr 0.0012515777524966226\n",
      "epoch:5 step:1000 train loss: 0.914, r2: 0.534, rate:0.625, acc:0.375 val loss: 0.692, r2: 0.491, rate:0.688, acc:0.812 lr 0.0012515777524966226\n",
      "epoch:5 step:1100 train loss: 1.875, r2: -0.163, rate:0.938, acc:0.938 val loss: 1.109, r2: 0.445, rate:0.688, acc:0.688 lr 0.0012515777524966226\n",
      "epoch:5 step:1200 train loss: 1.696, r2: -0.391, rate:0.812, acc:0.750 val loss: 2.149, r2: -6.203, rate:0.875, acc:0.688 lr 0.0012515777524966226\n",
      "epoch:5 step:1300 train loss: 1.679, r2: -8.792, rate:0.438, acc:0.125 val loss: 0.971, r2: 0.398, rate:0.875, acc:0.625 lr 0.0012515777524966226\n",
      "epoch:5 step:1400 train loss: 0.936, r2: -0.065, rate:0.562, acc:0.562 val loss: 0.721, r2: 0.307, rate:0.688, acc:0.375 lr 0.0012515777524966226\n",
      "epoch:5 step:1500 train loss: 0.830, r2: -0.489, rate:0.562, acc:0.562 val loss: 1.044, r2: -0.211, rate:0.688, acc:0.812 lr 0.0012515777524966226\n",
      "epoch:5 step:1600 train loss: 0.329, r2: 0.737, rate:1.000, acc:0.938 val loss: 1.462, r2: -0.158, rate:0.688, acc:0.688 lr 0.0012515777524966226\n",
      "epoch:5 step:1700 train loss: 0.972, r2: 0.000, rate:0.750, acc:0.500 val loss: 0.687, r2: 0.584, rate:0.750, acc:0.625 lr 0.0011264199772469603\n",
      "epoch:5 step:1800 train loss: 0.670, r2: 0.075, rate:0.688, acc:0.438 val loss: 0.818, r2: 0.648, rate:0.688, acc:0.625 lr 0.0011264199772469603\n",
      "epoch:5 step:1900 train loss: 0.692, r2: -4.308, rate:0.438, acc:0.688 val loss: 1.197, r2: -0.173, rate:0.500, acc:0.438 lr 0.0011264199772469603\n",
      "epoch:5 step:2000 train loss: 3.052, r2: -3.503, rate:0.625, acc:0.875 val loss: 0.832, r2: 0.495, rate:0.750, acc:0.750 lr 0.0011264199772469603\n",
      "epoch:5 step:2100 train loss: 1.185, r2: -0.361, rate:0.625, acc:0.688 val loss: 1.211, r2: 0.013, rate:0.375, acc:0.688 lr 0.0011264199772469603\n",
      "epoch:5 step:2200 train loss: 3.850, r2: -23.411, rate:0.875, acc:0.625 val loss: 0.988, r2: -1.230, rate:0.562, acc:0.188 lr 0.0011264199772469603\n",
      "epoch:5 step:2300 train loss: 0.813, r2: -0.125, rate:0.750, acc:0.688 val loss: 1.092, r2: -0.780, rate:0.562, acc:0.438 lr 0.0011264199772469603\n",
      "epoch:5 step:2400 train loss: 0.592, r2: 0.880, rate:0.750, acc:0.500 val loss: 3.719, r2: -2.972, rate:0.562, acc:0.438 lr 0.0011264199772469603\n",
      "epoch:5 step:2500 train loss: 0.927, r2: -1.753, rate:0.438, acc:0.438 val loss: 0.534, r2: 0.672, rate:0.875, acc:0.812 lr 0.0011264199772469603\n",
      "epoch:5 step:2600 train loss: 0.664, r2: 0.077, rate:0.812, acc:0.875 val loss: 0.810, r2: 0.084, rate:0.812, acc:0.938 lr 0.0011264199772469603\n",
      "epoch:5 step:2700 train loss: 0.587, r2: 0.590, rate:0.750, acc:0.750 val loss: 0.841, r2: 0.652, rate:0.812, acc:0.688 lr 0.0011264199772469603\n",
      "epoch:5 step:2800 train loss: 0.579, r2: 0.783, rate:0.875, acc:0.312 val loss: 0.607, r2: 0.574, rate:0.938, acc:0.875 lr 0.0010137779795222643\n",
      "epoch:5 step:2900 train loss: 1.645, r2: -0.803, rate:0.500, acc:0.562 val loss: 0.978, r2: -0.507, rate:0.562, acc:0.500 lr 0.0010137779795222643\n",
      "epoch:5 step:3000 train loss: 0.649, r2: 0.232, rate:0.625, acc:0.625 val loss: 0.648, r2: 0.542, rate:0.688, acc:0.750 lr 0.0010137779795222643\n",
      "epoch:5 step:3100 train loss: 1.003, r2: 0.085, rate:0.562, acc:0.500 val loss: 0.565, r2: 0.676, rate:0.750, acc:0.688 lr 0.0010137779795222643\n",
      "epoch:5 step:3200 train loss: 0.817, r2: -2.723, rate:0.438, acc:0.312 val loss: 1.707, r2: -0.797, rate:0.562, acc:0.375 lr 0.0010137779795222643\n",
      "epoch:5 step:3300 train loss: 1.484, r2: -1.226, rate:0.500, acc:0.250 val loss: 0.800, r2: 0.170, rate:0.688, acc:0.312 lr 0.0010137779795222643\n",
      "epoch:5 step:3400 train loss: 0.824, r2: -1.867, rate:0.562, acc:0.812 val loss: 1.147, r2: -0.319, rate:0.812, acc:0.562 lr 0.0010137779795222643\n",
      "epoch:5 step:3500 train loss: 0.773, r2: -4.486, rate:0.312, acc:0.562 val loss: 0.738, r2: 0.483, rate:0.875, acc:0.562 lr 0.0010137779795222643\n",
      "epoch:5 step:3600 train loss: 0.628, r2: -0.341, rate:0.688, acc:0.688 val loss: 0.656, r2: 0.431, rate:0.875, acc:0.750 lr 0.0010137779795222643\n",
      "epoch:5 step:3700 train loss: 0.825, r2: -0.208, rate:0.812, acc:0.875 val loss: 0.649, r2: 0.617, rate:0.750, acc:0.562 lr 0.0010137779795222643\n",
      "epoch:5 step:3800 train loss: 2.954, r2: -0.516, rate:0.625, acc:0.812 val loss: 1.041, r2: 0.492, rate:0.938, acc:0.875 lr 0.0010137779795222643\n",
      "epoch:5 step:3900 train loss: 1.092, r2: -0.283, rate:1.000, acc:1.000 val loss: 0.623, r2: 0.634, rate:0.875, acc:0.562 lr 0.0009124001815700379\n",
      "epoch:5 step:4000 train loss: 0.766, r2: -0.852, rate:0.562, acc:0.875 val loss: 0.718, r2: 0.609, rate:0.812, acc:0.750 lr 0.0009124001815700379\n",
      "epoch:5 step:4100 train loss: 0.876, r2: 0.300, rate:0.875, acc:0.438 val loss: 0.924, r2: -0.149, rate:0.500, acc:0.438 lr 0.0009124001815700379\n",
      "epoch:5 step:4200 train loss: 0.801, r2: -1.468, rate:0.438, acc:0.625 val loss: 0.900, r2: -0.711, rate:0.438, acc:0.688 lr 0.0009124001815700379\n",
      "epoch:5 step:4300 train loss: 1.038, r2: -0.996, rate:0.750, acc:0.750 val loss: 1.023, r2: 0.100, rate:0.562, acc:0.750 lr 0.0009124001815700379\n",
      "epoch:5 step:4400 train loss: 0.779, r2: -0.087, rate:0.750, acc:0.562 val loss: 0.730, r2: 0.517, rate:0.750, acc:0.500 lr 0.0009124001815700379\n",
      "epoch:5 step:4500 train loss: 1.192, r2: -1.352, rate:0.875, acc:0.812 val loss: 0.718, r2: 0.268, rate:0.938, acc:0.812 lr 0.0009124001815700379\n",
      "epoch:5 step:4600 train loss: 0.519, r2: 0.337, rate:0.938, acc:0.750 val loss: 1.038, r2: -0.629, rate:0.875, acc:0.938 lr 0.0009124001815700379\n",
      "epoch:5 step:4700 train loss: 1.096, r2: 0.501, rate:0.812, acc:0.812 val loss: 0.725, r2: 0.437, rate:0.812, acc:0.750 lr 0.0009124001815700379\n",
      "epoch:5 step:4800 train loss: 0.576, r2: 0.614, rate:1.000, acc:0.875 val loss: 0.769, r2: -0.334, rate:0.500, acc:0.625 lr 0.0009124001815700379\n",
      "epoch:5 step:4900 train loss: 2.189, r2: -1.244, rate:0.438, acc:0.625 val loss: 1.030, r2: 0.121, rate:0.500, acc:0.562 lr 0.0009124001815700379\n",
      "epoch:5 step:5000 train loss: 0.659, r2: -0.030, rate:0.688, acc:0.875 val loss: 0.835, r2: 0.241, rate:0.750, acc:0.688 lr 0.0008211601634130341\n",
      "epoch:5 step:5100 train loss: 0.885, r2: -6.734, rate:0.438, acc:0.625 val loss: 0.991, r2: -0.205, rate:0.688, acc:0.375 lr 0.0008211601634130341\n",
      "epoch:5 step:5200 train loss: 1.160, r2: -0.093, rate:0.750, acc:0.375 val loss: 0.679, r2: 0.434, rate:0.812, acc:0.875 lr 0.0008211601634130341\n",
      "epoch:5 step:5300 train loss: 1.107, r2: -0.360, rate:0.875, acc:0.812 val loss: 0.693, r2: 0.419, rate:0.812, acc:0.688 lr 0.0008211601634130341\n",
      "epoch:5 step:5400 train loss: 0.809, r2: -0.582, rate:0.562, acc:0.375 val loss: 0.877, r2: -0.423, rate:0.562, acc:0.625 lr 0.0008211601634130341\n",
      "epoch:5 step:5500 train loss: 1.229, r2: -0.023, rate:0.625, acc:0.812 val loss: 5.502, r2: -9.991, rate:0.250, acc:0.625 lr 0.0008211601634130341\n",
      "epoch:5 step:5600 train loss: 0.654, r2: 0.723, rate:0.812, acc:0.938 val loss: 1.722, r2: -0.192, rate:0.625, acc:0.562 lr 0.0008211601634130341\n",
      "epoch:5 step:5700 train loss: 3.690, r2: -4.902, rate:0.812, acc:0.688 val loss: 0.734, r2: 0.250, rate:0.812, acc:0.875 lr 0.0008211601634130341\n",
      "epoch:5 step:5800 train loss: 1.132, r2: -0.074, rate:0.562, acc:0.750 val loss: 0.968, r2: -0.085, rate:0.750, acc:0.750 lr 0.0008211601634130341\n",
      "epoch:5 step:5900 train loss: 1.256, r2: -0.190, rate:0.625, acc:0.562 val loss: 1.076, r2: 0.280, rate:0.812, acc:0.688 lr 0.0008211601634130341\n",
      "epoch:5 step:6000 train loss: 0.584, r2: 0.537, rate:0.812, acc:0.562 val loss: 0.869, r2: -0.421, rate:0.688, acc:0.625 lr 0.0008211601634130341\n",
      "epoch:5 step:6100 train loss: 0.840, r2: 0.348, rate:0.750, acc:0.562 val loss: 0.773, r2: -0.816, rate:0.688, acc:0.938 lr 0.0007390441470717307\n",
      "epoch:5 step:6200 train loss: 0.845, r2: -1.752, rate:0.625, acc:0.438 val loss: 1.195, r2: -0.001, rate:0.625, acc:0.500 lr 0.0007390441470717307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:6300 train loss: 0.888, r2: 0.483, rate:0.562, acc:0.562 val loss: 0.780, r2: -0.564, rate:0.750, acc:0.562 lr 0.0007390441470717307\n",
      "epoch:5 step:6400 train loss: 0.658, r2: 0.534, rate:0.688, acc:0.688 val loss: 1.472, r2: -0.189, rate:0.500, acc:0.438 lr 0.0007390441470717307\n",
      "epoch:5 step:6500 train loss: 0.900, r2: 0.634, rate:0.688, acc:0.812 val loss: 1.477, r2: -0.775, rate:0.625, acc:0.688 lr 0.0007390441470717307\n",
      "epoch:5 step:6600 train loss: 0.954, r2: -0.374, rate:0.500, acc:0.625 val loss: 0.717, r2: 0.051, rate:0.500, acc:0.875 lr 0.0007390441470717307\n",
      "epoch:5 step:6700 train loss: 0.476, r2: 0.843, rate:0.938, acc:0.938 val loss: 0.975, r2: 0.125, rate:0.625, acc:0.625 lr 0.0007390441470717307\n",
      "epoch:5 step:6800 train loss: 0.748, r2: -0.797, rate:0.688, acc:0.562 val loss: 1.162, r2: -1.770, rate:0.500, acc:0.250 lr 0.0007390441470717307\n",
      "epoch:5 step:6900 train loss: 0.717, r2: 0.452, rate:0.812, acc:0.875 val loss: 0.933, r2: 0.281, rate:0.438, acc:0.500 lr 0.0007390441470717307\n",
      "epoch:5 step:7000 train loss: 0.835, r2: -2.038, rate:0.812, acc:0.938 val loss: 0.807, r2: 0.582, rate:1.000, acc:0.938 lr 0.0007390441470717307\n",
      "epoch:5 step:7100 train loss: 0.960, r2: -0.018, rate:0.875, acc:0.750 val loss: 1.884, r2: -0.083, rate:0.500, acc:0.438 lr 0.0007390441470717307\n",
      "epoch:5 step:7200 train loss: 2.026, r2: -1.722, rate:0.688, acc:0.500 val loss: 1.050, r2: 0.021, rate:0.500, acc:0.562 lr 0.0006651397323645576\n",
      "epoch:5 step:7300 train loss: 0.793, r2: 0.039, rate:0.750, acc:0.500 val loss: 0.767, r2: -0.089, rate:0.812, acc:0.500 lr 0.0006651397323645576\n",
      "epoch:5 step:7400 train loss: 1.092, r2: -0.214, rate:0.375, acc:0.812 val loss: 0.639, r2: 0.292, rate:0.875, acc:0.875 lr 0.0006651397323645576\n",
      "epoch:5 step:7500 train loss: 0.848, r2: 0.306, rate:0.812, acc:0.812 val loss: 0.834, r2: 0.443, rate:0.750, acc:0.875 lr 0.0006651397323645576\n",
      "epoch:5 step:7600 train loss: 1.521, r2: -0.416, rate:0.438, acc:0.625 val loss: 0.816, r2: 0.619, rate:0.812, acc:0.812 lr 0.0006651397323645576\n",
      "epoch:5 step:7700 train loss: 0.753, r2: 0.640, rate:0.562, acc:0.688 val loss: 0.702, r2: 0.406, rate:0.750, acc:0.562 lr 0.0006651397323645576\n",
      "epoch:5 step:7800 train loss: 1.036, r2: -1.949, rate:0.562, acc:0.312 val loss: 1.294, r2: -1.247, rate:0.625, acc:0.250 lr 0.0006651397323645576\n",
      "epoch:5 step:0.331 lr 0.000665\n",
      "epoch:6 step:0 train loss: 0.538, r2: 0.709, rate:0.938, acc:0.688 val loss: 0.755, r2: 0.125, rate:0.812, acc:0.875 lr 0.0006651397323645576\n",
      "epoch:6 step:100 train loss: 1.384, r2: 0.026, rate:0.625, acc:0.938 val loss: 0.869, r2: 0.265, rate:0.750, acc:0.500 lr 0.0006651397323645576\n",
      "epoch:6 step:200 train loss: 1.710, r2: -0.361, rate:0.562, acc:0.688 val loss: 1.023, r2: -0.031, rate:0.688, acc:0.625 lr 0.0006651397323645576\n",
      "epoch:6 step:300 train loss: 0.776, r2: 0.504, rate:0.812, acc:0.812 val loss: 1.975, r2: -1.913, rate:0.688, acc:0.875 lr 0.0006651397323645576\n",
      "epoch:6 step:400 train loss: 0.898, r2: -0.021, rate:0.875, acc:0.812 val loss: 1.624, r2: -1.420, rate:0.562, acc:0.500 lr 0.0005986257591281018\n",
      "epoch:6 step:500 train loss: 0.870, r2: -0.960, rate:0.375, acc:0.188 val loss: 0.692, r2: 0.452, rate:0.688, acc:0.500 lr 0.0005986257591281018\n",
      "epoch:6 step:600 train loss: 0.770, r2: -1.270, rate:0.625, acc:0.375 val loss: 0.705, r2: 0.692, rate:0.812, acc:0.750 lr 0.0005986257591281018\n",
      "epoch:6 step:700 train loss: 0.648, r2: 0.347, rate:0.750, acc:0.562 val loss: 0.651, r2: -0.433, rate:0.812, acc:0.375 lr 0.0005986257591281018\n",
      "epoch:6 step:800 train loss: 1.395, r2: -0.002, rate:0.500, acc:0.375 val loss: 1.072, r2: -1.473, rate:0.688, acc:0.250 lr 0.0005986257591281018\n",
      "epoch:6 step:900 train loss: 0.575, r2: 0.654, rate:0.812, acc:0.750 val loss: 0.921, r2: -0.476, rate:0.812, acc:0.875 lr 0.0005986257591281018\n",
      "epoch:6 step:1000 train loss: 0.598, r2: 0.189, rate:0.875, acc:0.562 val loss: 0.569, r2: 0.502, rate:0.938, acc:0.750 lr 0.0005986257591281018\n",
      "epoch:6 step:1100 train loss: 2.444, r2: -0.232, rate:0.812, acc:0.938 val loss: 0.585, r2: 0.230, rate:0.875, acc:0.438 lr 0.0005986257591281018\n",
      "epoch:6 step:1200 train loss: 0.713, r2: 0.729, rate:0.625, acc:0.562 val loss: 0.882, r2: 0.498, rate:0.562, acc:0.438 lr 0.0005986257591281018\n",
      "epoch:6 step:1300 train loss: 1.384, r2: 0.216, rate:0.625, acc:0.375 val loss: 1.232, r2: -0.576, rate:0.625, acc:0.500 lr 0.0005986257591281018\n",
      "epoch:6 step:1400 train loss: 2.469, r2: -2.395, rate:0.875, acc:0.750 val loss: 0.809, r2: 0.306, rate:0.812, acc:0.938 lr 0.0005986257591281018\n",
      "epoch:6 step:1500 train loss: 1.626, r2: -0.088, rate:0.562, acc:0.500 val loss: 0.721, r2: 0.522, rate:0.875, acc:0.875 lr 0.0005387631832152916\n",
      "epoch:6 step:1600 train loss: 0.687, r2: 0.250, rate:0.750, acc:0.812 val loss: 0.941, r2: 0.178, rate:0.562, acc:0.625 lr 0.0005387631832152916\n",
      "epoch:6 step:1700 train loss: 0.823, r2: 0.516, rate:0.875, acc:0.938 val loss: 1.057, r2: 0.478, rate:1.000, acc:0.938 lr 0.0005387631832152916\n",
      "epoch:6 step:1800 train loss: 0.690, r2: 0.301, rate:0.562, acc:0.625 val loss: 0.869, r2: 0.323, rate:0.938, acc:0.875 lr 0.0005387631832152916\n",
      "epoch:6 step:1900 train loss: 0.778, r2: 0.344, rate:0.688, acc:0.625 val loss: 3.904, r2: 0.124, rate:0.625, acc:0.500 lr 0.0005387631832152916\n",
      "epoch:6 step:2000 train loss: 0.923, r2: 0.563, rate:0.625, acc:0.500 val loss: 1.082, r2: -0.382, rate:0.562, acc:0.312 lr 0.0005387631832152916\n",
      "epoch:6 step:2100 train loss: 1.892, r2: -0.212, rate:0.500, acc:0.375 val loss: 1.200, r2: 0.094, rate:0.562, acc:0.688 lr 0.0005387631832152916\n",
      "epoch:6 step:2200 train loss: 2.100, r2: -2.370, rate:0.375, acc:0.438 val loss: 1.372, r2: 0.327, rate:0.688, acc:0.625 lr 0.0005387631832152916\n",
      "epoch:6 step:2300 train loss: 0.768, r2: -0.190, rate:0.688, acc:0.750 val loss: 1.079, r2: 0.359, rate:0.688, acc:0.625 lr 0.0005387631832152916\n",
      "epoch:6 step:2400 train loss: 0.747, r2: -0.356, rate:0.625, acc:0.625 val loss: 0.904, r2: -0.811, rate:0.625, acc:0.688 lr 0.0005387631832152916\n",
      "epoch:6 step:2500 train loss: 0.747, r2: 0.506, rate:0.875, acc:0.688 val loss: 1.026, r2: 0.038, rate:0.500, acc:0.750 lr 0.0005387631832152916\n",
      "epoch:6 step:2600 train loss: 2.288, r2: -0.154, rate:0.688, acc:0.688 val loss: 0.677, r2: 0.165, rate:0.688, acc:0.688 lr 0.0004848868648937625\n",
      "epoch:6 step:2700 train loss: 1.159, r2: -0.906, rate:0.500, acc:0.875 val loss: 1.103, r2: -0.011, rate:0.500, acc:0.438 lr 0.0004848868648937625\n",
      "epoch:6 step:2800 train loss: 0.962, r2: -1.694, rate:0.625, acc:0.875 val loss: 1.090, r2: -0.062, rate:0.875, acc:0.750 lr 0.0004848868648937625\n",
      "epoch:6 step:2900 train loss: 0.842, r2: -0.009, rate:0.688, acc:0.688 val loss: 0.591, r2: 0.580, rate:0.875, acc:0.938 lr 0.0004848868648937625\n",
      "epoch:6 step:3000 train loss: 0.628, r2: 0.792, rate:0.750, acc:0.875 val loss: 0.781, r2: -2.106, rate:0.625, acc:0.938 lr 0.0004848868648937625\n",
      "epoch:6 step:3100 train loss: 0.715, r2: -1.453, rate:0.625, acc:0.938 val loss: 0.994, r2: 0.493, rate:0.812, acc:0.625 lr 0.0004848868648937625\n",
      "epoch:6 step:3200 train loss: 0.464, r2: -0.658, rate:0.875, acc:0.938 val loss: 0.991, r2: -1.315, rate:0.500, acc:0.188 lr 0.0004848868648937625\n",
      "epoch:6 step:3300 train loss: 0.770, r2: -0.447, rate:0.312, acc:0.812 val loss: 1.104, r2: 0.178, rate:0.812, acc:0.875 lr 0.0004848868648937625\n",
      "epoch:6 step:3400 train loss: 1.094, r2: -2.761, rate:0.688, acc:0.500 val loss: 2.130, r2: -5.036, rate:0.500, acc:0.875 lr 0.0004848868648937625\n",
      "epoch:6 step:3500 train loss: 1.324, r2: -1.519, rate:0.500, acc:0.562 val loss: 1.048, r2: 0.163, rate:0.625, acc:0.500 lr 0.0004848868648937625\n",
      "epoch:6 step:3600 train loss: 0.890, r2: -0.103, rate:0.500, acc:0.312 val loss: 0.896, r2: 0.074, rate:0.750, acc:0.625 lr 0.0004848868648937625\n",
      "epoch:6 step:3700 train loss: 0.733, r2: -0.043, rate:0.688, acc:0.688 val loss: 0.811, r2: -0.382, rate:0.688, acc:0.875 lr 0.0004363981784043862\n",
      "epoch:6 step:3800 train loss: 2.096, r2: 0.519, rate:0.688, acc:0.688 val loss: 1.381, r2: -0.262, rate:0.562, acc:0.750 lr 0.0004363981784043862\n",
      "epoch:6 step:3900 train loss: 0.734, r2: 0.308, rate:0.688, acc:0.812 val loss: 0.787, r2: 0.426, rate:0.750, acc:0.812 lr 0.0004363981784043862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:4000 train loss: 0.502, r2: 0.820, rate:0.875, acc:0.938 val loss: 0.730, r2: 0.105, rate:0.750, acc:0.875 lr 0.0004363981784043862\n",
      "epoch:6 step:4100 train loss: 2.793, r2: 0.389, rate:0.688, acc:0.688 val loss: 2.049, r2: -1.308, rate:0.688, acc:0.500 lr 0.0004363981784043862\n",
      "epoch:6 step:4200 train loss: 1.037, r2: -3.345, rate:0.375, acc:0.125 val loss: 2.029, r2: -0.813, rate:0.688, acc:0.812 lr 0.0004363981784043862\n",
      "epoch:6 step:4300 train loss: 0.646, r2: 0.269, rate:0.812, acc:0.500 val loss: 0.656, r2: -1.202, rate:0.688, acc:0.500 lr 0.0004363981784043862\n",
      "epoch:6 step:4400 train loss: 0.761, r2: -0.777, rate:0.750, acc:0.938 val loss: 0.696, r2: 0.544, rate:0.750, acc:0.938 lr 0.0004363981784043862\n",
      "epoch:6 step:4500 train loss: 1.147, r2: -0.004, rate:0.750, acc:0.312 val loss: 0.795, r2: 0.242, rate:0.562, acc:0.688 lr 0.0004363981784043862\n",
      "epoch:6 step:4600 train loss: 0.806, r2: -0.748, rate:0.375, acc:0.688 val loss: 1.148, r2: 0.158, rate:0.812, acc:0.500 lr 0.0004363981784043862\n",
      "epoch:6 step:4700 train loss: 2.362, r2: -0.061, rate:0.438, acc:0.375 val loss: 0.846, r2: -0.199, rate:0.438, acc:0.562 lr 0.0004363981784043862\n",
      "epoch:6 step:4800 train loss: 0.558, r2: 0.477, rate:0.875, acc:0.938 val loss: 0.698, r2: 0.278, rate:0.562, acc:0.750 lr 0.0003927583605639476\n",
      "epoch:6 step:4900 train loss: 0.591, r2: 0.764, rate:1.000, acc:1.000 val loss: 1.155, r2: 0.570, rate:0.750, acc:0.562 lr 0.0003927583605639476\n",
      "epoch:6 step:5000 train loss: 0.582, r2: 0.561, rate:0.875, acc:0.875 val loss: 1.317, r2: 0.380, rate:0.875, acc:0.812 lr 0.0003927583605639476\n",
      "epoch:6 step:5100 train loss: 0.620, r2: 0.503, rate:0.875, acc:0.812 val loss: 2.104, r2: -0.831, rate:0.375, acc:0.312 lr 0.0003927583605639476\n",
      "epoch:6 step:5200 train loss: 1.089, r2: -2.847, rate:0.125, acc:0.250 val loss: 1.148, r2: -4.083, rate:0.562, acc:0.625 lr 0.0003927583605639476\n",
      "epoch:6 step:5300 train loss: 1.238, r2: 0.388, rate:0.750, acc:0.562 val loss: 0.645, r2: 0.764, rate:0.812, acc:0.812 lr 0.0003927583605639476\n",
      "epoch:6 step:5400 train loss: 1.179, r2: 0.525, rate:0.562, acc:0.438 val loss: 1.102, r2: 0.183, rate:0.750, acc:0.750 lr 0.0003927583605639476\n",
      "epoch:6 step:5500 train loss: 1.384, r2: -1.660, rate:0.375, acc:0.375 val loss: 0.828, r2: -12.820, rate:0.688, acc:0.750 lr 0.0003927583605639476\n",
      "epoch:6 step:5600 train loss: 1.065, r2: 0.044, rate:0.938, acc:0.812 val loss: 0.714, r2: 0.496, rate:0.688, acc:0.562 lr 0.0003927583605639476\n",
      "epoch:6 step:5700 train loss: 0.893, r2: 0.383, rate:0.688, acc:0.562 val loss: 0.720, r2: 0.532, rate:0.750, acc:0.750 lr 0.0003927583605639476\n",
      "epoch:6 step:5800 train loss: 2.258, r2: 0.353, rate:0.688, acc:0.625 val loss: 1.147, r2: -0.709, rate:0.688, acc:0.500 lr 0.0003927583605639476\n",
      "epoch:6 step:5900 train loss: 0.859, r2: 0.506, rate:0.875, acc:0.812 val loss: 0.821, r2: -0.790, rate:0.750, acc:0.625 lr 0.00035348252450755286\n",
      "epoch:6 step:6000 train loss: 0.782, r2: 0.351, rate:0.562, acc:0.562 val loss: 1.056, r2: -8.051, rate:0.562, acc:0.438 lr 0.00035348252450755286\n",
      "epoch:6 step:6100 train loss: 0.722, r2: 0.563, rate:0.688, acc:0.500 val loss: 0.584, r2: 0.496, rate:0.938, acc:0.875 lr 0.00035348252450755286\n",
      "epoch:6 step:6200 train loss: 1.161, r2: 0.058, rate:0.875, acc:0.562 val loss: 0.848, r2: 0.281, rate:0.688, acc:0.938 lr 0.00035348252450755286\n",
      "epoch:6 step:6300 train loss: 1.588, r2: -0.710, rate:0.875, acc:0.750 val loss: 1.871, r2: 0.437, rate:0.812, acc:0.562 lr 0.00035348252450755286\n",
      "epoch:6 step:6400 train loss: 1.198, r2: -0.595, rate:0.688, acc:0.562 val loss: 2.434, r2: 0.189, rate:0.750, acc:0.812 lr 0.00035348252450755286\n",
      "epoch:6 step:6500 train loss: 1.499, r2: -3.784, rate:0.438, acc:0.250 val loss: 0.606, r2: 0.224, rate:0.750, acc:0.938 lr 0.00035348252450755286\n",
      "epoch:6 step:6600 train loss: 1.295, r2: -0.643, rate:0.562, acc:0.312 val loss: 1.373, r2: -0.305, rate:0.625, acc:0.562 lr 0.00035348252450755286\n",
      "epoch:6 step:6700 train loss: 1.461, r2: 0.355, rate:0.875, acc:0.812 val loss: 1.104, r2: -0.073, rate:0.562, acc:0.625 lr 0.00035348252450755286\n",
      "epoch:6 step:6800 train loss: 1.218, r2: 0.498, rate:0.812, acc:0.688 val loss: 0.671, r2: 0.455, rate:0.812, acc:0.562 lr 0.00035348252450755286\n",
      "epoch:6 step:6900 train loss: 1.130, r2: -0.901, rate:0.812, acc:0.875 val loss: 0.891, r2: 0.425, rate:0.750, acc:0.812 lr 0.00035348252450755286\n",
      "epoch:6 step:7000 train loss: 0.751, r2: 0.529, rate:0.812, acc:0.500 val loss: 0.742, r2: 0.223, rate:0.500, acc:0.312 lr 0.0003181342720567976\n",
      "epoch:6 step:7100 train loss: 1.420, r2: -2.414, rate:0.375, acc:0.125 val loss: 0.846, r2: 0.412, rate:0.812, acc:0.688 lr 0.0003181342720567976\n",
      "epoch:6 step:7200 train loss: 0.664, r2: 0.423, rate:0.688, acc:0.688 val loss: 0.705, r2: 0.462, rate:0.688, acc:0.938 lr 0.0003181342720567976\n",
      "epoch:6 step:7300 train loss: 0.804, r2: 0.034, rate:0.625, acc:0.938 val loss: 1.119, r2: -0.594, rate:0.375, acc:0.438 lr 0.0003181342720567976\n",
      "epoch:6 step:7400 train loss: 0.654, r2: 0.320, rate:0.812, acc:0.875 val loss: 1.954, r2: 0.130, rate:0.625, acc:0.500 lr 0.0003181342720567976\n",
      "epoch:6 step:7500 train loss: 3.078, r2: -6.591, rate:0.625, acc:0.938 val loss: 1.038, r2: -1.104, rate:0.562, acc:0.250 lr 0.0003181342720567976\n",
      "epoch:6 step:7600 train loss: 1.706, r2: 0.205, rate:0.688, acc:0.500 val loss: 2.069, r2: -2.114, rate:0.500, acc:0.625 lr 0.0003181342720567976\n",
      "epoch:6 step:7700 train loss: 1.720, r2: -0.119, rate:0.562, acc:0.500 val loss: 1.744, r2: 0.010, rate:0.625, acc:0.562 lr 0.0003181342720567976\n",
      "epoch:6 step:7800 train loss: 0.702, r2: 0.219, rate:0.750, acc:0.750 val loss: 1.080, r2: 0.117, rate:0.500, acc:0.625 lr 0.0003181342720567976\n",
      "epoch:6 step:0.326 lr 0.000318\n",
      "epoch:7 step:0 train loss: 0.652, r2: -1.975, rate:0.625, acc:0.875 val loss: 1.418, r2: -0.329, rate:0.938, acc:0.625 lr 0.0003181342720567976\n",
      "epoch:7 step:100 train loss: 0.813, r2: -0.811, rate:0.812, acc:0.500 val loss: 1.732, r2: -0.692, rate:0.375, acc:0.188 lr 0.0003181342720567976\n",
      "epoch:7 step:200 train loss: 2.051, r2: -2.194, rate:0.688, acc:0.562 val loss: 0.855, r2: 0.048, rate:0.562, acc:0.312 lr 0.00028632084485111784\n",
      "epoch:7 step:300 train loss: 0.826, r2: -0.011, rate:0.562, acc:0.500 val loss: 0.917, r2: 0.314, rate:0.750, acc:0.812 lr 0.00028632084485111784\n",
      "epoch:7 step:400 train loss: 0.677, r2: -5.314, rate:0.625, acc:0.562 val loss: 0.690, r2: 0.167, rate:0.812, acc:0.750 lr 0.00028632084485111784\n",
      "epoch:7 step:500 train loss: 1.550, r2: -2.699, rate:0.750, acc:0.250 val loss: 1.582, r2: -1.547, rate:0.375, acc:0.250 lr 0.00028632084485111784\n",
      "epoch:7 step:600 train loss: 1.871, r2: -3.353, rate:0.812, acc:0.625 val loss: 3.145, r2: 0.356, rate:0.625, acc:0.500 lr 0.00028632084485111784\n",
      "epoch:7 step:700 train loss: 0.574, r2: 0.785, rate:0.750, acc:0.625 val loss: 2.167, r2: -2.340, rate:0.562, acc:0.438 lr 0.00028632084485111784\n",
      "epoch:7 step:800 train loss: 0.911, r2: -0.417, rate:0.625, acc:0.312 val loss: 0.825, r2: -0.965, rate:0.625, acc:0.625 lr 0.00028632084485111784\n",
      "epoch:7 step:900 train loss: 0.632, r2: 0.556, rate:0.625, acc:0.688 val loss: 0.757, r2: 0.521, rate:0.750, acc:0.750 lr 0.00028632084485111784\n",
      "epoch:7 step:1000 train loss: 0.940, r2: 0.383, rate:0.625, acc:0.750 val loss: 0.611, r2: 0.713, rate:0.875, acc:0.562 lr 0.00028632084485111784\n",
      "epoch:7 step:1100 train loss: 0.761, r2: 0.371, rate:0.938, acc:0.625 val loss: 1.019, r2: 0.192, rate:0.625, acc:0.688 lr 0.00028632084485111784\n",
      "epoch:7 step:1200 train loss: 1.021, r2: -1.905, rate:0.875, acc:0.500 val loss: 0.937, r2: -0.812, rate:0.562, acc:0.812 lr 0.00028632084485111784\n",
      "epoch:7 step:1300 train loss: 0.657, r2: -0.016, rate:0.625, acc:0.500 val loss: 1.024, r2: 0.479, rate:0.688, acc:0.812 lr 0.00025768876036600606\n",
      "epoch:7 step:1400 train loss: 1.514, r2: -0.567, rate:0.688, acc:0.500 val loss: 1.195, r2: 0.283, rate:0.812, acc:0.625 lr 0.00025768876036600606\n",
      "epoch:7 step:1500 train loss: 0.570, r2: -0.730, rate:0.750, acc:0.688 val loss: 1.103, r2: 0.054, rate:0.750, acc:0.875 lr 0.00025768876036600606\n",
      "epoch:7 step:1600 train loss: 0.775, r2: -1.109, rate:0.438, acc:0.562 val loss: 0.965, r2: 0.409, rate:0.562, acc:0.688 lr 0.00025768876036600606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:1700 train loss: 0.953, r2: -0.114, rate:0.812, acc:0.562 val loss: 0.730, r2: -1.013, rate:0.500, acc:0.500 lr 0.00025768876036600606\n",
      "epoch:7 step:1800 train loss: 0.959, r2: -0.477, rate:0.500, acc:0.312 val loss: 1.656, r2: -1.343, rate:0.375, acc:0.688 lr 0.00025768876036600606\n",
      "epoch:7 step:1900 train loss: 0.740, r2: 0.219, rate:0.750, acc:0.562 val loss: 0.756, r2: -0.008, rate:0.688, acc:0.625 lr 0.00025768876036600606\n",
      "epoch:7 step:2000 train loss: 1.961, r2: -2.793, rate:0.250, acc:0.125 val loss: 1.187, r2: -1.729, rate:0.688, acc:0.500 lr 0.00025768876036600606\n",
      "epoch:7 step:2100 train loss: 0.695, r2: 0.242, rate:0.750, acc:0.938 val loss: 0.745, r2: 0.259, rate:0.812, acc:0.562 lr 0.00025768876036600606\n",
      "epoch:7 step:2200 train loss: 0.777, r2: -0.088, rate:0.750, acc:0.812 val loss: 0.865, r2: -0.483, rate:0.688, acc:0.438 lr 0.00025768876036600606\n",
      "epoch:7 step:2300 train loss: 0.808, r2: 0.042, rate:0.688, acc:0.375 val loss: 2.659, r2: 0.106, rate:0.500, acc:0.562 lr 0.00025768876036600606\n",
      "epoch:7 step:2400 train loss: 0.694, r2: 0.324, rate:0.500, acc:0.500 val loss: 1.182, r2: 0.179, rate:0.750, acc:0.812 lr 0.00023191988432940547\n",
      "epoch:7 step:2500 train loss: 0.698, r2: 0.329, rate:0.625, acc:0.625 val loss: 2.160, r2: 0.205, rate:0.750, acc:0.750 lr 0.00023191988432940547\n",
      "epoch:7 step:2600 train loss: 0.633, r2: 0.305, rate:0.688, acc:0.812 val loss: 1.069, r2: 0.397, rate:0.688, acc:0.562 lr 0.00023191988432940547\n",
      "epoch:7 step:2700 train loss: 0.825, r2: -1.538, rate:0.312, acc:0.562 val loss: 1.032, r2: -0.485, rate:0.688, acc:0.938 lr 0.00023191988432940547\n",
      "epoch:7 step:2800 train loss: 0.743, r2: 0.464, rate:0.688, acc:0.750 val loss: 3.145, r2: -1.168, rate:0.750, acc:0.750 lr 0.00023191988432940547\n",
      "epoch:7 step:2900 train loss: 1.130, r2: -0.677, rate:0.562, acc:0.812 val loss: 1.382, r2: 0.183, rate:0.438, acc:0.375 lr 0.00023191988432940547\n",
      "epoch:7 step:3000 train loss: 0.709, r2: -0.071, rate:1.000, acc:1.000 val loss: 1.069, r2: -1.414, rate:1.000, acc:1.000 lr 0.00023191988432940547\n",
      "epoch:7 step:3100 train loss: 1.012, r2: -0.495, rate:0.750, acc:0.938 val loss: 2.628, r2: -0.170, rate:0.562, acc:0.688 lr 0.00023191988432940547\n",
      "epoch:7 step:3200 train loss: 1.352, r2: -8.350, rate:0.500, acc:0.688 val loss: 0.799, r2: 0.432, rate:0.812, acc:0.875 lr 0.00023191988432940547\n",
      "epoch:7 step:3300 train loss: 1.247, r2: -5.902, rate:0.562, acc:0.500 val loss: 0.809, r2: -0.735, rate:0.625, acc:0.688 lr 0.00023191988432940547\n",
      "epoch:7 step:3400 train loss: 0.693, r2: -0.311, rate:0.688, acc:0.375 val loss: 0.977, r2: -0.306, rate:0.625, acc:0.250 lr 0.00023191988432940547\n",
      "epoch:7 step:3500 train loss: 0.740, r2: 0.195, rate:0.500, acc:0.562 val loss: 0.653, r2: 0.585, rate:0.625, acc:0.688 lr 0.00020872789589646492\n",
      "epoch:7 step:3600 train loss: 0.808, r2: -0.810, rate:0.500, acc:0.688 val loss: 0.871, r2: -0.330, rate:0.625, acc:0.688 lr 0.00020872789589646492\n",
      "epoch:7 step:3700 train loss: 1.212, r2: 0.144, rate:0.688, acc:0.500 val loss: 1.013, r2: -0.721, rate:0.625, acc:0.938 lr 0.00020872789589646492\n",
      "epoch:7 step:3800 train loss: 3.559, r2: -1.692, rate:0.688, acc:0.750 val loss: 1.299, r2: -0.428, rate:0.625, acc:0.625 lr 0.00020872789589646492\n",
      "epoch:7 step:3900 train loss: 0.933, r2: -0.060, rate:0.750, acc:0.688 val loss: 1.811, r2: -1.464, rate:0.812, acc:0.688 lr 0.00020872789589646492\n",
      "epoch:7 step:4000 train loss: 0.604, r2: 0.024, rate:0.688, acc:0.812 val loss: 1.285, r2: -1.472, rate:0.562, acc:0.312 lr 0.00020872789589646492\n",
      "epoch:7 step:4100 train loss: 2.712, r2: -0.574, rate:0.625, acc:0.500 val loss: 0.560, r2: 0.220, rate:0.688, acc:0.688 lr 0.00020872789589646492\n",
      "epoch:7 step:4200 train loss: 0.606, r2: 0.428, rate:0.875, acc:0.750 val loss: 1.431, r2: -0.231, rate:0.312, acc:0.562 lr 0.00020872789589646492\n",
      "epoch:7 step:4300 train loss: 0.608, r2: -1.796, rate:0.750, acc:0.688 val loss: 1.186, r2: -0.020, rate:0.875, acc:1.000 lr 0.00020872789589646492\n",
      "epoch:7 step:4400 train loss: 0.994, r2: 0.053, rate:0.688, acc:0.812 val loss: 1.052, r2: -0.592, rate:0.500, acc:0.375 lr 0.00020872789589646492\n",
      "epoch:7 step:4500 train loss: 0.587, r2: -3.072, rate:0.750, acc:0.938 val loss: 0.759, r2: 0.629, rate:0.812, acc:0.938 lr 0.00020872789589646492\n",
      "epoch:7 step:4600 train loss: 1.576, r2: -0.490, rate:0.375, acc:0.375 val loss: 0.586, r2: 0.610, rate:0.812, acc:0.750 lr 0.00018785510630681842\n",
      "epoch:7 step:4700 train loss: 1.553, r2: -0.449, rate:0.812, acc:0.875 val loss: 0.521, r2: -0.087, rate:0.875, acc:0.812 lr 0.00018785510630681842\n",
      "epoch:7 step:4800 train loss: 0.684, r2: 0.241, rate:0.625, acc:0.188 val loss: 0.810, r2: 0.640, rate:0.500, acc:0.688 lr 0.00018785510630681842\n",
      "epoch:7 step:4900 train loss: 0.752, r2: -0.313, rate:0.688, acc:0.938 val loss: 0.621, r2: 0.290, rate:0.750, acc:0.562 lr 0.00018785510630681842\n",
      "epoch:7 step:5000 train loss: 0.602, r2: 0.549, rate:0.812, acc:0.750 val loss: 0.622, r2: 0.721, rate:0.938, acc:0.750 lr 0.00018785510630681842\n",
      "epoch:7 step:5100 train loss: 0.663, r2: 0.621, rate:0.750, acc:0.500 val loss: 1.113, r2: 0.225, rate:0.688, acc:0.750 lr 0.00018785510630681842\n",
      "epoch:7 step:5200 train loss: 0.810, r2: -1.309, rate:0.562, acc:0.688 val loss: 0.667, r2: -1.821, rate:0.688, acc:1.000 lr 0.00018785510630681842\n",
      "epoch:7 step:5300 train loss: 1.060, r2: -0.407, rate:0.750, acc:0.625 val loss: 1.219, r2: -1.080, rate:0.688, acc:0.938 lr 0.00018785510630681842\n",
      "epoch:7 step:5400 train loss: 0.723, r2: 0.243, rate:0.438, acc:0.812 val loss: 1.347, r2: -2.283, rate:0.688, acc:0.812 lr 0.00018785510630681842\n",
      "epoch:7 step:5500 train loss: 0.977, r2: 0.280, rate:0.688, acc:0.938 val loss: 0.651, r2: 0.276, rate:0.812, acc:0.688 lr 0.00018785510630681842\n",
      "epoch:7 step:5600 train loss: 0.976, r2: 0.421, rate:0.812, acc:0.562 val loss: 1.014, r2: -0.234, rate:0.812, acc:0.562 lr 0.00018785510630681842\n",
      "epoch:7 step:5700 train loss: 0.879, r2: 0.443, rate:0.875, acc:0.625 val loss: 1.170, r2: -0.165, rate:0.688, acc:0.625 lr 0.00016906959567613658\n",
      "epoch:7 step:5800 train loss: 0.561, r2: 0.554, rate:0.938, acc:0.875 val loss: 0.694, r2: 0.286, rate:0.750, acc:0.812 lr 0.00016906959567613658\n",
      "epoch:7 step:5900 train loss: 1.788, r2: -0.738, rate:0.688, acc:0.500 val loss: 0.733, r2: 0.684, rate:0.688, acc:0.625 lr 0.00016906959567613658\n",
      "epoch:7 step:6000 train loss: 0.819, r2: -0.166, rate:0.562, acc:0.375 val loss: 0.888, r2: -0.012, rate:0.750, acc:0.688 lr 0.00016906959567613658\n",
      "epoch:7 step:6100 train loss: 1.281, r2: -0.014, rate:0.812, acc:1.000 val loss: 2.933, r2: 0.580, rate:0.938, acc:0.938 lr 0.00016906959567613658\n",
      "epoch:7 step:6200 train loss: 0.999, r2: -0.373, rate:0.562, acc:0.562 val loss: 0.855, r2: 0.279, rate:0.562, acc:0.750 lr 0.00016906959567613658\n",
      "epoch:7 step:6300 train loss: 0.573, r2: 0.526, rate:0.812, acc:0.688 val loss: 0.667, r2: 0.407, rate:0.875, acc:0.938 lr 0.00016906959567613658\n",
      "epoch:7 step:6400 train loss: 1.379, r2: -10.196, rate:0.875, acc:1.000 val loss: 0.963, r2: 0.239, rate:0.750, acc:0.688 lr 0.00016906959567613658\n",
      "epoch:7 step:6500 train loss: 2.261, r2: -1.388, rate:0.750, acc:0.875 val loss: 0.685, r2: 0.590, rate:0.750, acc:0.812 lr 0.00016906959567613658\n",
      "epoch:7 step:6600 train loss: 0.698, r2: 0.351, rate:0.750, acc:0.375 val loss: 1.074, r2: -0.048, rate:0.750, acc:0.875 lr 0.00016906959567613658\n",
      "epoch:7 step:6700 train loss: 0.605, r2: 0.748, rate:0.750, acc:0.500 val loss: 0.996, r2: -0.314, rate:0.625, acc:0.812 lr 0.00016906959567613658\n",
      "epoch:7 step:6800 train loss: 2.727, r2: -2.664, rate:0.562, acc:0.875 val loss: 0.693, r2: 0.324, rate:0.812, acc:0.625 lr 0.00015216263610852294\n",
      "epoch:7 step:6900 train loss: 1.270, r2: 0.190, rate:0.812, acc:0.438 val loss: 2.646, r2: -1.426, rate:0.750, acc:0.750 lr 0.00015216263610852294\n",
      "epoch:7 step:7000 train loss: 0.624, r2: 0.119, rate:0.875, acc:0.250 val loss: 0.672, r2: 0.165, rate:0.688, acc:0.812 lr 0.00015216263610852294\n",
      "epoch:7 step:7100 train loss: 0.661, r2: 0.646, rate:0.688, acc:0.562 val loss: 6.123, r2: -3.527, rate:0.750, acc:0.938 lr 0.00015216263610852294\n",
      "epoch:7 step:7200 train loss: 0.905, r2: -0.450, rate:0.500, acc:0.500 val loss: 0.844, r2: -0.124, rate:0.750, acc:0.562 lr 0.00015216263610852294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:7300 train loss: 0.779, r2: -0.158, rate:0.562, acc:0.875 val loss: 0.687, r2: -0.223, rate:0.625, acc:0.812 lr 0.00015216263610852294\n",
      "epoch:7 step:7400 train loss: 0.843, r2: 0.551, rate:0.750, acc:0.750 val loss: 0.870, r2: -0.039, rate:0.562, acc:0.625 lr 0.00015216263610852294\n",
      "epoch:7 step:7500 train loss: 0.924, r2: -2.164, rate:0.625, acc:0.750 val loss: 1.186, r2: 0.282, rate:0.562, acc:0.562 lr 0.00015216263610852294\n",
      "epoch:7 step:7600 train loss: 0.905, r2: -0.386, rate:0.562, acc:0.812 val loss: 2.996, r2: -2.555, rate:0.625, acc:0.438 lr 0.00015216263610852294\n",
      "epoch:7 step:7700 train loss: 3.288, r2: -0.501, rate:0.938, acc:1.000 val loss: 1.064, r2: 0.335, rate:0.625, acc:0.688 lr 0.00015216263610852294\n",
      "epoch:7 step:7800 train loss: 0.827, r2: 0.593, rate:0.625, acc:0.688 val loss: 0.979, r2: -2.814, rate:0.562, acc:1.000 lr 0.00015216263610852294\n",
      "epoch:7 step:0.315 lr 0.000152\n",
      "epoch:8 step:0 train loss: 0.719, r2: 0.539, rate:0.625, acc:0.625 val loss: 0.951, r2: -0.401, rate:0.750, acc:0.750 lr 0.00013694637249767063\n",
      "epoch:8 step:100 train loss: 0.929, r2: -1.024, rate:0.688, acc:0.562 val loss: 2.247, r2: -0.076, rate:0.500, acc:0.312 lr 0.00013694637249767063\n",
      "epoch:8 step:200 train loss: 1.120, r2: -3.249, rate:0.938, acc:1.000 val loss: 0.994, r2: -0.060, rate:0.625, acc:0.750 lr 0.00013694637249767063\n",
      "epoch:8 step:300 train loss: 0.675, r2: 0.627, rate:0.562, acc:0.438 val loss: 1.166, r2: -0.143, rate:0.625, acc:0.750 lr 0.00013694637249767063\n",
      "epoch:8 step:400 train loss: 0.826, r2: -0.294, rate:0.688, acc:0.375 val loss: 1.108, r2: -0.705, rate:0.562, acc:0.375 lr 0.00013694637249767063\n",
      "epoch:8 step:500 train loss: 4.006, r2: -4.475, rate:0.562, acc:0.438 val loss: 0.529, r2: 0.595, rate:0.938, acc:0.875 lr 0.00013694637249767063\n",
      "epoch:8 step:600 train loss: 0.902, r2: -0.935, rate:0.750, acc:0.938 val loss: 0.963, r2: 0.616, rate:0.688, acc:0.625 lr 0.00013694637249767063\n",
      "epoch:8 step:700 train loss: 0.587, r2: 0.765, rate:0.812, acc:0.750 val loss: 0.797, r2: -0.129, rate:0.625, acc:0.750 lr 0.00013694637249767063\n",
      "epoch:8 step:800 train loss: 0.966, r2: -0.628, rate:0.812, acc:0.438 val loss: 0.647, r2: 0.395, rate:0.750, acc:0.750 lr 0.00013694637249767063\n",
      "epoch:8 step:900 train loss: 0.588, r2: 0.291, rate:0.938, acc:0.688 val loss: 0.770, r2: -0.431, rate:0.688, acc:0.688 lr 0.00013694637249767063\n",
      "epoch:8 step:1000 train loss: 0.613, r2: 0.770, rate:0.750, acc:0.625 val loss: 0.846, r2: -0.065, rate:0.688, acc:0.688 lr 0.00013694637249767063\n",
      "epoch:8 step:1100 train loss: 2.632, r2: -1.203, rate:0.438, acc:0.688 val loss: 0.814, r2: 0.126, rate:0.375, acc:0.688 lr 0.00012325173524790358\n",
      "epoch:8 step:1200 train loss: 0.721, r2: 0.418, rate:0.500, acc:0.688 val loss: 0.985, r2: -0.351, rate:0.562, acc:0.438 lr 0.00012325173524790358\n",
      "epoch:8 step:1300 train loss: 1.229, r2: -0.683, rate:0.500, acc:0.750 val loss: 0.582, r2: 0.292, rate:0.750, acc:0.625 lr 0.00012325173524790358\n",
      "epoch:8 step:1400 train loss: 1.602, r2: -3.392, rate:0.500, acc:0.812 val loss: 1.101, r2: 0.053, rate:0.688, acc:0.688 lr 0.00012325173524790358\n",
      "epoch:8 step:1500 train loss: 2.005, r2: -1.780, rate:0.312, acc:0.500 val loss: 0.909, r2: -0.336, rate:0.750, acc:0.812 lr 0.00012325173524790358\n",
      "epoch:8 step:1600 train loss: 1.030, r2: -1.173, rate:0.625, acc:0.438 val loss: 0.841, r2: 0.423, rate:0.625, acc:0.812 lr 0.00012325173524790358\n",
      "epoch:8 step:1700 train loss: 1.245, r2: -4.612, rate:0.875, acc:0.938 val loss: 0.814, r2: 0.444, rate:0.812, acc:0.812 lr 0.00012325173524790358\n",
      "epoch:8 step:1800 train loss: 0.973, r2: 0.128, rate:0.688, acc:0.562 val loss: 0.908, r2: 0.082, rate:0.688, acc:0.375 lr 0.00012325173524790358\n",
      "epoch:8 step:1900 train loss: 0.738, r2: 0.019, rate:0.688, acc:0.750 val loss: 1.247, r2: -0.412, rate:0.750, acc:0.500 lr 0.00012325173524790358\n",
      "epoch:8 step:2000 train loss: 1.125, r2: 0.563, rate:0.938, acc:0.938 val loss: 1.047, r2: 0.459, rate:0.750, acc:0.812 lr 0.00012325173524790358\n",
      "epoch:8 step:2100 train loss: 1.416, r2: -4.816, rate:0.500, acc:0.750 val loss: 0.594, r2: 0.116, rate:0.750, acc:0.875 lr 0.00012325173524790358\n",
      "epoch:8 step:2200 train loss: 2.266, r2: -4.594, rate:0.938, acc:0.812 val loss: 0.594, r2: -2.802, rate:0.750, acc:0.625 lr 0.00011092656172311323\n",
      "epoch:8 step:2300 train loss: 0.782, r2: 0.136, rate:0.812, acc:0.750 val loss: 0.499, r2: 0.739, rate:0.875, acc:0.938 lr 0.00011092656172311323\n",
      "epoch:8 step:2400 train loss: 0.747, r2: 0.272, rate:0.688, acc:0.562 val loss: 0.666, r2: 0.336, rate:0.812, acc:0.875 lr 0.00011092656172311323\n",
      "epoch:8 step:2500 train loss: 0.796, r2: -0.050, rate:0.500, acc:0.625 val loss: 1.040, r2: 0.043, rate:0.562, acc:0.812 lr 0.00011092656172311323\n",
      "epoch:8 step:2600 train loss: 1.084, r2: -0.359, rate:0.812, acc:0.500 val loss: 0.788, r2: -0.353, rate:0.750, acc:0.500 lr 0.00011092656172311323\n",
      "epoch:8 step:2700 train loss: 0.657, r2: 0.610, rate:0.812, acc:0.688 val loss: 0.587, r2: -0.187, rate:0.812, acc:0.875 lr 0.00011092656172311323\n",
      "epoch:8 step:2800 train loss: 0.715, r2: -0.714, rate:0.688, acc:0.750 val loss: 1.218, r2: 0.224, rate:0.750, acc:0.688 lr 0.00011092656172311323\n",
      "epoch:8 step:2900 train loss: 0.769, r2: 0.431, rate:0.625, acc:0.562 val loss: 1.346, r2: 0.217, rate:0.812, acc:0.500 lr 0.00011092656172311323\n",
      "epoch:8 step:3000 train loss: 0.643, r2: -1.606, rate:1.000, acc:1.000 val loss: 0.951, r2: -0.032, rate:0.625, acc:0.500 lr 0.00011092656172311323\n",
      "epoch:8 step:3100 train loss: 0.889, r2: -0.046, rate:0.750, acc:0.875 val loss: 2.473, r2: -1.618, rate:0.625, acc:0.875 lr 0.00011092656172311323\n",
      "epoch:8 step:3200 train loss: 2.043, r2: -5.661, rate:0.562, acc:0.812 val loss: 0.961, r2: 0.298, rate:0.812, acc:0.562 lr 0.00011092656172311323\n",
      "epoch:8 step:3300 train loss: 0.760, r2: -0.636, rate:0.812, acc:0.938 val loss: 0.857, r2: -0.030, rate:0.938, acc:0.625 lr 9.983390555080191e-05\n",
      "epoch:8 step:3400 train loss: 0.708, r2: -4.422, rate:0.375, acc:0.500 val loss: 1.239, r2: 0.043, rate:0.688, acc:0.688 lr 9.983390555080191e-05\n",
      "epoch:8 step:3500 train loss: 1.227, r2: -1.807, rate:0.500, acc:0.688 val loss: 4.589, r2: -0.562, rate:0.438, acc:0.375 lr 9.983390555080191e-05\n",
      "epoch:8 step:3600 train loss: 0.903, r2: -1.341, rate:0.938, acc:0.562 val loss: 0.827, r2: -8.466, rate:0.750, acc:0.312 lr 9.983390555080191e-05\n",
      "epoch:8 step:3700 train loss: 1.165, r2: -0.741, rate:0.562, acc:0.312 val loss: 2.541, r2: 0.538, rate:0.688, acc:0.750 lr 9.983390555080191e-05\n",
      "epoch:8 step:3800 train loss: 0.686, r2: -3.012, rate:0.875, acc:0.438 val loss: 0.955, r2: -0.551, rate:0.625, acc:0.688 lr 9.983390555080191e-05\n",
      "epoch:8 step:3900 train loss: 0.786, r2: -0.551, rate:0.625, acc:0.562 val loss: 0.903, r2: -1.060, rate:0.625, acc:0.688 lr 9.983390555080191e-05\n",
      "epoch:8 step:4000 train loss: 1.954, r2: -4.832, rate:0.812, acc:0.812 val loss: 0.587, r2: 0.288, rate:0.875, acc:0.938 lr 9.983390555080191e-05\n",
      "epoch:8 step:4100 train loss: 0.759, r2: 0.343, rate:0.625, acc:0.688 val loss: 1.211, r2: -0.587, rate:0.562, acc:0.562 lr 9.983390555080191e-05\n",
      "epoch:8 step:4200 train loss: 1.195, r2: 0.041, rate:0.812, acc:0.875 val loss: 0.774, r2: -0.050, rate:0.688, acc:0.375 lr 9.983390555080191e-05\n",
      "epoch:8 step:4300 train loss: 0.672, r2: -0.441, rate:0.875, acc:0.750 val loss: 0.628, r2: 0.457, rate:0.688, acc:0.500 lr 9.983390555080191e-05\n",
      "epoch:8 step:4400 train loss: 0.636, r2: 0.477, rate:0.812, acc:0.875 val loss: 1.146, r2: -1.149, rate:0.750, acc:0.312 lr 8.985051499572172e-05\n",
      "epoch:8 step:4500 train loss: 0.697, r2: 0.119, rate:1.000, acc:0.812 val loss: 1.258, r2: -0.600, rate:0.750, acc:0.625 lr 8.985051499572172e-05\n",
      "epoch:8 step:4600 train loss: 0.736, r2: -0.362, rate:0.625, acc:0.562 val loss: 0.746, r2: -0.031, rate:0.625, acc:0.750 lr 8.985051499572172e-05\n",
      "epoch:8 step:4700 train loss: 2.120, r2: 0.485, rate:0.688, acc:0.688 val loss: 1.421, r2: -1.440, rate:1.000, acc:0.750 lr 8.985051499572172e-05\n",
      "epoch:8 step:4800 train loss: 1.480, r2: -0.097, rate:0.875, acc:0.750 val loss: 0.762, r2: -0.414, rate:0.500, acc:0.250 lr 8.985051499572172e-05\n",
      "epoch:8 step:4900 train loss: 4.844, r2: -2.034, rate:0.812, acc:0.750 val loss: 1.026, r2: 0.317, rate:0.562, acc:0.625 lr 8.985051499572172e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:5000 train loss: 1.240, r2: -0.331, rate:0.750, acc:1.000 val loss: 0.651, r2: 0.136, rate:0.812, acc:0.562 lr 8.985051499572172e-05\n",
      "epoch:8 step:5100 train loss: 0.555, r2: 0.815, rate:0.812, acc:0.812 val loss: 1.021, r2: -2.102, rate:0.750, acc:0.875 lr 8.985051499572172e-05\n",
      "epoch:8 step:5200 train loss: 0.701, r2: -1.045, rate:0.625, acc:0.312 val loss: 1.213, r2: 0.182, rate:0.688, acc:0.625 lr 8.985051499572172e-05\n",
      "epoch:8 step:5300 train loss: 1.504, r2: -0.377, rate:0.812, acc:0.625 val loss: 0.870, r2: 0.444, rate:0.750, acc:0.625 lr 8.985051499572172e-05\n",
      "epoch:8 step:5400 train loss: 0.562, r2: 0.311, rate:0.875, acc:0.938 val loss: 1.374, r2: 0.642, rate:0.438, acc:0.438 lr 8.985051499572172e-05\n",
      "epoch:8 step:5500 train loss: 1.188, r2: -9.217, rate:0.625, acc:0.562 val loss: 1.050, r2: -0.908, rate:0.562, acc:0.500 lr 8.086546349614955e-05\n",
      "epoch:8 step:5600 train loss: 6.008, r2: -17.617, rate:0.938, acc:0.625 val loss: 0.532, r2: -0.599, rate:0.750, acc:1.000 lr 8.086546349614955e-05\n",
      "epoch:8 step:5700 train loss: 0.714, r2: -0.938, rate:0.562, acc:0.375 val loss: 0.724, r2: 0.372, rate:0.875, acc:0.750 lr 8.086546349614955e-05\n",
      "epoch:8 step:5800 train loss: 1.086, r2: 0.136, rate:0.750, acc:0.688 val loss: 1.644, r2: -0.341, rate:0.625, acc:0.688 lr 8.086546349614955e-05\n",
      "epoch:8 step:5900 train loss: 0.582, r2: 0.699, rate:0.812, acc:0.812 val loss: 0.813, r2: 0.030, rate:0.688, acc:0.438 lr 8.086546349614955e-05\n",
      "epoch:8 step:6000 train loss: 1.234, r2: -0.608, rate:0.375, acc:0.500 val loss: 1.098, r2: -0.699, rate:0.562, acc:0.750 lr 8.086546349614955e-05\n",
      "epoch:8 step:6100 train loss: 0.856, r2: -0.210, rate:0.500, acc:0.750 val loss: 0.686, r2: 0.090, rate:0.812, acc:0.875 lr 8.086546349614955e-05\n",
      "epoch:8 step:6200 train loss: 1.198, r2: -1.478, rate:0.438, acc:0.188 val loss: 0.853, r2: 0.317, rate:0.875, acc:0.875 lr 8.086546349614955e-05\n",
      "epoch:8 step:6300 train loss: 0.753, r2: 0.451, rate:0.750, acc:0.312 val loss: 0.859, r2: 0.344, rate:0.812, acc:0.750 lr 8.086546349614955e-05\n",
      "epoch:8 step:6400 train loss: 0.601, r2: 0.728, rate:0.938, acc:0.625 val loss: 0.859, r2: -4.492, rate:0.500, acc:0.125 lr 8.086546349614955e-05\n",
      "epoch:8 step:6500 train loss: 0.635, r2: -0.075, rate:0.812, acc:1.000 val loss: 2.070, r2: -1.100, rate:0.750, acc:0.875 lr 8.086546349614955e-05\n",
      "epoch:8 step:6600 train loss: 0.726, r2: 0.321, rate:0.625, acc:0.750 val loss: 1.585, r2: 0.199, rate:0.812, acc:0.750 lr 7.27789171465346e-05\n",
      "epoch:8 step:6700 train loss: 0.959, r2: 0.214, rate:0.688, acc:0.438 val loss: 0.954, r2: 0.071, rate:0.625, acc:0.625 lr 7.27789171465346e-05\n",
      "epoch:8 step:6800 train loss: 0.717, r2: 0.226, rate:0.812, acc:0.812 val loss: 0.830, r2: 0.610, rate:0.812, acc:0.875 lr 7.27789171465346e-05\n",
      "epoch:8 step:6900 train loss: 0.850, r2: 0.404, rate:0.812, acc:0.875 val loss: 1.046, r2: -0.864, rate:0.625, acc:0.438 lr 7.27789171465346e-05\n",
      "epoch:8 step:7000 train loss: 1.269, r2: 0.170, rate:0.812, acc:0.625 val loss: 1.059, r2: -0.273, rate:0.625, acc:0.750 lr 7.27789171465346e-05\n",
      "epoch:8 step:7100 train loss: 0.954, r2: 0.144, rate:0.812, acc:0.750 val loss: 1.170, r2: -0.391, rate:0.750, acc:0.625 lr 7.27789171465346e-05\n",
      "epoch:8 step:7200 train loss: 1.157, r2: -1.342, rate:0.812, acc:0.625 val loss: 3.603, r2: 0.143, rate:0.688, acc:0.688 lr 7.27789171465346e-05\n",
      "epoch:8 step:7300 train loss: 0.958, r2: -0.065, rate:0.688, acc:0.562 val loss: 0.635, r2: -0.931, rate:0.688, acc:0.625 lr 7.27789171465346e-05\n",
      "epoch:8 step:7400 train loss: 0.987, r2: -0.270, rate:0.562, acc:0.500 val loss: 2.706, r2: -0.914, rate:0.750, acc:0.500 lr 7.27789171465346e-05\n",
      "epoch:8 step:7500 train loss: 0.763, r2: -0.213, rate:0.750, acc:0.812 val loss: 0.728, r2: 0.321, rate:0.500, acc:0.625 lr 7.27789171465346e-05\n",
      "epoch:8 step:7600 train loss: 1.315, r2: -0.072, rate:0.688, acc:0.938 val loss: 1.496, r2: -0.326, rate:0.562, acc:0.688 lr 7.27789171465346e-05\n",
      "epoch:8 step:7700 train loss: 0.615, r2: 0.316, rate:0.812, acc:0.688 val loss: 0.740, r2: 0.387, rate:0.750, acc:0.438 lr 6.550102543188114e-05\n",
      "epoch:8 step:7800 train loss: 0.699, r2: -0.196, rate:0.750, acc:0.750 val loss: 0.887, r2: -0.596, rate:0.688, acc:0.438 lr 6.550102543188114e-05\n",
      "epoch:8 step:0.328 lr 0.000066\n",
      "epoch:9 step:0 train loss: 0.661, r2: 0.386, rate:0.688, acc:0.875 val loss: 1.348, r2: 0.315, rate:0.875, acc:0.812 lr 6.550102543188114e-05\n",
      "epoch:9 step:100 train loss: 2.088, r2: -0.598, rate:0.688, acc:0.312 val loss: 1.101, r2: 0.548, rate:0.875, acc:0.812 lr 6.550102543188114e-05\n",
      "epoch:9 step:200 train loss: 8.355, r2: -16.325, rate:0.625, acc:0.750 val loss: 0.622, r2: 0.658, rate:0.875, acc:0.812 lr 6.550102543188114e-05\n",
      "epoch:9 step:300 train loss: 0.586, r2: -2.044, rate:0.750, acc:0.875 val loss: 2.122, r2: 0.201, rate:0.938, acc:0.688 lr 6.550102543188114e-05\n",
      "epoch:9 step:400 train loss: 0.732, r2: -0.579, rate:0.750, acc:0.625 val loss: 0.725, r2: 0.604, rate:0.812, acc:0.812 lr 6.550102543188114e-05\n",
      "epoch:9 step:500 train loss: 1.089, r2: 0.097, rate:0.750, acc:0.688 val loss: 0.604, r2: 0.124, rate:0.750, acc:0.688 lr 6.550102543188114e-05\n",
      "epoch:9 step:600 train loss: 0.781, r2: -0.259, rate:0.812, acc:0.875 val loss: 0.360, r2: 0.858, rate:1.000, acc:0.938 lr 6.550102543188114e-05\n",
      "epoch:9 step:700 train loss: 1.178, r2: 0.656, rate:0.688, acc:0.812 val loss: 0.936, r2: -0.057, rate:0.500, acc:0.562 lr 6.550102543188114e-05\n",
      "epoch:9 step:800 train loss: 0.586, r2: 0.485, rate:0.875, acc:0.875 val loss: 0.996, r2: 0.074, rate:0.562, acc:0.875 lr 6.550102543188114e-05\n",
      "epoch:9 step:900 train loss: 0.584, r2: 0.074, rate:0.688, acc:0.875 val loss: 0.919, r2: 0.390, rate:0.688, acc:0.812 lr 5.895092288869303e-05\n",
      "epoch:9 step:1000 train loss: 0.731, r2: 0.562, rate:0.750, acc:0.562 val loss: 1.562, r2: -0.702, rate:0.625, acc:0.375 lr 5.895092288869303e-05\n",
      "epoch:9 step:1100 train loss: 1.787, r2: -0.575, rate:0.688, acc:0.812 val loss: 2.061, r2: -0.936, rate:0.812, acc:0.625 lr 5.895092288869303e-05\n",
      "epoch:9 step:1200 train loss: 1.201, r2: -1.203, rate:0.562, acc:0.500 val loss: 0.916, r2: -0.874, rate:0.750, acc:0.875 lr 5.895092288869303e-05\n",
      "epoch:9 step:1300 train loss: 1.063, r2: -0.319, rate:0.688, acc:0.938 val loss: 0.917, r2: 0.411, rate:0.750, acc:0.750 lr 5.895092288869303e-05\n",
      "epoch:9 step:1400 train loss: 1.120, r2: -0.720, rate:0.375, acc:0.750 val loss: 1.310, r2: -1.636, rate:0.812, acc:0.562 lr 5.895092288869303e-05\n",
      "epoch:9 step:1500 train loss: 0.729, r2: -0.586, rate:0.500, acc:0.438 val loss: 0.665, r2: 0.604, rate:0.812, acc:0.375 lr 5.895092288869303e-05\n",
      "epoch:9 step:1600 train loss: 0.738, r2: 0.141, rate:0.875, acc:0.688 val loss: 1.220, r2: 0.220, rate:0.750, acc:0.750 lr 5.895092288869303e-05\n",
      "epoch:9 step:1700 train loss: 1.427, r2: 0.395, rate:0.562, acc:0.625 val loss: 1.837, r2: -0.615, rate:0.688, acc:0.625 lr 5.895092288869303e-05\n",
      "epoch:9 step:1800 train loss: 0.802, r2: -0.407, rate:0.688, acc:0.562 val loss: 0.636, r2: 0.331, rate:0.750, acc:0.625 lr 5.895092288869303e-05\n",
      "epoch:9 step:1900 train loss: 1.210, r2: -2.853, rate:0.625, acc:0.438 val loss: 0.944, r2: -0.777, rate:0.625, acc:0.562 lr 5.895092288869303e-05\n",
      "epoch:9 step:2000 train loss: 1.157, r2: -1.071, rate:0.562, acc:0.875 val loss: 2.485, r2: -0.075, rate:0.688, acc:0.812 lr 5.305583059982373e-05\n",
      "epoch:9 step:2100 train loss: 1.812, r2: -0.250, rate:0.750, acc:0.688 val loss: 2.499, r2: -0.199, rate:0.500, acc:0.438 lr 5.305583059982373e-05\n",
      "epoch:9 step:2200 train loss: 0.807, r2: 0.081, rate:0.875, acc:0.812 val loss: 3.972, r2: -0.293, rate:0.750, acc:0.750 lr 5.305583059982373e-05\n",
      "epoch:9 step:2300 train loss: 0.835, r2: -0.323, rate:0.875, acc:1.000 val loss: 1.136, r2: 0.567, rate:0.688, acc:0.688 lr 5.305583059982373e-05\n",
      "epoch:9 step:2400 train loss: 0.992, r2: -3.270, rate:0.938, acc:0.750 val loss: 1.027, r2: -0.254, rate:0.688, acc:0.688 lr 5.305583059982373e-05\n",
      "epoch:9 step:2500 train loss: 1.542, r2: -0.633, rate:0.812, acc:0.438 val loss: 2.033, r2: -0.936, rate:0.562, acc:0.688 lr 5.305583059982373e-05\n",
      "epoch:9 step:2600 train loss: 1.121, r2: -0.457, rate:0.688, acc:0.688 val loss: 0.678, r2: 0.038, rate:0.875, acc:0.625 lr 5.305583059982373e-05\n",
      "epoch:9 step:2700 train loss: 0.688, r2: -2.605, rate:0.625, acc:0.875 val loss: 0.793, r2: 0.304, rate:0.688, acc:0.625 lr 5.305583059982373e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:2800 train loss: 1.455, r2: -3.287, rate:0.438, acc:0.312 val loss: 0.745, r2: -0.003, rate:0.938, acc:0.562 lr 5.305583059982373e-05\n",
      "epoch:9 step:2900 train loss: 2.493, r2: -1.461, rate:0.875, acc:1.000 val loss: 0.692, r2: 0.321, rate:0.688, acc:0.625 lr 5.305583059982373e-05\n",
      "epoch:9 step:3000 train loss: 0.805, r2: -0.448, rate:0.500, acc:0.562 val loss: 0.694, r2: 0.697, rate:0.750, acc:0.750 lr 5.305583059982373e-05\n",
      "epoch:9 step:3100 train loss: 0.898, r2: 0.068, rate:0.688, acc:0.625 val loss: 1.016, r2: -1.136, rate:0.812, acc:0.625 lr 4.7750247539841356e-05\n",
      "epoch:9 step:3200 train loss: 0.859, r2: -2.290, rate:0.500, acc:0.500 val loss: 0.849, r2: 0.575, rate:0.562, acc:0.500 lr 4.7750247539841356e-05\n",
      "epoch:9 step:3300 train loss: 0.727, r2: 0.083, rate:0.812, acc:0.312 val loss: 0.904, r2: -0.823, rate:0.625, acc:0.375 lr 4.7750247539841356e-05\n",
      "epoch:9 step:3400 train loss: 1.044, r2: -0.440, rate:0.375, acc:0.438 val loss: 0.893, r2: -0.291, rate:0.812, acc:0.875 lr 4.7750247539841356e-05\n",
      "epoch:9 step:3500 train loss: 1.066, r2: 0.685, rate:0.750, acc:1.000 val loss: 0.816, r2: -0.871, rate:0.625, acc:0.688 lr 4.7750247539841356e-05\n",
      "epoch:9 step:3600 train loss: 0.748, r2: -0.621, rate:0.562, acc:0.375 val loss: 0.850, r2: 0.081, rate:0.688, acc:0.625 lr 4.7750247539841356e-05\n",
      "epoch:9 step:3700 train loss: 0.944, r2: 0.035, rate:0.750, acc:0.375 val loss: 0.813, r2: 0.078, rate:0.562, acc:0.562 lr 4.7750247539841356e-05\n",
      "epoch:9 step:3800 train loss: 4.153, r2: 0.337, rate:0.750, acc:0.750 val loss: 1.026, r2: 0.277, rate:0.625, acc:0.625 lr 4.7750247539841356e-05\n",
      "epoch:9 step:3900 train loss: 0.819, r2: -1.128, rate:0.625, acc:0.500 val loss: 0.743, r2: 0.339, rate:0.688, acc:0.750 lr 4.7750247539841356e-05\n",
      "epoch:9 step:4000 train loss: 0.738, r2: -1.945, rate:0.375, acc:0.562 val loss: 0.748, r2: -0.078, rate:0.688, acc:0.875 lr 4.7750247539841356e-05\n",
      "epoch:9 step:4100 train loss: 0.638, r2: 0.555, rate:0.750, acc:0.812 val loss: 2.377, r2: 0.368, rate:0.875, acc:0.625 lr 4.7750247539841356e-05\n",
      "epoch:9 step:4200 train loss: 1.152, r2: -0.429, rate:0.500, acc:0.688 val loss: 0.805, r2: -0.242, rate:0.688, acc:0.500 lr 4.297522278585722e-05\n",
      "epoch:9 step:4300 train loss: 0.756, r2: -0.071, rate:0.625, acc:0.500 val loss: 1.317, r2: -0.549, rate:0.688, acc:0.500 lr 4.297522278585722e-05\n",
      "epoch:9 step:4400 train loss: 0.790, r2: 0.586, rate:0.938, acc:0.562 val loss: 1.756, r2: 0.318, rate:0.750, acc:0.750 lr 4.297522278585722e-05\n",
      "epoch:9 step:4500 train loss: 0.705, r2: 0.696, rate:0.812, acc:0.688 val loss: 0.814, r2: 0.097, rate:0.625, acc:0.250 lr 4.297522278585722e-05\n",
      "epoch:9 step:4600 train loss: 1.375, r2: -4.444, rate:0.562, acc:0.500 val loss: 0.912, r2: -1.942, rate:0.562, acc:0.812 lr 4.297522278585722e-05\n",
      "epoch:9 step:4700 train loss: 3.614, r2: -0.010, rate:0.688, acc:0.500 val loss: 0.977, r2: -0.150, rate:0.625, acc:0.688 lr 4.297522278585722e-05\n",
      "epoch:9 step:4800 train loss: 0.824, r2: 0.003, rate:0.625, acc:0.812 val loss: 1.206, r2: 0.317, rate:0.750, acc:0.500 lr 4.297522278585722e-05\n",
      "epoch:9 step:4900 train loss: 3.027, r2: -11.245, rate:0.625, acc:0.500 val loss: 0.603, r2: 0.627, rate:0.750, acc:0.750 lr 4.297522278585722e-05\n",
      "epoch:9 step:5000 train loss: 3.267, r2: -1.054, rate:0.375, acc:0.500 val loss: 0.689, r2: 0.557, rate:0.938, acc:0.625 lr 4.297522278585722e-05\n",
      "epoch:9 step:5100 train loss: 0.929, r2: -2.540, rate:0.500, acc:0.438 val loss: 0.660, r2: 0.026, rate:0.625, acc:0.500 lr 4.297522278585722e-05\n",
      "epoch:9 step:5200 train loss: 0.595, r2: 0.779, rate:0.688, acc:0.875 val loss: 2.294, r2: -0.075, rate:0.750, acc:0.750 lr 4.297522278585722e-05\n",
      "epoch:9 step:5300 train loss: 1.841, r2: -0.420, rate:0.812, acc:0.938 val loss: 0.873, r2: 0.507, rate:0.688, acc:0.562 lr 3.86777005072715e-05\n",
      "epoch:9 step:5400 train loss: 1.507, r2: -3.296, rate:0.750, acc:0.438 val loss: 0.650, r2: 0.536, rate:0.750, acc:0.750 lr 3.86777005072715e-05\n",
      "epoch:9 step:5500 train loss: 0.913, r2: -0.908, rate:0.375, acc:0.250 val loss: 1.015, r2: -0.710, rate:0.688, acc:0.562 lr 3.86777005072715e-05\n",
      "epoch:9 step:5600 train loss: 1.964, r2: -1.982, rate:0.750, acc:0.812 val loss: 0.799, r2: 0.548, rate:0.688, acc:0.688 lr 3.86777005072715e-05\n",
      "epoch:9 step:5700 train loss: 1.589, r2: -0.968, rate:0.438, acc:0.625 val loss: 0.532, r2: 0.245, rate:0.750, acc:0.875 lr 3.86777005072715e-05\n",
      "epoch:9 step:5800 train loss: 0.973, r2: 0.732, rate:1.000, acc:0.938 val loss: 0.935, r2: 0.269, rate:0.625, acc:0.625 lr 3.86777005072715e-05\n",
      "epoch:9 step:5900 train loss: 1.485, r2: 0.053, rate:0.875, acc:0.688 val loss: 0.910, r2: 0.202, rate:0.812, acc:0.625 lr 3.86777005072715e-05\n",
      "epoch:9 step:6000 train loss: 1.046, r2: -0.511, rate:0.625, acc:0.812 val loss: 0.750, r2: -0.824, rate:0.750, acc:0.688 lr 3.86777005072715e-05\n",
      "epoch:9 step:6100 train loss: 0.726, r2: -0.361, rate:0.750, acc:0.312 val loss: 1.242, r2: 0.269, rate:0.688, acc:0.750 lr 3.86777005072715e-05\n",
      "epoch:9 step:6200 train loss: 0.844, r2: 0.713, rate:0.875, acc:0.750 val loss: 0.796, r2: 0.133, rate:0.688, acc:0.562 lr 3.86777005072715e-05\n",
      "epoch:9 step:6300 train loss: 0.653, r2: 0.363, rate:0.750, acc:0.562 val loss: 0.682, r2: -0.426, rate:0.688, acc:0.750 lr 3.86777005072715e-05\n",
      "epoch:9 step:6400 train loss: 0.916, r2: -0.380, rate:0.500, acc:0.688 val loss: 0.740, r2: -0.109, rate:0.875, acc:0.812 lr 3.480993045654435e-05\n",
      "epoch:9 step:6500 train loss: 2.743, r2: 0.551, rate:0.812, acc:0.688 val loss: 0.568, r2: 0.397, rate:0.938, acc:0.938 lr 3.480993045654435e-05\n",
      "epoch:9 step:6600 train loss: 0.938, r2: -0.126, rate:0.812, acc:0.688 val loss: 1.902, r2: 0.111, rate:0.562, acc:0.562 lr 3.480993045654435e-05\n",
      "epoch:9 step:6700 train loss: 1.100, r2: -0.297, rate:0.562, acc:0.375 val loss: 0.944, r2: -4.020, rate:0.438, acc:0.438 lr 3.480993045654435e-05\n",
      "epoch:9 step:6800 train loss: 0.923, r2: -0.842, rate:0.375, acc:0.438 val loss: 0.704, r2: -0.266, rate:0.812, acc:0.688 lr 3.480993045654435e-05\n",
      "epoch:9 step:6900 train loss: 0.699, r2: -0.558, rate:0.812, acc:0.625 val loss: 0.765, r2: -0.716, rate:0.562, acc:0.812 lr 3.480993045654435e-05\n",
      "epoch:9 step:7000 train loss: 0.780, r2: -1.009, rate:0.688, acc:0.438 val loss: 1.550, r2: 0.264, rate:0.625, acc:0.562 lr 3.480993045654435e-05\n",
      "epoch:9 step:7100 train loss: 0.964, r2: -3.801, rate:0.312, acc:0.438 val loss: 0.795, r2: 0.127, rate:0.750, acc:0.812 lr 3.480993045654435e-05\n",
      "epoch:9 step:7200 train loss: 0.914, r2: -1.456, rate:0.625, acc:0.312 val loss: 0.769, r2: -0.739, rate:0.562, acc:0.500 lr 3.480993045654435e-05\n",
      "epoch:9 step:7300 train loss: 0.764, r2: 0.180, rate:0.500, acc:0.562 val loss: 0.630, r2: 0.434, rate:0.750, acc:0.750 lr 3.480993045654435e-05\n",
      "epoch:9 step:7400 train loss: 0.683, r2: -2.891, rate:0.562, acc:0.812 val loss: 0.638, r2: 0.073, rate:0.812, acc:0.625 lr 3.480993045654435e-05\n",
      "epoch:9 step:7500 train loss: 0.845, r2: 0.530, rate:0.750, acc:0.688 val loss: 0.841, r2: 0.441, rate:0.562, acc:0.875 lr 3.1328937410889915e-05\n",
      "epoch:9 step:7600 train loss: 1.131, r2: -0.040, rate:0.938, acc:0.812 val loss: 0.713, r2: 0.546, rate:0.750, acc:0.688 lr 3.1328937410889915e-05\n",
      "epoch:9 step:7700 train loss: 0.559, r2: 0.481, rate:0.750, acc:0.938 val loss: 0.983, r2: 0.472, rate:0.625, acc:0.500 lr 3.1328937410889915e-05\n",
      "epoch:9 step:7800 train loss: 1.453, r2: -1.127, rate:0.938, acc:0.688 val loss: 0.803, r2: 0.218, rate:0.438, acc:0.688 lr 3.1328937410889915e-05\n",
      "epoch:9 step:0.326 lr 0.000031\n",
      "epoch:10 step:0 train loss: 1.020, r2: -10.905, rate:0.500, acc:0.438 val loss: 2.680, r2: -0.564, rate:0.688, acc:0.562 lr 3.1328937410889915e-05\n",
      "epoch:10 step:100 train loss: 0.765, r2: 0.033, rate:0.562, acc:0.750 val loss: 0.927, r2: -0.760, rate:0.438, acc:0.688 lr 3.1328937410889915e-05\n",
      "epoch:10 step:200 train loss: 4.941, r2: -1.444, rate:0.688, acc:0.438 val loss: 0.689, r2: 0.595, rate:0.750, acc:0.875 lr 3.1328937410889915e-05\n",
      "epoch:10 step:300 train loss: 0.771, r2: 0.157, rate:0.812, acc:0.375 val loss: 0.781, r2: 0.086, rate:0.688, acc:0.625 lr 3.1328937410889915e-05\n",
      "epoch:10 step:400 train loss: 1.792, r2: -0.027, rate:0.750, acc:0.812 val loss: 1.395, r2: 0.212, rate:0.562, acc:0.500 lr 3.1328937410889915e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:500 train loss: 0.808, r2: 0.361, rate:0.625, acc:0.688 val loss: 1.572, r2: -0.004, rate:0.438, acc:0.375 lr 3.1328937410889915e-05\n",
      "epoch:10 step:600 train loss: 0.737, r2: 0.220, rate:0.625, acc:0.562 val loss: 1.070, r2: -0.289, rate:0.688, acc:0.688 lr 3.1328937410889915e-05\n",
      "epoch:10 step:700 train loss: 0.673, r2: 0.387, rate:0.812, acc:0.688 val loss: 1.027, r2: 0.412, rate:0.500, acc:0.688 lr 2.8196043669800925e-05\n",
      "epoch:10 step:800 train loss: 1.296, r2: 0.567, rate:0.688, acc:0.625 val loss: 3.603, r2: 0.386, rate:0.812, acc:0.750 lr 2.8196043669800925e-05\n",
      "epoch:10 step:900 train loss: 1.842, r2: -0.864, rate:0.438, acc:0.250 val loss: 0.540, r2: 0.826, rate:0.812, acc:0.938 lr 2.8196043669800925e-05\n",
      "epoch:10 step:1000 train loss: 0.639, r2: -2.658, rate:0.750, acc:1.000 val loss: 0.821, r2: -0.240, rate:0.812, acc:0.500 lr 2.8196043669800925e-05\n",
      "epoch:10 step:1100 train loss: 1.988, r2: -0.020, rate:1.000, acc:1.000 val loss: 0.644, r2: 0.669, rate:0.812, acc:0.688 lr 2.8196043669800925e-05\n",
      "epoch:10 step:1200 train loss: 0.657, r2: 0.448, rate:0.562, acc:0.688 val loss: 1.361, r2: -0.923, rate:0.750, acc:0.562 lr 2.8196043669800925e-05\n",
      "epoch:10 step:1300 train loss: 0.926, r2: 0.476, rate:0.750, acc:0.688 val loss: 2.005, r2: 0.100, rate:0.688, acc:0.750 lr 2.8196043669800925e-05\n",
      "epoch:10 step:1400 train loss: 3.264, r2: -8.006, rate:0.875, acc:0.938 val loss: 1.448, r2: 0.261, rate:0.812, acc:0.562 lr 2.8196043669800925e-05\n",
      "epoch:10 step:1500 train loss: 0.698, r2: 0.164, rate:0.625, acc:0.375 val loss: 0.710, r2: 0.342, rate:0.750, acc:0.625 lr 2.8196043669800925e-05\n",
      "epoch:10 step:1600 train loss: 0.722, r2: -13.941, rate:0.500, acc:0.750 val loss: 0.963, r2: 0.190, rate:0.812, acc:0.812 lr 2.8196043669800925e-05\n",
      "epoch:10 step:1700 train loss: 1.540, r2: 0.304, rate:0.562, acc:0.688 val loss: 0.919, r2: 0.218, rate:0.750, acc:0.750 lr 2.8196043669800925e-05\n",
      "epoch:10 step:1800 train loss: 2.322, r2: 0.021, rate:0.688, acc:0.625 val loss: 0.833, r2: 0.279, rate:0.750, acc:0.625 lr 2.5376439302820835e-05\n",
      "epoch:10 step:1900 train loss: 0.582, r2: -1.066, rate:0.875, acc:0.938 val loss: 1.742, r2: -24.988, rate:0.812, acc:1.000 lr 2.5376439302820835e-05\n",
      "epoch:10 step:2000 train loss: 1.576, r2: 0.240, rate:0.500, acc:0.500 val loss: 2.852, r2: 0.039, rate:0.688, acc:0.625 lr 2.5376439302820835e-05\n",
      "epoch:10 step:2100 train loss: 0.674, r2: 0.410, rate:0.625, acc:0.750 val loss: 0.892, r2: 0.027, rate:0.812, acc:0.688 lr 2.5376439302820835e-05\n",
      "epoch:10 step:2200 train loss: 3.141, r2: -2.488, rate:0.750, acc:0.875 val loss: 0.740, r2: 0.429, rate:0.750, acc:0.688 lr 2.5376439302820835e-05\n",
      "epoch:10 step:2300 train loss: 7.409, r2: -5.318, rate:0.750, acc:0.688 val loss: 0.961, r2: -1.599, rate:0.688, acc:0.688 lr 2.5376439302820835e-05\n",
      "epoch:10 step:2400 train loss: 0.736, r2: 0.282, rate:0.500, acc:0.812 val loss: 1.104, r2: 0.492, rate:0.562, acc:0.688 lr 2.5376439302820835e-05\n",
      "epoch:10 step:2500 train loss: 0.681, r2: -3.422, rate:0.812, acc:0.938 val loss: 0.911, r2: -2.148, rate:0.625, acc:0.812 lr 2.5376439302820835e-05\n",
      "epoch:10 step:2600 train loss: 1.700, r2: -0.117, rate:0.812, acc:0.625 val loss: 0.769, r2: -0.656, rate:0.562, acc:0.562 lr 2.5376439302820835e-05\n",
      "epoch:10 step:2700 train loss: 1.477, r2: -1.195, rate:0.750, acc:0.625 val loss: 0.840, r2: 0.201, rate:0.625, acc:0.562 lr 2.5376439302820835e-05\n",
      "epoch:10 step:2800 train loss: 0.729, r2: 0.072, rate:0.625, acc:0.750 val loss: 0.516, r2: -1.474, rate:1.000, acc:1.000 lr 2.5376439302820835e-05\n",
      "epoch:10 step:2900 train loss: 1.751, r2: -0.812, rate:0.562, acc:0.500 val loss: 0.556, r2: -7.315, rate:0.875, acc:0.938 lr 2.2838795372538753e-05\n",
      "epoch:10 step:3000 train loss: 0.875, r2: -2.600, rate:0.562, acc:0.250 val loss: 1.053, r2: 0.260, rate:0.750, acc:0.812 lr 2.2838795372538753e-05\n",
      "epoch:10 step:3100 train loss: 7.131, r2: -7.794, rate:0.688, acc:0.688 val loss: 0.881, r2: -0.183, rate:0.688, acc:0.688 lr 2.2838795372538753e-05\n",
      "epoch:10 step:3200 train loss: 0.739, r2: 0.211, rate:0.812, acc:0.750 val loss: 1.790, r2: -1.324, rate:0.500, acc:0.438 lr 2.2838795372538753e-05\n",
      "epoch:10 step:3300 train loss: 0.803, r2: -2.367, rate:0.562, acc:0.625 val loss: 0.916, r2: 0.472, rate:0.875, acc:0.688 lr 2.2838795372538753e-05\n",
      "epoch:10 step:3400 train loss: 1.384, r2: -1.361, rate:0.562, acc:0.812 val loss: 0.954, r2: 0.376, rate:0.688, acc:0.750 lr 2.2838795372538753e-05\n",
      "epoch:10 step:3500 train loss: 2.414, r2: -0.529, rate:0.812, acc:1.000 val loss: 4.849, r2: -9.887, rate:0.500, acc:0.438 lr 2.2838795372538753e-05\n",
      "epoch:10 step:3600 train loss: 1.118, r2: 0.310, rate:0.688, acc:0.812 val loss: 0.681, r2: 0.312, rate:0.812, acc:0.750 lr 2.2838795372538753e-05\n",
      "epoch:10 step:3700 train loss: 1.361, r2: -2.060, rate:0.438, acc:0.312 val loss: 1.170, r2: 0.150, rate:0.812, acc:0.688 lr 2.2838795372538753e-05\n",
      "epoch:10 step:3800 train loss: 0.861, r2: -0.183, rate:0.625, acc:0.812 val loss: 2.123, r2: 0.037, rate:0.500, acc:0.438 lr 2.2838795372538753e-05\n",
      "epoch:10 step:3900 train loss: 0.710, r2: 0.473, rate:0.812, acc:0.562 val loss: 0.875, r2: 0.149, rate:0.625, acc:0.438 lr 2.2838795372538753e-05\n",
      "epoch:10 step:4000 train loss: 3.279, r2: -1.873, rate:0.375, acc:0.375 val loss: 0.636, r2: -1.681, rate:0.688, acc:0.438 lr 2.0554915835284878e-05\n",
      "epoch:10 step:4100 train loss: 1.976, r2: 0.250, rate:0.562, acc:0.562 val loss: 1.107, r2: -1.105, rate:0.312, acc:0.438 lr 2.0554915835284878e-05\n",
      "epoch:10 step:4200 train loss: 0.679, r2: 0.670, rate:0.812, acc:0.812 val loss: 1.057, r2: -0.276, rate:0.750, acc:0.500 lr 2.0554915835284878e-05\n",
      "epoch:10 step:4300 train loss: 0.854, r2: 0.694, rate:1.000, acc:0.812 val loss: 0.939, r2: 0.441, rate:0.688, acc:0.625 lr 2.0554915835284878e-05\n",
      "epoch:10 step:4400 train loss: 0.994, r2: -0.699, rate:0.812, acc:0.812 val loss: 1.036, r2: 0.116, rate:0.750, acc:0.750 lr 2.0554915835284878e-05\n",
      "epoch:10 step:4500 train loss: 0.669, r2: -0.380, rate:0.750, acc:0.625 val loss: 0.879, r2: 0.098, rate:0.750, acc:0.875 lr 2.0554915835284878e-05\n",
      "epoch:10 step:4600 train loss: 0.725, r2: -0.053, rate:0.688, acc:0.562 val loss: 1.166, r2: -2.798, rate:0.625, acc:0.875 lr 2.0554915835284878e-05\n",
      "epoch:10 step:4700 train loss: 1.404, r2: -0.418, rate:0.312, acc:0.250 val loss: 0.877, r2: 0.505, rate:0.688, acc:0.750 lr 2.0554915835284878e-05\n",
      "epoch:10 step:4800 train loss: 0.729, r2: 0.795, rate:0.812, acc:0.938 val loss: 1.500, r2: 0.050, rate:0.500, acc:0.438 lr 2.0554915835284878e-05\n",
      "epoch:10 step:4900 train loss: 0.628, r2: 0.303, rate:0.875, acc:0.875 val loss: 2.907, r2: -0.426, rate:0.562, acc:0.500 lr 2.0554915835284878e-05\n",
      "epoch:10 step:5000 train loss: 3.002, r2: -2.267, rate:0.688, acc:0.750 val loss: 0.845, r2: 0.301, rate:0.750, acc:0.812 lr 2.0554915835284878e-05\n",
      "epoch:10 step:5100 train loss: 0.889, r2: 0.234, rate:0.625, acc:0.625 val loss: 3.048, r2: 0.015, rate:0.688, acc:0.875 lr 1.849942425175639e-05\n",
      "epoch:10 step:5200 train loss: 1.027, r2: -0.689, rate:0.500, acc:0.188 val loss: 0.864, r2: -1.109, rate:0.438, acc:0.500 lr 1.849942425175639e-05\n",
      "epoch:10 step:5300 train loss: 0.741, r2: 0.454, rate:0.625, acc:0.562 val loss: 1.165, r2: -0.249, rate:0.562, acc:0.438 lr 1.849942425175639e-05\n",
      "epoch:10 step:5400 train loss: 0.505, r2: 0.424, rate:0.875, acc:1.000 val loss: 2.156, r2: 0.294, rate:0.688, acc:0.625 lr 1.849942425175639e-05\n",
      "epoch:10 step:5500 train loss: 1.063, r2: 0.482, rate:0.688, acc:0.688 val loss: 0.714, r2: 0.364, rate:0.875, acc:0.688 lr 1.849942425175639e-05\n",
      "epoch:10 step:5600 train loss: 2.370, r2: -3.255, rate:0.500, acc:0.438 val loss: 0.990, r2: -0.759, rate:0.500, acc:0.812 lr 1.849942425175639e-05\n",
      "epoch:10 step:5700 train loss: 0.574, r2: 0.768, rate:0.812, acc:0.750 val loss: 0.906, r2: -1.571, rate:0.750, acc:0.625 lr 1.849942425175639e-05\n",
      "epoch:10 step:5800 train loss: 1.563, r2: 0.345, rate:0.688, acc:0.625 val loss: 0.550, r2: 0.411, rate:0.688, acc:0.812 lr 1.849942425175639e-05\n",
      "epoch:10 step:5900 train loss: 0.790, r2: 0.496, rate:0.562, acc:0.688 val loss: 0.819, r2: -2.459, rate:0.438, acc:0.500 lr 1.849942425175639e-05\n",
      "epoch:10 step:6000 train loss: 0.748, r2: 0.145, rate:0.625, acc:0.562 val loss: 1.353, r2: -0.219, rate:0.750, acc:0.500 lr 1.849942425175639e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:6100 train loss: 0.817, r2: 0.412, rate:0.562, acc:0.750 val loss: 1.609, r2: 0.166, rate:0.562, acc:0.562 lr 1.849942425175639e-05\n",
      "epoch:10 step:6200 train loss: 0.975, r2: 0.786, rate:0.812, acc:0.688 val loss: 0.748, r2: -2.488, rate:0.625, acc:0.438 lr 1.6649481826580752e-05\n",
      "epoch:10 step:6300 train loss: 0.640, r2: -0.203, rate:0.750, acc:0.875 val loss: 1.492, r2: -2.820, rate:0.562, acc:0.625 lr 1.6649481826580752e-05\n",
      "epoch:10 step:6400 train loss: 2.153, r2: -2.224, rate:0.562, acc:0.562 val loss: 0.956, r2: -2.167, rate:0.562, acc:0.312 lr 1.6649481826580752e-05\n",
      "epoch:10 step:6500 train loss: 0.540, r2: 0.622, rate:0.938, acc:0.938 val loss: 0.799, r2: 0.253, rate:0.750, acc:0.688 lr 1.6649481826580752e-05\n",
      "epoch:10 step:6600 train loss: 0.732, r2: -1.894, rate:0.562, acc:0.375 val loss: 1.077, r2: 0.464, rate:0.688, acc:0.750 lr 1.6649481826580752e-05\n",
      "epoch:10 step:6700 train loss: 2.479, r2: -7.322, rate:0.812, acc:1.000 val loss: 1.173, r2: -1.076, rate:0.500, acc:0.250 lr 1.6649481826580752e-05\n",
      "epoch:10 step:6800 train loss: 0.578, r2: 0.680, rate:0.875, acc:0.562 val loss: 0.977, r2: 0.450, rate:0.812, acc:0.562 lr 1.6649481826580752e-05\n",
      "epoch:10 step:6900 train loss: 0.975, r2: 0.492, rate:0.688, acc:0.562 val loss: 0.736, r2: 0.604, rate:0.562, acc:0.562 lr 1.6649481826580752e-05\n",
      "epoch:10 step:7000 train loss: 0.717, r2: -0.167, rate:0.750, acc:0.375 val loss: 1.533, r2: -5.901, rate:0.312, acc:0.500 lr 1.6649481826580752e-05\n",
      "epoch:10 step:7100 train loss: 1.486, r2: 0.621, rate:0.750, acc:0.812 val loss: 1.667, r2: -1.154, rate:0.250, acc:0.250 lr 1.6649481826580752e-05\n",
      "epoch:10 step:7200 train loss: 0.713, r2: 0.124, rate:0.562, acc:0.375 val loss: 0.643, r2: 0.502, rate:0.625, acc:0.750 lr 1.6649481826580752e-05\n",
      "epoch:10 step:7300 train loss: 3.010, r2: -5.051, rate:0.812, acc:0.812 val loss: 0.823, r2: -0.322, rate:0.750, acc:0.750 lr 1.4984533643922677e-05\n",
      "epoch:10 step:7400 train loss: 3.372, r2: -5.615, rate:0.625, acc:0.938 val loss: 1.303, r2: -0.327, rate:0.438, acc:0.688 lr 1.4984533643922677e-05\n",
      "epoch:10 step:7500 train loss: 0.779, r2: -0.118, rate:0.625, acc:0.938 val loss: 0.817, r2: -0.061, rate:0.688, acc:0.375 lr 1.4984533643922677e-05\n",
      "epoch:10 step:7600 train loss: 1.012, r2: -0.415, rate:0.938, acc:1.000 val loss: 0.959, r2: -0.758, rate:0.375, acc:0.625 lr 1.4984533643922677e-05\n",
      "epoch:10 step:7700 train loss: 1.333, r2: -5.273, rate:1.000, acc:0.688 val loss: 0.724, r2: -0.222, rate:0.688, acc:0.750 lr 1.4984533643922677e-05\n",
      "epoch:10 step:7800 train loss: 0.630, r2: 0.024, rate:0.750, acc:0.562 val loss: 0.816, r2: -0.118, rate:0.750, acc:0.500 lr 1.4984533643922677e-05\n",
      "epoch:10 step:0.326 lr 0.000015\n",
      "epoch:11 step:0 train loss: 0.724, r2: -1.200, rate:0.500, acc:0.625 val loss: 0.671, r2: -1.161, rate:0.688, acc:0.938 lr 1.4984533643922677e-05\n",
      "epoch:11 step:100 train loss: 0.748, r2: 0.673, rate:0.812, acc:0.875 val loss: 0.879, r2: -0.138, rate:0.750, acc:0.312 lr 1.4984533643922677e-05\n",
      "epoch:11 step:200 train loss: 2.598, r2: -1.778, rate:0.750, acc:1.000 val loss: 1.626, r2: 0.230, rate:0.812, acc:0.688 lr 1.4984533643922677e-05\n",
      "epoch:11 step:300 train loss: 0.770, r2: 0.187, rate:0.562, acc:0.375 val loss: 1.585, r2: -1.777, rate:0.438, acc:0.312 lr 1.4984533643922677e-05\n",
      "epoch:11 step:400 train loss: 0.810, r2: 0.435, rate:0.875, acc:0.625 val loss: 1.006, r2: 0.002, rate:0.625, acc:0.500 lr 1.4984533643922677e-05\n",
      "epoch:11 step:500 train loss: 1.077, r2: 0.485, rate:0.875, acc:0.625 val loss: 1.583, r2: 0.018, rate:0.625, acc:0.500 lr 1.348608027953041e-05\n",
      "epoch:11 step:600 train loss: 0.673, r2: 0.213, rate:0.688, acc:0.812 val loss: 0.733, r2: 0.265, rate:0.750, acc:0.625 lr 1.348608027953041e-05\n",
      "epoch:11 step:700 train loss: 0.710, r2: -0.047, rate:0.500, acc:0.750 val loss: 0.856, r2: -5.427, rate:0.625, acc:0.562 lr 1.348608027953041e-05\n",
      "epoch:11 step:800 train loss: 0.801, r2: -0.300, rate:0.812, acc:0.875 val loss: 1.054, r2: -0.923, rate:0.438, acc:0.500 lr 1.348608027953041e-05\n",
      "epoch:11 step:900 train loss: 0.768, r2: -1.478, rate:0.438, acc:0.625 val loss: 0.706, r2: 0.166, rate:0.812, acc:0.688 lr 1.348608027953041e-05\n",
      "epoch:11 step:1000 train loss: 0.975, r2: -1.096, rate:0.562, acc:0.375 val loss: 0.752, r2: 0.642, rate:0.812, acc:0.938 lr 1.348608027953041e-05\n",
      "epoch:11 step:1100 train loss: 1.837, r2: -0.440, rate:0.688, acc:0.562 val loss: 1.145, r2: 0.395, rate:0.625, acc:0.562 lr 1.348608027953041e-05\n",
      "epoch:11 step:1200 train loss: 0.763, r2: -0.332, rate:0.750, acc:0.812 val loss: 1.470, r2: 0.322, rate:0.562, acc:0.500 lr 1.348608027953041e-05\n",
      "epoch:11 step:1300 train loss: 1.262, r2: -1.184, rate:0.562, acc:0.938 val loss: 0.958, r2: 0.546, rate:0.688, acc:0.625 lr 1.348608027953041e-05\n",
      "epoch:11 step:1400 train loss: 1.469, r2: -0.827, rate:0.562, acc:0.500 val loss: 1.168, r2: -1.023, rate:0.312, acc:0.688 lr 1.348608027953041e-05\n",
      "epoch:11 step:1500 train loss: 1.100, r2: -0.659, rate:0.688, acc:0.812 val loss: 0.877, r2: -0.410, rate:0.625, acc:0.438 lr 1.348608027953041e-05\n",
      "epoch:11 step:1600 train loss: 0.703, r2: 0.035, rate:0.500, acc:0.750 val loss: 1.227, r2: 0.340, rate:0.875, acc:0.750 lr 1.213747225157737e-05\n",
      "epoch:11 step:1700 train loss: 1.880, r2: 0.635, rate:0.750, acc:0.750 val loss: 0.913, r2: 0.624, rate:0.875, acc:0.750 lr 1.213747225157737e-05\n",
      "epoch:11 step:1800 train loss: 0.828, r2: -0.228, rate:0.688, acc:0.375 val loss: 1.484, r2: -0.250, rate:0.375, acc:0.625 lr 1.213747225157737e-05\n",
      "epoch:11 step:1900 train loss: 0.965, r2: -0.048, rate:0.750, acc:0.500 val loss: 0.839, r2: -1.143, rate:0.500, acc:0.500 lr 1.213747225157737e-05\n",
      "epoch:11 step:2000 train loss: 4.795, r2: -5.083, rate:0.750, acc:0.938 val loss: 1.108, r2: -2.091, rate:0.688, acc:0.375 lr 1.213747225157737e-05\n",
      "epoch:11 step:2100 train loss: 0.706, r2: 0.559, rate:0.688, acc:0.750 val loss: 0.942, r2: -0.029, rate:0.438, acc:0.562 lr 1.213747225157737e-05\n",
      "epoch:11 step:2200 train loss: 1.458, r2: -0.635, rate:0.875, acc:0.875 val loss: 1.177, r2: -5.302, rate:0.250, acc:0.438 lr 1.213747225157737e-05\n",
      "epoch:11 step:2300 train loss: 1.290, r2: -3.583, rate:0.625, acc:0.875 val loss: 0.866, r2: -1.002, rate:0.375, acc:0.938 lr 1.213747225157737e-05\n",
      "epoch:11 step:2400 train loss: 0.977, r2: -0.808, rate:0.625, acc:0.375 val loss: 0.655, r2: 0.487, rate:0.812, acc:0.750 lr 1.213747225157737e-05\n",
      "epoch:11 step:2500 train loss: 2.032, r2: -4.871, rate:0.562, acc:0.375 val loss: 0.770, r2: -5.899, rate:0.938, acc:1.000 lr 1.213747225157737e-05\n",
      "epoch:11 step:2600 train loss: 1.028, r2: -0.212, rate:0.688, acc:0.812 val loss: 1.541, r2: -0.207, rate:0.562, acc:0.312 lr 1.213747225157737e-05\n",
      "epoch:11 step:2700 train loss: 1.498, r2: -0.408, rate:0.688, acc:0.500 val loss: 1.225, r2: -2.133, rate:0.312, acc:0.500 lr 1.0923725026419632e-05\n",
      "epoch:11 step:2800 train loss: 1.099, r2: -1.513, rate:0.688, acc:0.562 val loss: 1.051, r2: -1.899, rate:0.500, acc:0.875 lr 1.0923725026419632e-05\n",
      "epoch:11 step:2900 train loss: 0.586, r2: 0.557, rate:0.875, acc:0.875 val loss: 0.699, r2: 0.311, rate:0.875, acc:0.562 lr 1.0923725026419632e-05\n",
      "epoch:11 step:3000 train loss: 0.424, r2: 0.680, rate:0.938, acc:1.000 val loss: 1.089, r2: 0.198, rate:0.812, acc:0.625 lr 1.0923725026419632e-05\n",
      "epoch:11 step:3100 train loss: 2.293, r2: -19.269, rate:0.688, acc:0.312 val loss: 0.567, r2: 0.129, rate:0.812, acc:0.750 lr 1.0923725026419632e-05\n",
      "epoch:11 step:3200 train loss: 1.157, r2: -0.000, rate:0.938, acc:0.562 val loss: 1.260, r2: -0.001, rate:0.812, acc:0.625 lr 1.0923725026419632e-05\n",
      "epoch:11 step:3300 train loss: 0.589, r2: -0.153, rate:0.750, acc:0.688 val loss: 0.851, r2: -0.203, rate:0.750, acc:0.812 lr 1.0923725026419632e-05\n",
      "epoch:11 step:3400 train loss: 0.699, r2: 0.190, rate:0.688, acc:0.688 val loss: 0.964, r2: -1.060, rate:0.562, acc:0.438 lr 1.0923725026419632e-05\n",
      "epoch:11 step:3500 train loss: 7.886, r2: -1.440, rate:0.625, acc:0.812 val loss: 1.744, r2: 0.002, rate:0.500, acc:0.438 lr 1.0923725026419632e-05\n",
      "epoch:11 step:3600 train loss: 0.557, r2: 0.582, rate:0.875, acc:0.812 val loss: 1.652, r2: 0.539, rate:0.750, acc:0.875 lr 1.0923725026419632e-05\n",
      "epoch:11 step:3700 train loss: 1.065, r2: -1.546, rate:0.625, acc:1.000 val loss: 1.187, r2: 0.364, rate:0.438, acc:0.375 lr 1.0923725026419632e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:3800 train loss: 1.595, r2: 0.322, rate:0.812, acc:0.688 val loss: 1.410, r2: 0.126, rate:0.562, acc:0.375 lr 9.831352523777669e-06\n",
      "epoch:11 step:3900 train loss: 1.324, r2: -3.329, rate:0.875, acc:0.875 val loss: 0.693, r2: 0.407, rate:0.688, acc:0.688 lr 9.831352523777669e-06\n",
      "epoch:11 step:4000 train loss: 1.672, r2: -5.231, rate:0.688, acc:1.000 val loss: 1.147, r2: 0.135, rate:0.938, acc:0.875 lr 9.831352523777669e-06\n",
      "epoch:11 step:4100 train loss: 0.633, r2: 0.787, rate:0.875, acc:0.938 val loss: 2.058, r2: 0.025, rate:0.500, acc:0.312 lr 9.831352523777669e-06\n",
      "epoch:11 step:4200 train loss: 2.185, r2: -1.242, rate:0.812, acc:0.500 val loss: 0.577, r2: 0.565, rate:0.812, acc:0.812 lr 9.831352523777669e-06\n",
      "epoch:11 step:4300 train loss: 0.961, r2: -1.375, rate:0.438, acc:0.688 val loss: 0.666, r2: 0.604, rate:0.688, acc:0.625 lr 9.831352523777669e-06\n",
      "epoch:11 step:4400 train loss: 1.601, r2: 0.087, rate:0.500, acc:0.625 val loss: 0.808, r2: 0.589, rate:0.625, acc:0.688 lr 9.831352523777669e-06\n",
      "epoch:11 step:4500 train loss: 0.758, r2: -2.193, rate:0.562, acc:0.375 val loss: 1.043, r2: -0.303, rate:0.625, acc:0.875 lr 9.831352523777669e-06\n",
      "epoch:11 step:4600 train loss: 0.821, r2: 0.429, rate:0.562, acc:0.625 val loss: 0.970, r2: 0.400, rate:0.812, acc:0.750 lr 9.831352523777669e-06\n",
      "epoch:11 step:4700 train loss: 0.783, r2: 0.577, rate:0.562, acc:0.625 val loss: 1.097, r2: -1.681, rate:0.500, acc:0.500 lr 9.831352523777669e-06\n",
      "epoch:11 step:4800 train loss: 0.946, r2: 0.090, rate:1.000, acc:1.000 val loss: 0.565, r2: 0.467, rate:0.812, acc:0.750 lr 9.831352523777669e-06\n",
      "epoch:11 step:4900 train loss: 0.841, r2: -0.730, rate:0.500, acc:0.812 val loss: 0.783, r2: 0.131, rate:0.312, acc:0.562 lr 8.848217271399903e-06\n",
      "epoch:11 step:5000 train loss: 0.948, r2: 0.242, rate:0.562, acc:0.500 val loss: 1.638, r2: -1.013, rate:0.750, acc:0.812 lr 8.848217271399903e-06\n",
      "epoch:11 step:5100 train loss: 2.164, r2: -0.394, rate:0.750, acc:0.812 val loss: 0.730, r2: -1.990, rate:0.500, acc:0.562 lr 8.848217271399903e-06\n",
      "epoch:11 step:5200 train loss: 0.672, r2: -2.212, rate:0.625, acc:1.000 val loss: 0.798, r2: -0.386, rate:0.625, acc:0.688 lr 8.848217271399903e-06\n",
      "epoch:11 step:5300 train loss: 1.518, r2: 0.194, rate:0.750, acc:0.812 val loss: 0.599, r2: 0.667, rate:0.875, acc:0.625 lr 8.848217271399903e-06\n",
      "epoch:11 step:5400 train loss: 0.877, r2: -0.730, rate:0.562, acc:0.625 val loss: 2.665, r2: -1.227, rate:0.500, acc:0.625 lr 8.848217271399903e-06\n",
      "epoch:11 step:5500 train loss: 1.229, r2: -1.685, rate:0.625, acc:0.625 val loss: 0.797, r2: -0.176, rate:0.750, acc:0.562 lr 8.848217271399903e-06\n",
      "epoch:11 step:5600 train loss: 1.928, r2: -1.024, rate:0.812, acc:0.625 val loss: 1.126, r2: 0.008, rate:0.500, acc:0.500 lr 8.848217271399903e-06\n",
      "epoch:11 step:5700 train loss: 1.242, r2: -0.324, rate:0.562, acc:0.625 val loss: 1.630, r2: 0.125, rate:0.438, acc:0.812 lr 8.848217271399903e-06\n",
      "epoch:11 step:5800 train loss: 1.470, r2: -0.845, rate:0.250, acc:0.188 val loss: 0.717, r2: 0.370, rate:0.688, acc:0.750 lr 8.848217271399903e-06\n",
      "epoch:11 step:5900 train loss: 2.424, r2: -0.315, rate:0.688, acc:0.562 val loss: 2.037, r2: -4.800, rate:0.312, acc:0.562 lr 8.848217271399903e-06\n",
      "epoch:11 step:6000 train loss: 1.882, r2: -10.382, rate:0.875, acc:1.000 val loss: 2.025, r2: 0.229, rate:0.688, acc:0.750 lr 7.963395544259913e-06\n",
      "epoch:11 step:6100 train loss: 0.680, r2: -0.769, rate:0.688, acc:0.438 val loss: 0.653, r2: -1.746, rate:0.938, acc:1.000 lr 7.963395544259913e-06\n",
      "epoch:11 step:6200 train loss: 0.940, r2: 0.175, rate:0.750, acc:0.750 val loss: 0.817, r2: 0.467, rate:0.750, acc:0.500 lr 7.963395544259913e-06\n",
      "epoch:11 step:6300 train loss: 0.548, r2: -1.698, rate:0.875, acc:0.688 val loss: 1.084, r2: -10.382, rate:0.562, acc:0.500 lr 7.963395544259913e-06\n",
      "epoch:11 step:6400 train loss: 1.064, r2: -0.720, rate:0.500, acc:0.812 val loss: 0.768, r2: 0.515, rate:0.688, acc:0.562 lr 7.963395544259913e-06\n",
      "epoch:11 step:6500 train loss: 0.767, r2: -1.409, rate:0.562, acc:0.250 val loss: 0.778, r2: 0.355, rate:0.750, acc:0.688 lr 7.963395544259913e-06\n",
      "epoch:11 step:6600 train loss: 1.076, r2: -0.644, rate:0.562, acc:0.750 val loss: 0.805, r2: -0.282, rate:0.562, acc:0.250 lr 7.963395544259913e-06\n",
      "epoch:11 step:6700 train loss: 0.724, r2: 0.436, rate:0.750, acc:0.812 val loss: 0.598, r2: 0.127, rate:0.750, acc:0.625 lr 7.963395544259913e-06\n",
      "epoch:11 step:6800 train loss: 0.711, r2: 0.372, rate:0.688, acc:0.312 val loss: 1.169, r2: -0.207, rate:0.688, acc:0.688 lr 7.963395544259913e-06\n",
      "epoch:11 step:6900 train loss: 0.723, r2: -0.534, rate:0.812, acc:1.000 val loss: 0.661, r2: 0.539, rate:0.688, acc:0.688 lr 7.963395544259913e-06\n",
      "epoch:11 step:7000 train loss: 1.316, r2: -0.597, rate:0.500, acc:0.562 val loss: 0.572, r2: 0.664, rate:0.812, acc:0.750 lr 7.963395544259913e-06\n",
      "epoch:11 step:7100 train loss: 1.154, r2: -1.188, rate:0.688, acc:0.750 val loss: 0.915, r2: 0.513, rate:0.625, acc:0.562 lr 7.167055989833922e-06\n",
      "epoch:11 step:7200 train loss: 0.472, r2: 0.860, rate:0.938, acc:0.875 val loss: 0.874, r2: 0.122, rate:0.562, acc:0.875 lr 7.167055989833922e-06\n",
      "epoch:11 step:7300 train loss: 0.768, r2: 0.671, rate:0.625, acc:0.688 val loss: 0.879, r2: 0.236, rate:0.688, acc:0.562 lr 7.167055989833922e-06\n",
      "epoch:11 step:7400 train loss: 1.102, r2: 0.396, rate:0.750, acc:0.812 val loss: 1.710, r2: 0.003, rate:0.625, acc:0.500 lr 7.167055989833922e-06\n",
      "epoch:11 step:7500 train loss: 0.869, r2: 0.439, rate:0.750, acc:0.688 val loss: 1.114, r2: -0.661, rate:0.438, acc:0.750 lr 7.167055989833922e-06\n",
      "epoch:11 step:7600 train loss: 0.544, r2: 0.508, rate:0.875, acc:0.812 val loss: 0.815, r2: -3.623, rate:0.375, acc:0.750 lr 7.167055989833922e-06\n",
      "epoch:11 step:7700 train loss: 0.984, r2: -0.245, rate:0.500, acc:0.500 val loss: 1.869, r2: -0.942, rate:0.688, acc:0.562 lr 7.167055989833922e-06\n",
      "epoch:11 step:7800 train loss: 0.980, r2: -1.533, rate:0.438, acc:0.375 val loss: 0.743, r2: -0.195, rate:0.750, acc:0.875 lr 7.167055989833922e-06\n",
      "epoch:11 step:0.326 lr 0.000007\n",
      "epoch:12 step:0 train loss: 0.720, r2: -1.184, rate:0.875, acc:0.938 val loss: 0.786, r2: -0.105, rate:0.688, acc:0.875 lr 7.167055989833922e-06\n",
      "epoch:12 step:100 train loss: 0.757, r2: -3.544, rate:0.625, acc:0.688 val loss: 1.025, r2: -0.627, rate:0.438, acc:0.500 lr 7.167055989833922e-06\n",
      "epoch:12 step:200 train loss: 1.985, r2: -0.976, rate:0.438, acc:0.688 val loss: 0.577, r2: -0.021, rate:0.688, acc:0.562 lr 7.167055989833922e-06\n",
      "epoch:12 step:300 train loss: 1.164, r2: -0.544, rate:0.625, acc:0.438 val loss: 1.811, r2: -0.347, rate:0.688, acc:0.562 lr 6.4503503908505294e-06\n",
      "epoch:12 step:400 train loss: 1.149, r2: 0.118, rate:0.625, acc:0.750 val loss: 0.853, r2: -0.498, rate:0.562, acc:0.812 lr 6.4503503908505294e-06\n",
      "epoch:12 step:500 train loss: 0.868, r2: 0.239, rate:0.750, acc:0.688 val loss: 2.310, r2: -0.946, rate:0.938, acc:0.812 lr 6.4503503908505294e-06\n",
      "epoch:12 step:600 train loss: 3.491, r2: -3.793, rate:0.562, acc:0.250 val loss: 0.689, r2: 0.487, rate:0.812, acc:0.750 lr 6.4503503908505294e-06\n",
      "epoch:12 step:700 train loss: 0.584, r2: 0.846, rate:1.000, acc:0.812 val loss: 0.969, r2: 0.708, rate:0.875, acc:0.875 lr 6.4503503908505294e-06\n",
      "epoch:12 step:800 train loss: 1.136, r2: 0.557, rate:0.750, acc:0.750 val loss: 1.049, r2: -0.507, rate:0.812, acc:1.000 lr 6.4503503908505294e-06\n",
      "epoch:12 step:900 train loss: 0.826, r2: -0.123, rate:0.688, acc:0.625 val loss: 1.407, r2: 0.183, rate:0.625, acc:0.625 lr 6.4503503908505294e-06\n",
      "epoch:12 step:1000 train loss: 0.766, r2: 0.329, rate:0.562, acc:0.750 val loss: 0.574, r2: 0.763, rate:0.625, acc:0.562 lr 6.4503503908505294e-06\n",
      "epoch:12 step:1100 train loss: 1.184, r2: -1.479, rate:0.500, acc:0.500 val loss: 0.859, r2: 0.282, rate:0.562, acc:0.750 lr 6.4503503908505294e-06\n",
      "epoch:12 step:1200 train loss: 0.877, r2: 0.195, rate:0.562, acc:0.688 val loss: 0.714, r2: 0.504, rate:0.812, acc:0.938 lr 6.4503503908505294e-06\n",
      "epoch:12 step:1300 train loss: 0.889, r2: -0.096, rate:0.688, acc:0.812 val loss: 0.635, r2: 0.013, rate:0.688, acc:0.438 lr 6.4503503908505294e-06\n",
      "epoch:12 step:1400 train loss: 0.379, r2: 0.763, rate:1.000, acc:0.875 val loss: 0.713, r2: -1.456, rate:0.625, acc:0.562 lr 5.805315351765477e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:1500 train loss: 1.720, r2: -1.658, rate:0.875, acc:1.000 val loss: 1.079, r2: -0.464, rate:0.688, acc:0.812 lr 5.805315351765477e-06\n",
      "epoch:12 step:1600 train loss: 0.608, r2: 0.283, rate:0.812, acc:0.625 val loss: 1.611, r2: 0.289, rate:0.562, acc:0.688 lr 5.805315351765477e-06\n",
      "epoch:12 step:1700 train loss: 4.478, r2: -1.747, rate:0.562, acc:0.750 val loss: 0.812, r2: 0.131, rate:0.500, acc:0.438 lr 5.805315351765477e-06\n",
      "epoch:12 step:1800 train loss: 0.770, r2: -0.449, rate:0.812, acc:0.875 val loss: 0.778, r2: 0.488, rate:0.750, acc:0.625 lr 5.805315351765477e-06\n",
      "epoch:12 step:1900 train loss: 0.978, r2: -0.449, rate:0.562, acc:0.312 val loss: 1.034, r2: -0.289, rate:0.812, acc:0.812 lr 5.805315351765477e-06\n",
      "epoch:12 step:2000 train loss: 4.243, r2: -0.700, rate:0.750, acc:0.562 val loss: 1.406, r2: 0.134, rate:0.625, acc:0.688 lr 5.805315351765477e-06\n",
      "epoch:12 step:2100 train loss: 0.624, r2: 0.146, rate:0.875, acc:0.875 val loss: 0.935, r2: 0.089, rate:0.688, acc:0.562 lr 5.805315351765477e-06\n",
      "epoch:12 step:2200 train loss: 0.773, r2: 0.131, rate:0.438, acc:0.562 val loss: 1.012, r2: 0.259, rate:0.688, acc:0.625 lr 5.805315351765477e-06\n",
      "epoch:12 step:2300 train loss: 1.732, r2: -1.685, rate:0.438, acc:0.188 val loss: 0.904, r2: 0.288, rate:0.500, acc:0.250 lr 5.805315351765477e-06\n",
      "epoch:12 step:2400 train loss: 0.852, r2: -0.141, rate:0.625, acc:0.875 val loss: 1.428, r2: 0.053, rate:0.625, acc:0.500 lr 5.805315351765477e-06\n",
      "epoch:12 step:2500 train loss: 0.674, r2: 0.484, rate:0.562, acc:0.688 val loss: 0.588, r2: 0.137, rate:0.750, acc:0.625 lr 5.224783816588929e-06\n",
      "epoch:12 step:2600 train loss: 1.304, r2: -0.454, rate:0.750, acc:0.562 val loss: 0.672, r2: 0.222, rate:0.688, acc:0.812 lr 5.224783816588929e-06\n",
      "epoch:12 step:2700 train loss: 0.825, r2: -0.506, rate:0.688, acc:0.500 val loss: 1.314, r2: -0.094, rate:0.625, acc:0.562 lr 5.224783816588929e-06\n",
      "epoch:12 step:2800 train loss: 0.755, r2: -0.086, rate:0.812, acc:0.625 val loss: 1.239, r2: -0.125, rate:0.625, acc:0.625 lr 5.224783816588929e-06\n",
      "epoch:12 step:2900 train loss: 1.911, r2: -0.798, rate:0.625, acc:0.750 val loss: 0.857, r2: -0.398, rate:0.438, acc:0.625 lr 5.224783816588929e-06\n",
      "epoch:12 step:3000 train loss: 0.511, r2: -2.124, rate:0.875, acc:1.000 val loss: 1.350, r2: -0.280, rate:0.625, acc:0.688 lr 5.224783816588929e-06\n",
      "epoch:12 step:3100 train loss: 1.457, r2: 0.324, rate:0.750, acc:0.812 val loss: 0.673, r2: 0.417, rate:0.750, acc:0.875 lr 5.224783816588929e-06\n",
      "epoch:12 step:3200 train loss: 1.373, r2: -0.135, rate:0.750, acc:0.625 val loss: 0.552, r2: 0.465, rate:0.938, acc:0.625 lr 5.224783816588929e-06\n",
      "epoch:12 step:3300 train loss: 3.307, r2: -0.895, rate:0.500, acc:0.500 val loss: 1.417, r2: -3.175, rate:0.688, acc:0.750 lr 5.224783816588929e-06\n",
      "epoch:12 step:3400 train loss: 0.932, r2: -0.245, rate:0.562, acc:0.312 val loss: 0.898, r2: 0.064, rate:0.688, acc:0.812 lr 5.224783816588929e-06\n",
      "epoch:12 step:3500 train loss: 0.771, r2: 0.364, rate:0.625, acc:0.562 val loss: 0.904, r2: 0.264, rate:0.875, acc:0.875 lr 5.224783816588929e-06\n",
      "epoch:12 step:3600 train loss: 0.632, r2: -0.592, rate:0.875, acc:0.875 val loss: 1.181, r2: 0.533, rate:0.812, acc:0.812 lr 4.702305434930037e-06\n",
      "epoch:12 step:3700 train loss: 0.890, r2: 0.160, rate:0.812, acc:0.875 val loss: 1.054, r2: -0.495, rate:0.562, acc:0.688 lr 4.702305434930037e-06\n",
      "epoch:12 step:3800 train loss: 3.578, r2: -0.266, rate:0.562, acc:0.750 val loss: 1.117, r2: -0.729, rate:0.688, acc:0.938 lr 4.702305434930037e-06\n",
      "epoch:12 step:3900 train loss: 1.156, r2: 0.129, rate:0.875, acc:0.875 val loss: 0.861, r2: -0.289, rate:0.500, acc:0.688 lr 4.702305434930037e-06\n",
      "epoch:12 step:4000 train loss: 8.631, r2: -5.445, rate:0.812, acc:0.938 val loss: 1.294, r2: 0.252, rate:0.688, acc:0.750 lr 4.702305434930037e-06\n",
      "epoch:12 step:4100 train loss: 3.465, r2: -2.879, rate:0.750, acc:0.500 val loss: 2.395, r2: -0.328, rate:0.688, acc:0.625 lr 4.702305434930037e-06\n",
      "epoch:12 step:4200 train loss: 1.130, r2: -2.502, rate:0.688, acc:0.562 val loss: 0.677, r2: -1.239, rate:0.688, acc:0.875 lr 4.702305434930037e-06\n",
      "epoch:12 step:4300 train loss: 1.092, r2: -4.937, rate:0.812, acc:0.312 val loss: 0.599, r2: 0.613, rate:0.750, acc:0.625 lr 4.702305434930037e-06\n",
      "epoch:12 step:4400 train loss: 1.379, r2: -0.487, rate:0.562, acc:0.625 val loss: 0.552, r2: 0.317, rate:0.750, acc:0.750 lr 4.702305434930037e-06\n",
      "epoch:12 step:4500 train loss: 1.709, r2: -4.375, rate:0.312, acc:0.750 val loss: 0.852, r2: 0.261, rate:0.625, acc:0.500 lr 4.702305434930037e-06\n",
      "epoch:12 step:4600 train loss: 1.912, r2: -2.268, rate:0.562, acc:0.750 val loss: 1.227, r2: -0.393, rate:0.500, acc:0.500 lr 4.702305434930037e-06\n",
      "epoch:12 step:4700 train loss: 20.350, r2: -0.649, rate:0.500, acc:0.500 val loss: 1.069, r2: -0.037, rate:0.750, acc:0.750 lr 4.232074891437034e-06\n",
      "epoch:12 step:4800 train loss: 1.174, r2: -0.183, rate:0.750, acc:0.625 val loss: 0.759, r2: 0.583, rate:0.750, acc:0.500 lr 4.232074891437034e-06\n",
      "epoch:12 step:4900 train loss: 1.272, r2: -2.539, rate:0.750, acc:0.938 val loss: 0.793, r2: 0.568, rate:0.688, acc:0.750 lr 4.232074891437034e-06\n",
      "epoch:12 step:5000 train loss: 1.384, r2: -3.599, rate:0.562, acc:0.125 val loss: 0.639, r2: -0.039, rate:0.875, acc:0.750 lr 4.232074891437034e-06\n",
      "epoch:12 step:5100 train loss: 2.158, r2: -1.373, rate:0.625, acc:0.375 val loss: 0.695, r2: 0.192, rate:0.875, acc:1.000 lr 4.232074891437034e-06\n",
      "epoch:12 step:5200 train loss: 0.869, r2: 0.339, rate:0.625, acc:0.625 val loss: 1.056, r2: 0.245, rate:0.812, acc:0.750 lr 4.232074891437034e-06\n",
      "epoch:12 step:5300 train loss: 0.870, r2: -0.066, rate:0.625, acc:0.562 val loss: 1.799, r2: 0.247, rate:0.688, acc:0.625 lr 4.232074891437034e-06\n",
      "epoch:12 step:5400 train loss: 0.577, r2: 0.541, rate:0.812, acc:1.000 val loss: 0.644, r2: 0.635, rate:0.875, acc:0.812 lr 4.232074891437034e-06\n",
      "epoch:12 step:5500 train loss: 1.499, r2: -0.633, rate:0.625, acc:0.688 val loss: 0.578, r2: 0.793, rate:0.812, acc:0.625 lr 4.232074891437034e-06\n",
      "epoch:12 step:5600 train loss: 1.494, r2: -1.120, rate:0.500, acc:0.562 val loss: 1.128, r2: -0.939, rate:0.688, acc:0.812 lr 4.232074891437034e-06\n",
      "epoch:12 step:5700 train loss: 0.625, r2: 0.489, rate:0.750, acc:0.938 val loss: 0.738, r2: 0.277, rate:0.875, acc:0.812 lr 4.232074891437034e-06\n",
      "epoch:12 step:5800 train loss: 2.525, r2: -4.085, rate:0.500, acc:0.375 val loss: 0.878, r2: -1.122, rate:0.625, acc:0.500 lr 3.8088674022933302e-06\n",
      "epoch:12 step:5900 train loss: 0.824, r2: -1.049, rate:0.812, acc:1.000 val loss: 1.013, r2: -0.146, rate:0.625, acc:0.688 lr 3.8088674022933302e-06\n",
      "epoch:12 step:6000 train loss: 0.743, r2: 0.175, rate:0.625, acc:0.438 val loss: 0.837, r2: 0.523, rate:0.812, acc:0.750 lr 3.8088674022933302e-06\n",
      "epoch:12 step:6100 train loss: 0.740, r2: -0.285, rate:0.500, acc:0.188 val loss: 1.251, r2: 0.128, rate:0.562, acc:0.625 lr 3.8088674022933302e-06\n",
      "epoch:12 step:6200 train loss: 0.739, r2: -0.514, rate:0.500, acc:0.625 val loss: 1.030, r2: 0.176, rate:0.625, acc:0.688 lr 3.8088674022933302e-06\n",
      "epoch:12 step:6300 train loss: 0.870, r2: 0.713, rate:0.688, acc:0.812 val loss: 1.082, r2: 0.292, rate:0.812, acc:0.875 lr 3.8088674022933302e-06\n",
      "epoch:12 step:6400 train loss: 1.530, r2: -0.197, rate:0.625, acc:0.625 val loss: 0.773, r2: 0.576, rate:0.812, acc:0.625 lr 3.8088674022933302e-06\n",
      "epoch:12 step:6500 train loss: 0.848, r2: -1.158, rate:0.688, acc:0.625 val loss: 1.341, r2: -0.656, rate:0.562, acc:0.438 lr 3.8088674022933302e-06\n",
      "epoch:12 step:6600 train loss: 0.586, r2: 0.609, rate:0.750, acc:0.812 val loss: 0.787, r2: -1.014, rate:0.812, acc:0.312 lr 3.8088674022933302e-06\n",
      "epoch:12 step:6700 train loss: 1.250, r2: -0.372, rate:0.875, acc:0.750 val loss: 2.168, r2: 0.033, rate:0.625, acc:0.500 lr 3.8088674022933302e-06\n",
      "epoch:12 step:6800 train loss: 0.717, r2: 0.397, rate:0.750, acc:0.750 val loss: 0.565, r2: -0.446, rate:0.812, acc:0.938 lr 3.8088674022933302e-06\n",
      "epoch:12 step:6900 train loss: 1.812, r2: -1.572, rate:0.625, acc:0.812 val loss: 0.914, r2: 0.616, rate:0.812, acc:0.562 lr 3.427980662063997e-06\n",
      "epoch:12 step:7000 train loss: 0.665, r2: 0.539, rate:0.688, acc:0.812 val loss: 0.832, r2: 0.270, rate:0.500, acc:0.562 lr 3.427980662063997e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:7100 train loss: 1.336, r2: -0.359, rate:0.625, acc:0.562 val loss: 0.766, r2: 0.526, rate:0.750, acc:0.875 lr 3.427980662063997e-06\n",
      "epoch:12 step:7200 train loss: 0.669, r2: 0.295, rate:0.688, acc:0.625 val loss: 0.754, r2: 0.491, rate:0.625, acc:0.562 lr 3.427980662063997e-06\n",
      "epoch:12 step:7300 train loss: 6.484, r2: -0.790, rate:0.312, acc:0.250 val loss: 0.909, r2: -0.427, rate:0.875, acc:0.625 lr 3.427980662063997e-06\n",
      "epoch:12 step:7400 train loss: 3.981, r2: 0.262, rate:0.688, acc:0.500 val loss: 0.622, r2: 0.021, rate:1.000, acc:0.750 lr 3.427980662063997e-06\n",
      "epoch:12 step:7500 train loss: 1.669, r2: -4.160, rate:0.750, acc:0.938 val loss: 1.042, r2: -1.502, rate:0.562, acc:0.375 lr 3.427980662063997e-06\n",
      "epoch:12 step:7600 train loss: 0.668, r2: -1.094, rate:0.625, acc:0.750 val loss: 0.903, r2: -0.918, rate:0.625, acc:0.562 lr 3.427980662063997e-06\n",
      "epoch:12 step:7700 train loss: 1.523, r2: 0.359, rate:0.688, acc:0.750 val loss: 0.843, r2: -1.586, rate:0.562, acc:0.750 lr 3.427980662063997e-06\n",
      "epoch:12 step:7800 train loss: 1.227, r2: -0.185, rate:0.688, acc:0.375 val loss: 1.187, r2: -0.895, rate:0.625, acc:0.438 lr 3.427980662063997e-06\n",
      "epoch:12 step:0.323 lr 0.000003\n",
      "epoch:13 step:0 train loss: 0.996, r2: -6.056, rate:0.625, acc:0.312 val loss: 0.813, r2: 0.486, rate:0.812, acc:0.688 lr 3.427980662063997e-06\n",
      "epoch:13 step:100 train loss: 3.960, r2: -2.613, rate:0.375, acc:0.438 val loss: 0.722, r2: 0.370, rate:0.750, acc:0.562 lr 3.0851825958575973e-06\n",
      "epoch:13 step:200 train loss: 2.165, r2: 0.396, rate:0.875, acc:0.688 val loss: 1.245, r2: -2.105, rate:0.625, acc:0.688 lr 3.0851825958575973e-06\n",
      "epoch:13 step:300 train loss: 0.889, r2: 0.510, rate:0.812, acc:0.875 val loss: 0.814, r2: 0.707, rate:0.750, acc:0.688 lr 3.0851825958575973e-06\n",
      "epoch:13 step:400 train loss: 0.985, r2: -2.825, rate:0.438, acc:0.188 val loss: 1.471, r2: 0.020, rate:0.562, acc:0.438 lr 3.0851825958575973e-06\n",
      "epoch:13 step:500 train loss: 1.562, r2: -2.171, rate:0.750, acc:1.000 val loss: 0.970, r2: -0.236, rate:0.688, acc:0.562 lr 3.0851825958575973e-06\n",
      "epoch:13 step:600 train loss: 0.869, r2: -0.433, rate:0.750, acc:0.438 val loss: 0.941, r2: -0.959, rate:0.500, acc:0.625 lr 3.0851825958575973e-06\n",
      "epoch:13 step:700 train loss: 0.846, r2: 0.254, rate:0.438, acc:0.812 val loss: 0.671, r2: 0.438, rate:0.812, acc:0.812 lr 3.0851825958575973e-06\n",
      "epoch:13 step:800 train loss: 0.724, r2: 0.608, rate:0.812, acc:0.500 val loss: 0.982, r2: -0.095, rate:0.562, acc:0.562 lr 3.0851825958575973e-06\n",
      "epoch:13 step:900 train loss: 0.651, r2: -9.205, rate:0.562, acc:0.812 val loss: 0.814, r2: 0.289, rate:0.812, acc:0.500 lr 3.0851825958575973e-06\n",
      "epoch:13 step:1000 train loss: 1.158, r2: 0.043, rate:0.562, acc:0.375 val loss: 0.785, r2: 0.018, rate:0.812, acc:0.812 lr 3.0851825958575973e-06\n",
      "epoch:13 step:1100 train loss: 1.389, r2: 0.444, rate:0.500, acc:0.500 val loss: 1.195, r2: 0.342, rate:0.750, acc:0.812 lr 3.0851825958575973e-06\n",
      "epoch:13 step:1200 train loss: 1.180, r2: 0.328, rate:0.812, acc:0.625 val loss: 1.159, r2: -0.399, rate:0.562, acc:0.812 lr 2.7766643362718377e-06\n",
      "epoch:13 step:1300 train loss: 1.564, r2: -0.339, rate:0.750, acc:0.875 val loss: 0.764, r2: 0.233, rate:0.688, acc:0.625 lr 2.7766643362718377e-06\n",
      "epoch:13 step:1400 train loss: 0.787, r2: -0.231, rate:0.625, acc:0.688 val loss: 1.008, r2: -3.680, rate:0.500, acc:0.938 lr 2.7766643362718377e-06\n",
      "epoch:13 step:1500 train loss: 1.573, r2: -0.017, rate:0.688, acc:0.625 val loss: 0.746, r2: 0.035, rate:0.562, acc:0.750 lr 2.7766643362718377e-06\n",
      "epoch:13 step:1600 train loss: 0.793, r2: -1.046, rate:0.562, acc:0.500 val loss: 0.706, r2: 0.156, rate:0.625, acc:0.562 lr 2.7766643362718377e-06\n",
      "epoch:13 step:1700 train loss: 7.122, r2: -4.612, rate:0.875, acc:0.688 val loss: 1.233, r2: -0.638, rate:0.500, acc:0.438 lr 2.7766643362718377e-06\n",
      "epoch:13 step:1800 train loss: 0.897, r2: -0.268, rate:0.625, acc:0.438 val loss: 0.906, r2: -2.415, rate:0.562, acc:0.562 lr 2.7766643362718377e-06\n",
      "epoch:13 step:1900 train loss: 1.783, r2: -1.608, rate:0.500, acc:0.500 val loss: 0.658, r2: 0.135, rate:0.875, acc:0.750 lr 2.7766643362718377e-06\n",
      "epoch:13 step:2000 train loss: 1.648, r2: -0.333, rate:0.500, acc:0.438 val loss: 1.105, r2: 0.134, rate:0.688, acc:0.625 lr 2.7766643362718377e-06\n",
      "epoch:13 step:2100 train loss: 0.794, r2: -0.877, rate:0.562, acc:0.812 val loss: 1.508, r2: -0.783, rate:0.500, acc:0.562 lr 2.7766643362718377e-06\n",
      "epoch:13 step:2200 train loss: 0.597, r2: 0.536, rate:0.812, acc:0.875 val loss: 0.593, r2: 0.551, rate:0.875, acc:0.875 lr 2.7766643362718377e-06\n",
      "epoch:13 step:2300 train loss: 2.515, r2: -3.901, rate:0.750, acc:0.500 val loss: 1.256, r2: -0.215, rate:0.625, acc:0.688 lr 2.498997902644654e-06\n",
      "epoch:13 step:2400 train loss: 0.786, r2: 0.227, rate:0.500, acc:0.625 val loss: 1.322, r2: -0.687, rate:0.438, acc:0.312 lr 2.498997902644654e-06\n",
      "epoch:13 step:2500 train loss: 1.398, r2: -2.910, rate:0.500, acc:0.500 val loss: 2.610, r2: -7.519, rate:0.875, acc:0.625 lr 2.498997902644654e-06\n",
      "epoch:13 step:2600 train loss: 0.955, r2: 0.412, rate:0.688, acc:0.625 val loss: 0.638, r2: 0.480, rate:0.812, acc:0.688 lr 2.498997902644654e-06\n",
      "epoch:13 step:2700 train loss: 0.950, r2: -1.562, rate:0.562, acc:0.375 val loss: 0.886, r2: -0.177, rate:0.750, acc:0.812 lr 2.498997902644654e-06\n",
      "epoch:13 step:2800 train loss: 1.609, r2: -0.819, rate:0.688, acc:0.625 val loss: 1.300, r2: -0.282, rate:0.500, acc:0.688 lr 2.498997902644654e-06\n",
      "epoch:13 step:2900 train loss: 1.731, r2: -2.434, rate:0.875, acc:0.938 val loss: 0.577, r2: 0.403, rate:0.812, acc:1.000 lr 2.498997902644654e-06\n",
      "epoch:13 step:3000 train loss: 0.784, r2: -6.911, rate:0.562, acc:0.750 val loss: 0.920, r2: -0.194, rate:0.500, acc:0.500 lr 2.498997902644654e-06\n",
      "epoch:13 step:3100 train loss: 0.857, r2: 0.229, rate:0.750, acc:0.562 val loss: 0.834, r2: 0.461, rate:0.812, acc:0.750 lr 2.498997902644654e-06\n",
      "epoch:13 step:3200 train loss: 0.541, r2: 0.644, rate:0.812, acc:0.812 val loss: 0.735, r2: 0.463, rate:0.688, acc:0.438 lr 2.498997902644654e-06\n",
      "epoch:13 step:3300 train loss: 3.963, r2: -13.915, rate:0.625, acc:0.312 val loss: 0.528, r2: 0.382, rate:0.875, acc:0.812 lr 2.498997902644654e-06\n",
      "epoch:13 step:3400 train loss: 0.620, r2: 0.692, rate:0.812, acc:0.812 val loss: 0.863, r2: 0.472, rate:0.688, acc:0.562 lr 2.2490981123801883e-06\n",
      "epoch:13 step:3500 train loss: 1.352, r2: -2.130, rate:0.625, acc:0.250 val loss: 0.746, r2: -0.054, rate:0.438, acc:0.750 lr 2.2490981123801883e-06\n",
      "epoch:13 step:3600 train loss: 27.079, r2: -3.880, rate:0.750, acc:0.750 val loss: 1.800, r2: -2.898, rate:0.938, acc:0.938 lr 2.2490981123801883e-06\n",
      "epoch:13 step:3700 train loss: 0.971, r2: 0.436, rate:0.562, acc:0.688 val loss: 0.936, r2: -1.551, rate:0.625, acc:0.875 lr 2.2490981123801883e-06\n",
      "epoch:13 step:3800 train loss: 1.878, r2: -0.126, rate:0.500, acc:0.500 val loss: 1.020, r2: -0.645, rate:0.875, acc:0.688 lr 2.2490981123801883e-06\n",
      "epoch:13 step:3900 train loss: 0.583, r2: 0.045, rate:0.875, acc:1.000 val loss: 0.922, r2: 0.517, rate:0.750, acc:0.750 lr 2.2490981123801883e-06\n",
      "epoch:13 step:4000 train loss: 0.839, r2: -0.332, rate:0.688, acc:0.750 val loss: 0.985, r2: 0.369, rate:0.812, acc:0.562 lr 2.2490981123801883e-06\n",
      "epoch:13 step:4100 train loss: 1.173, r2: -0.721, rate:0.750, acc:0.875 val loss: 0.662, r2: 0.278, rate:0.875, acc:0.688 lr 2.2490981123801883e-06\n",
      "epoch:13 step:4200 train loss: 0.810, r2: -1.336, rate:0.688, acc:0.938 val loss: 0.806, r2: 0.438, rate:0.875, acc:0.750 lr 2.2490981123801883e-06\n",
      "epoch:13 step:4300 train loss: 0.750, r2: -1.663, rate:0.625, acc:0.125 val loss: 2.047, r2: 0.396, rate:0.938, acc:0.875 lr 2.2490981123801883e-06\n",
      "epoch:13 step:4400 train loss: 1.236, r2: -0.161, rate:0.562, acc:0.875 val loss: 0.669, r2: 0.658, rate:0.812, acc:0.750 lr 2.2490981123801883e-06\n",
      "epoch:13 step:4500 train loss: 1.563, r2: 0.082, rate:0.375, acc:0.625 val loss: 1.088, r2: -0.161, rate:0.688, acc:0.500 lr 2.0241883011421697e-06\n",
      "epoch:13 step:4600 train loss: 0.800, r2: -0.462, rate:0.688, acc:0.812 val loss: 2.725, r2: -1.585, rate:0.625, acc:0.875 lr 2.0241883011421697e-06\n",
      "epoch:13 step:4700 train loss: 1.698, r2: -0.651, rate:0.688, acc:0.375 val loss: 0.727, r2: 0.573, rate:0.750, acc:0.750 lr 2.0241883011421697e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:4800 train loss: 0.880, r2: -2.627, rate:0.625, acc:0.625 val loss: 0.691, r2: 0.650, rate:0.562, acc:0.625 lr 2.0241883011421697e-06\n",
      "epoch:13 step:4900 train loss: 2.278, r2: -2.590, rate:0.625, acc:0.562 val loss: 1.701, r2: 0.179, rate:0.438, acc:0.688 lr 2.0241883011421697e-06\n",
      "epoch:13 step:5000 train loss: 0.905, r2: -0.099, rate:0.688, acc:0.500 val loss: 0.840, r2: 0.711, rate:0.812, acc:0.812 lr 2.0241883011421697e-06\n",
      "epoch:13 step:5100 train loss: 0.857, r2: -0.737, rate:0.500, acc:0.250 val loss: 0.929, r2: 0.179, rate:0.875, acc:0.750 lr 2.0241883011421697e-06\n",
      "epoch:13 step:5200 train loss: 0.748, r2: -2.040, rate:0.375, acc:0.562 val loss: 1.149, r2: -0.726, rate:0.438, acc:0.625 lr 2.0241883011421697e-06\n",
      "epoch:13 step:5300 train loss: 0.975, r2: -0.675, rate:0.812, acc:0.938 val loss: 1.634, r2: -1.892, rate:0.438, acc:0.312 lr 2.0241883011421697e-06\n",
      "epoch:13 step:5400 train loss: 0.859, r2: -0.552, rate:0.625, acc:0.750 val loss: 1.645, r2: -1.090, rate:0.375, acc:0.375 lr 2.0241883011421697e-06\n",
      "epoch:13 step:5500 train loss: 1.097, r2: -1.151, rate:0.625, acc:1.000 val loss: 1.317, r2: -1.299, rate:0.688, acc:1.000 lr 2.0241883011421697e-06\n",
      "epoch:13 step:5600 train loss: 1.750, r2: -2.361, rate:0.625, acc:0.938 val loss: 0.855, r2: -0.688, rate:0.500, acc:0.438 lr 1.8217694710279527e-06\n",
      "epoch:13 step:5700 train loss: 0.628, r2: -1.871, rate:0.750, acc:0.875 val loss: 1.910, r2: -3.663, rate:0.625, acc:0.938 lr 1.8217694710279527e-06\n",
      "epoch:13 step:5800 train loss: 1.012, r2: -0.005, rate:0.812, acc:0.688 val loss: 1.455, r2: -2.007, rate:0.625, acc:0.688 lr 1.8217694710279527e-06\n",
      "epoch:13 step:5900 train loss: 0.911, r2: -0.380, rate:0.688, acc:0.500 val loss: 0.968, r2: 0.137, rate:0.562, acc:0.562 lr 1.8217694710279527e-06\n",
      "epoch:13 step:6000 train loss: 1.255, r2: -0.275, rate:0.500, acc:0.625 val loss: 0.984, r2: -2.060, rate:0.500, acc:0.688 lr 1.8217694710279527e-06\n",
      "epoch:13 step:6100 train loss: 1.025, r2: -0.098, rate:0.562, acc:0.375 val loss: 1.256, r2: -0.756, rate:0.562, acc:0.500 lr 1.8217694710279527e-06\n",
      "epoch:13 step:6200 train loss: 1.056, r2: -0.210, rate:0.688, acc:0.500 val loss: 0.862, r2: -0.112, rate:0.562, acc:0.438 lr 1.8217694710279527e-06\n",
      "epoch:13 step:6300 train loss: 0.807, r2: 0.203, rate:0.562, acc:0.500 val loss: 0.838, r2: -1.192, rate:0.625, acc:0.562 lr 1.8217694710279527e-06\n",
      "epoch:13 step:6400 train loss: 0.571, r2: 0.035, rate:0.875, acc:0.812 val loss: 1.091, r2: -1.882, rate:0.688, acc:0.688 lr 1.8217694710279527e-06\n",
      "epoch:13 step:6500 train loss: 0.786, r2: -0.099, rate:0.812, acc:0.625 val loss: 1.032, r2: -0.234, rate:0.688, acc:0.500 lr 1.8217694710279527e-06\n",
      "epoch:13 step:6600 train loss: 1.253, r2: -1.095, rate:0.750, acc:0.875 val loss: 0.781, r2: 0.408, rate:0.812, acc:0.750 lr 1.8217694710279527e-06\n",
      "epoch:13 step:6700 train loss: 1.342, r2: 0.511, rate:0.875, acc:0.938 val loss: 0.883, r2: -0.031, rate:0.438, acc:0.562 lr 1.6395925239251574e-06\n",
      "epoch:13 step:6800 train loss: 0.891, r2: 0.102, rate:0.562, acc:0.375 val loss: 1.362, r2: -1.110, rate:0.562, acc:0.750 lr 1.6395925239251574e-06\n",
      "epoch:13 step:6900 train loss: 0.690, r2: 0.388, rate:0.875, acc:0.562 val loss: 1.003, r2: -3.733, rate:0.562, acc:0.062 lr 1.6395925239251574e-06\n",
      "epoch:13 step:7000 train loss: 0.739, r2: -0.267, rate:0.438, acc:0.438 val loss: 1.223, r2: 0.242, rate:0.562, acc:0.625 lr 1.6395925239251574e-06\n",
      "epoch:13 step:7100 train loss: 0.775, r2: -1.010, rate:0.750, acc:0.312 val loss: 0.859, r2: -2.084, rate:0.562, acc:0.375 lr 1.6395925239251574e-06\n",
      "epoch:13 step:7200 train loss: 0.775, r2: -4.339, rate:0.625, acc:1.000 val loss: 0.757, r2: 0.434, rate:0.688, acc:0.688 lr 1.6395925239251574e-06\n",
      "epoch:13 step:7300 train loss: 1.443, r2: -3.217, rate:0.438, acc:0.250 val loss: 1.487, r2: -0.258, rate:0.688, acc:0.875 lr 1.6395925239251574e-06\n",
      "epoch:13 step:7400 train loss: 2.386, r2: 0.477, rate:0.750, acc:1.000 val loss: 0.996, r2: 0.059, rate:0.750, acc:0.562 lr 1.6395925239251574e-06\n",
      "epoch:13 step:7500 train loss: 1.761, r2: -2.694, rate:0.562, acc:0.625 val loss: 1.972, r2: -3.259, rate:0.500, acc:0.562 lr 1.6395925239251574e-06\n",
      "epoch:13 step:7600 train loss: 1.830, r2: -0.062, rate:0.688, acc:0.625 val loss: 1.655, r2: 0.383, rate:0.812, acc:0.750 lr 1.6395925239251574e-06\n",
      "epoch:13 step:7700 train loss: 0.601, r2: 0.159, rate:0.750, acc:0.875 val loss: 0.764, r2: 0.159, rate:0.500, acc:0.688 lr 1.6395925239251574e-06\n",
      "epoch:13 step:7800 train loss: 1.070, r2: -0.015, rate:0.750, acc:0.938 val loss: 1.387, r2: -0.938, rate:0.688, acc:0.562 lr 1.4756332715326416e-06\n",
      "epoch:13 step:0.326 lr 0.000001\n",
      "epoch:14 step:0 train loss: 0.870, r2: -0.600, rate:0.938, acc:1.000 val loss: 3.376, r2: 0.264, rate:0.562, acc:0.500 lr 1.4756332715326416e-06\n",
      "epoch:14 step:100 train loss: 0.758, r2: -0.635, rate:0.688, acc:0.875 val loss: 0.788, r2: 0.369, rate:0.812, acc:0.875 lr 1.4756332715326416e-06\n",
      "epoch:14 step:200 train loss: 1.079, r2: -0.651, rate:0.625, acc:0.938 val loss: 0.903, r2: -0.145, rate:0.750, acc:0.688 lr 1.4756332715326416e-06\n",
      "epoch:14 step:300 train loss: 0.982, r2: -0.680, rate:0.625, acc:0.438 val loss: 0.978, r2: 0.428, rate:0.562, acc:0.188 lr 1.4756332715326416e-06\n",
      "epoch:14 step:400 train loss: 1.192, r2: 0.460, rate:0.875, acc:0.750 val loss: 0.496, r2: 0.476, rate:0.875, acc:0.875 lr 1.4756332715326416e-06\n",
      "epoch:14 step:500 train loss: 0.596, r2: 0.570, rate:0.812, acc:0.750 val loss: 0.594, r2: 0.685, rate:0.688, acc:0.875 lr 1.4756332715326416e-06\n",
      "epoch:14 step:600 train loss: 1.751, r2: -0.525, rate:0.688, acc:0.438 val loss: 1.097, r2: -4.886, rate:0.375, acc:0.688 lr 1.4756332715326416e-06\n",
      "epoch:14 step:700 train loss: 0.515, r2: 0.384, rate:0.875, acc:0.875 val loss: 0.761, r2: 0.414, rate:0.812, acc:0.875 lr 1.4756332715326416e-06\n",
      "epoch:14 step:800 train loss: 2.605, r2: 0.004, rate:0.375, acc:0.500 val loss: 0.734, r2: 0.345, rate:0.688, acc:0.500 lr 1.4756332715326416e-06\n",
      "epoch:14 step:900 train loss: 0.657, r2: -2.211, rate:0.750, acc:0.688 val loss: 1.110, r2: -0.122, rate:0.562, acc:0.625 lr 1.4756332715326416e-06\n",
      "epoch:14 step:1000 train loss: 1.056, r2: -7.199, rate:0.062, acc:0.250 val loss: 1.037, r2: -0.173, rate:0.562, acc:0.500 lr 1.3280699443793774e-06\n",
      "epoch:14 step:1100 train loss: 0.944, r2: 0.105, rate:0.688, acc:0.688 val loss: 0.853, r2: 0.212, rate:0.688, acc:0.625 lr 1.3280699443793774e-06\n",
      "epoch:14 step:1200 train loss: 1.845, r2: -0.372, rate:0.688, acc:0.500 val loss: 0.801, r2: -1.601, rate:0.750, acc:0.812 lr 1.3280699443793774e-06\n",
      "epoch:14 step:1300 train loss: 2.980, r2: -0.206, rate:0.688, acc:0.562 val loss: 0.773, r2: 0.195, rate:0.562, acc:0.438 lr 1.3280699443793774e-06\n",
      "epoch:14 step:1400 train loss: 0.586, r2: 0.687, rate:0.750, acc:0.812 val loss: 1.099, r2: -1.068, rate:0.625, acc:0.625 lr 1.3280699443793774e-06\n",
      "epoch:14 step:1500 train loss: 0.803, r2: -0.682, rate:0.562, acc:0.688 val loss: 0.655, r2: -0.194, rate:0.625, acc:0.875 lr 1.3280699443793774e-06\n",
      "epoch:14 step:1600 train loss: 0.585, r2: -0.429, rate:0.875, acc:0.812 val loss: 1.075, r2: -0.236, rate:0.688, acc:0.625 lr 1.3280699443793774e-06\n",
      "epoch:14 step:1700 train loss: 0.955, r2: -0.499, rate:0.438, acc:0.438 val loss: 0.860, r2: 0.270, rate:0.688, acc:0.750 lr 1.3280699443793774e-06\n",
      "epoch:14 step:1800 train loss: 2.679, r2: -5.698, rate:0.562, acc:0.500 val loss: 0.897, r2: 0.012, rate:0.688, acc:0.750 lr 1.3280699443793774e-06\n",
      "epoch:14 step:1900 train loss: 1.089, r2: 0.503, rate:0.750, acc:0.750 val loss: 2.582, r2: -0.678, rate:0.750, acc:0.812 lr 1.3280699443793774e-06\n",
      "epoch:14 step:2000 train loss: 1.303, r2: 0.485, rate:0.688, acc:0.688 val loss: 0.763, r2: 0.277, rate:0.750, acc:0.688 lr 1.3280699443793774e-06\n",
      "epoch:14 step:2100 train loss: 0.960, r2: -0.152, rate:0.812, acc:0.812 val loss: 1.511, r2: -0.579, rate:0.562, acc:0.438 lr 1.1952629499414398e-06\n",
      "epoch:14 step:2200 train loss: 1.717, r2: 0.057, rate:0.750, acc:0.438 val loss: 1.155, r2: -1.445, rate:0.750, acc:0.312 lr 1.1952629499414398e-06\n",
      "epoch:14 step:2300 train loss: 0.793, r2: -0.218, rate:0.625, acc:0.500 val loss: 1.318, r2: 0.377, rate:0.688, acc:0.750 lr 1.1952629499414398e-06\n",
      "epoch:14 step:2400 train loss: 3.727, r2: -2.756, rate:0.312, acc:0.438 val loss: 1.575, r2: -0.851, rate:0.688, acc:0.875 lr 1.1952629499414398e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:2500 train loss: 0.714, r2: -0.399, rate:0.688, acc:0.250 val loss: 0.963, r2: -2.082, rate:0.562, acc:0.688 lr 1.1952629499414398e-06\n",
      "epoch:14 step:2600 train loss: 0.801, r2: 0.179, rate:0.625, acc:0.375 val loss: 0.907, r2: 0.296, rate:0.375, acc:0.500 lr 1.1952629499414398e-06\n",
      "epoch:14 step:2700 train loss: 0.549, r2: -7.757, rate:0.688, acc:1.000 val loss: 0.609, r2: 0.650, rate:0.625, acc:0.750 lr 1.1952629499414398e-06\n",
      "epoch:14 step:2800 train loss: 0.637, r2: 0.523, rate:0.750, acc:0.688 val loss: 1.088, r2: -0.700, rate:0.688, acc:0.812 lr 1.1952629499414398e-06\n",
      "epoch:14 step:2900 train loss: 0.836, r2: 0.193, rate:0.625, acc:0.625 val loss: 1.030, r2: -5.093, rate:0.500, acc:0.625 lr 1.1952629499414398e-06\n",
      "epoch:14 step:3000 train loss: 0.893, r2: -2.494, rate:0.625, acc:0.688 val loss: 0.657, r2: 0.477, rate:0.562, acc:0.688 lr 1.1952629499414398e-06\n",
      "epoch:14 step:3100 train loss: 0.814, r2: 0.157, rate:0.750, acc:0.375 val loss: 1.627, r2: -0.674, rate:0.625, acc:0.688 lr 1.1952629499414398e-06\n",
      "epoch:14 step:3200 train loss: 0.629, r2: 0.699, rate:0.938, acc:0.812 val loss: 0.628, r2: 0.463, rate:0.875, acc:0.688 lr 1.075736654947296e-06\n",
      "epoch:14 step:3300 train loss: 0.999, r2: -1.919, rate:0.375, acc:0.562 val loss: 0.824, r2: -2.461, rate:0.375, acc:0.688 lr 1.075736654947296e-06\n",
      "epoch:14 step:3400 train loss: 1.050, r2: -6.348, rate:0.250, acc:0.750 val loss: 0.780, r2: 0.266, rate:0.812, acc:0.938 lr 1.075736654947296e-06\n",
      "epoch:14 step:3500 train loss: 0.790, r2: 0.539, rate:0.625, acc:0.312 val loss: 0.847, r2: 0.212, rate:0.812, acc:0.562 lr 1.075736654947296e-06\n",
      "epoch:14 step:3600 train loss: 6.853, r2: -4.975, rate:0.688, acc:0.562 val loss: 1.938, r2: 0.002, rate:0.750, acc:0.750 lr 1.075736654947296e-06\n",
      "epoch:14 step:3700 train loss: 0.710, r2: -0.037, rate:0.812, acc:0.438 val loss: 1.060, r2: 0.027, rate:0.500, acc:0.438 lr 1.075736654947296e-06\n",
      "epoch:14 step:3800 train loss: 4.198, r2: -3.692, rate:0.625, acc:0.688 val loss: 1.425, r2: -1.266, rate:0.688, acc:0.375 lr 1.075736654947296e-06\n",
      "epoch:14 step:3900 train loss: 0.602, r2: 0.225, rate:0.750, acc:0.938 val loss: 1.448, r2: -1.878, rate:0.438, acc:0.500 lr 1.075736654947296e-06\n",
      "epoch:14 step:4000 train loss: 0.798, r2: -0.727, rate:0.562, acc:0.688 val loss: 1.680, r2: -0.211, rate:0.500, acc:0.438 lr 1.075736654947296e-06\n",
      "epoch:14 step:4100 train loss: 1.074, r2: -0.558, rate:0.688, acc:0.562 val loss: 0.690, r2: 0.402, rate:0.625, acc:0.688 lr 1.075736654947296e-06\n",
      "epoch:14 step:4200 train loss: 1.284, r2: -21.237, rate:0.125, acc:0.125 val loss: 0.731, r2: -0.114, rate:0.750, acc:0.312 lr 1.075736654947296e-06\n",
      "epoch:14 step:4300 train loss: 0.968, r2: -2.239, rate:0.500, acc:0.375 val loss: 0.698, r2: 0.537, rate:0.812, acc:0.625 lr 9.681629894525663e-07\n",
      "epoch:14 step:4400 train loss: 2.109, r2: 0.209, rate:0.562, acc:0.625 val loss: 1.515, r2: -0.465, rate:0.500, acc:0.500 lr 9.681629894525663e-07\n",
      "epoch:14 step:4500 train loss: 0.611, r2: 0.641, rate:0.625, acc:1.000 val loss: 0.821, r2: 0.214, rate:0.750, acc:0.812 lr 9.681629894525663e-07\n",
      "epoch:14 step:4600 train loss: 0.979, r2: -1.481, rate:0.312, acc:0.188 val loss: 1.123, r2: -0.714, rate:0.562, acc:0.500 lr 9.681629894525663e-07\n",
      "epoch:14 step:4700 train loss: 1.080, r2: 0.519, rate:0.688, acc:0.562 val loss: 1.377, r2: -0.915, rate:0.500, acc:0.750 lr 9.681629894525663e-07\n",
      "epoch:14 step:4800 train loss: 0.935, r2: 0.271, rate:0.750, acc:0.625 val loss: 0.981, r2: -0.113, rate:0.688, acc:0.812 lr 9.681629894525663e-07\n",
      "epoch:14 step:4900 train loss: 0.717, r2: 0.503, rate:0.750, acc:0.875 val loss: 1.078, r2: -14.215, rate:0.312, acc:0.375 lr 9.681629894525663e-07\n",
      "epoch:14 step:5000 train loss: 0.745, r2: -0.931, rate:0.750, acc:0.312 val loss: 0.676, r2: 0.388, rate:0.812, acc:0.688 lr 9.681629894525663e-07\n",
      "epoch:14 step:5100 train loss: 0.637, r2: 0.733, rate:0.875, acc:0.938 val loss: 0.703, r2: 0.654, rate:0.812, acc:0.875 lr 9.681629894525663e-07\n",
      "epoch:14 step:5200 train loss: 0.797, r2: -3.742, rate:0.438, acc:0.438 val loss: 1.236, r2: -1.100, rate:0.500, acc:0.625 lr 9.681629894525663e-07\n",
      "epoch:14 step:5300 train loss: 1.995, r2: -0.739, rate:0.500, acc:0.312 val loss: 0.789, r2: -0.573, rate:0.688, acc:0.750 lr 9.681629894525663e-07\n",
      "epoch:14 step:5400 train loss: 1.414, r2: -3.706, rate:0.375, acc:0.312 val loss: 0.689, r2: 0.251, rate:0.875, acc:0.500 lr 8.713466905073097e-07\n",
      "epoch:14 step:5500 train loss: 2.203, r2: -3.177, rate:0.562, acc:0.438 val loss: 0.670, r2: -0.628, rate:0.750, acc:0.938 lr 8.713466905073097e-07\n",
      "epoch:14 step:5600 train loss: 1.408, r2: -0.254, rate:0.500, acc:0.625 val loss: 0.761, r2: -3.472, rate:0.875, acc:0.625 lr 8.713466905073097e-07\n",
      "epoch:14 step:5700 train loss: 0.666, r2: 0.666, rate:1.000, acc:0.500 val loss: 1.434, r2: -1.015, rate:0.562, acc:0.688 lr 8.713466905073097e-07\n",
      "epoch:14 step:5800 train loss: 0.587, r2: -1.604, rate:0.812, acc:0.938 val loss: 0.975, r2: -0.033, rate:0.625, acc:0.688 lr 8.713466905073097e-07\n",
      "epoch:14 step:5900 train loss: 1.217, r2: 0.309, rate:0.688, acc:0.438 val loss: 0.762, r2: -1.377, rate:0.688, acc:0.438 lr 8.713466905073097e-07\n",
      "epoch:14 step:6000 train loss: 0.723, r2: 0.588, rate:0.812, acc:0.688 val loss: 0.988, r2: -2.379, rate:0.750, acc:0.812 lr 8.713466905073097e-07\n",
      "epoch:14 step:6100 train loss: 0.704, r2: -0.333, rate:0.875, acc:0.875 val loss: 0.762, r2: -6.397, rate:0.500, acc:0.750 lr 8.713466905073097e-07\n",
      "epoch:14 step:6200 train loss: 0.829, r2: -0.074, rate:0.750, acc:0.938 val loss: 1.095, r2: 0.505, rate:0.750, acc:0.750 lr 8.713466905073097e-07\n",
      "epoch:14 step:6300 train loss: 0.817, r2: 0.172, rate:0.750, acc:0.562 val loss: 1.208, r2: -1.130, rate:0.562, acc:0.500 lr 8.713466905073097e-07\n",
      "epoch:14 step:6400 train loss: 0.874, r2: -1.444, rate:0.875, acc:0.938 val loss: 1.514, r2: -2.805, rate:0.562, acc:0.312 lr 8.713466905073097e-07\n",
      "epoch:14 step:6500 train loss: 5.550, r2: -2.253, rate:0.875, acc:0.875 val loss: 0.674, r2: -0.211, rate:0.875, acc:0.500 lr 7.842120214565787e-07\n",
      "epoch:14 step:6600 train loss: 1.359, r2: 0.251, rate:0.500, acc:0.500 val loss: 0.986, r2: -0.780, rate:0.688, acc:0.750 lr 7.842120214565787e-07\n",
      "epoch:14 step:6700 train loss: 2.825, r2: 0.029, rate:0.750, acc:0.562 val loss: 0.975, r2: -0.235, rate:0.688, acc:0.500 lr 7.842120214565787e-07\n",
      "epoch:14 step:6800 train loss: 1.721, r2: -0.581, rate:0.688, acc:0.812 val loss: 2.165, r2: -0.441, rate:0.500, acc:0.625 lr 7.842120214565787e-07\n",
      "epoch:14 step:6900 train loss: 1.107, r2: -1.838, rate:0.312, acc:0.250 val loss: 1.208, r2: 0.288, rate:0.750, acc:0.812 lr 7.842120214565787e-07\n",
      "epoch:14 step:7000 train loss: 0.561, r2: 0.708, rate:0.750, acc:0.750 val loss: 1.005, r2: 0.448, rate:0.562, acc:0.688 lr 7.842120214565787e-07\n",
      "epoch:14 step:7100 train loss: 0.844, r2: -0.001, rate:0.750, acc:0.500 val loss: 0.694, r2: 0.368, rate:0.750, acc:0.562 lr 7.842120214565787e-07\n",
      "epoch:14 step:7200 train loss: 0.641, r2: 0.472, rate:0.875, acc:0.875 val loss: 1.130, r2: 0.289, rate:0.688, acc:0.562 lr 7.842120214565787e-07\n",
      "epoch:14 step:7300 train loss: 0.916, r2: -1.183, rate:0.438, acc:0.312 val loss: 1.214, r2: 0.302, rate:0.812, acc:0.625 lr 7.842120214565787e-07\n",
      "epoch:14 step:7400 train loss: 1.898, r2: -1.008, rate:0.625, acc:0.250 val loss: 0.847, r2: -0.616, rate:0.438, acc:0.812 lr 7.842120214565787e-07\n",
      "epoch:14 step:7500 train loss: 0.530, r2: 0.409, rate:0.938, acc:0.875 val loss: 0.711, r2: 0.540, rate:0.750, acc:0.500 lr 7.842120214565787e-07\n",
      "epoch:14 step:7600 train loss: 0.876, r2: -0.292, rate:0.625, acc:0.750 val loss: 0.621, r2: 0.249, rate:0.750, acc:0.750 lr 7.057908193109208e-07\n",
      "epoch:14 step:7700 train loss: 0.953, r2: 0.560, rate:0.875, acc:0.750 val loss: 0.888, r2: 0.492, rate:0.750, acc:0.688 lr 7.057908193109208e-07\n",
      "epoch:14 step:7800 train loss: 0.607, r2: -0.108, rate:0.875, acc:0.812 val loss: 0.622, r2: -0.893, rate:0.750, acc:0.875 lr 7.057908193109208e-07\n",
      "epoch:14 step:0.327 lr 0.000001\n",
      "epoch:15 step:0 train loss: 1.205, r2: 0.417, rate:0.688, acc:0.688 val loss: 1.004, r2: 0.291, rate:0.688, acc:0.688 lr 7.057908193109208e-07\n",
      "epoch:15 step:100 train loss: 1.168, r2: -4.948, rate:0.500, acc:0.438 val loss: 0.823, r2: 0.160, rate:0.750, acc:0.562 lr 7.057908193109208e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:200 train loss: 1.205, r2: -0.049, rate:0.812, acc:0.562 val loss: 1.266, r2: 0.289, rate:0.750, acc:0.750 lr 7.057908193109208e-07\n",
      "epoch:15 step:300 train loss: 0.677, r2: 0.669, rate:0.812, acc:0.812 val loss: 1.122, r2: 0.134, rate:0.438, acc:0.562 lr 7.057908193109208e-07\n",
      "epoch:15 step:400 train loss: 1.103, r2: -0.300, rate:0.500, acc:0.562 val loss: 0.817, r2: 0.537, rate:0.500, acc:0.812 lr 7.057908193109208e-07\n",
      "epoch:15 step:500 train loss: 0.644, r2: -0.170, rate:0.875, acc:0.875 val loss: 1.185, r2: -0.324, rate:0.500, acc:0.438 lr 7.057908193109208e-07\n",
      "epoch:15 step:600 train loss: 1.062, r2: -0.598, rate:0.438, acc:0.312 val loss: 0.889, r2: -0.254, rate:0.562, acc:0.500 lr 7.057908193109208e-07\n",
      "epoch:15 step:700 train loss: 0.787, r2: -4.811, rate:0.438, acc:0.750 val loss: 0.507, r2: 0.190, rate:1.000, acc:0.625 lr 7.057908193109208e-07\n",
      "epoch:15 step:800 train loss: 0.763, r2: -0.298, rate:0.562, acc:0.812 val loss: 0.868, r2: -0.235, rate:0.688, acc:0.688 lr 6.352117373798287e-07\n",
      "epoch:15 step:900 train loss: 0.689, r2: -0.257, rate:0.750, acc:0.750 val loss: 0.804, r2: 0.446, rate:0.750, acc:0.562 lr 6.352117373798287e-07\n",
      "epoch:15 step:1000 train loss: 0.930, r2: -1.854, rate:0.500, acc:0.875 val loss: 1.931, r2: 0.083, rate:0.562, acc:0.688 lr 6.352117373798287e-07\n",
      "epoch:15 step:1100 train loss: 0.944, r2: -0.252, rate:0.688, acc:0.812 val loss: 0.637, r2: 0.390, rate:0.812, acc:0.688 lr 6.352117373798287e-07\n",
      "epoch:15 step:1200 train loss: 1.626, r2: -1.747, rate:0.562, acc:0.688 val loss: 0.789, r2: -0.925, rate:0.562, acc:0.562 lr 6.352117373798287e-07\n",
      "epoch:15 step:1300 train loss: 0.884, r2: -0.574, rate:0.938, acc:0.875 val loss: 1.062, r2: -2.894, rate:0.812, acc:1.000 lr 6.352117373798287e-07\n",
      "epoch:15 step:1400 train loss: 0.735, r2: 0.121, rate:0.812, acc:0.188 val loss: 0.475, r2: 0.782, rate:0.938, acc:0.688 lr 6.352117373798287e-07\n",
      "epoch:15 step:1500 train loss: 0.683, r2: -0.562, rate:0.500, acc:0.750 val loss: 0.956, r2: 0.312, rate:0.562, acc:0.500 lr 6.352117373798287e-07\n",
      "epoch:15 step:1600 train loss: 0.619, r2: 0.212, rate:0.812, acc:0.562 val loss: 2.376, r2: -0.079, rate:0.750, acc:0.812 lr 6.352117373798287e-07\n",
      "epoch:15 step:1700 train loss: 5.873, r2: -5.363, rate:0.500, acc:0.750 val loss: 0.889, r2: -5.313, rate:0.250, acc:0.312 lr 6.352117373798287e-07\n",
      "epoch:15 step:1800 train loss: 0.723, r2: -2.213, rate:0.625, acc:0.562 val loss: 0.548, r2: 0.570, rate:0.938, acc:0.750 lr 6.352117373798287e-07\n",
      "epoch:15 step:1900 train loss: 1.103, r2: -1.048, rate:0.688, acc:0.438 val loss: 0.364, r2: 0.247, rate:1.000, acc:1.000 lr 5.716905636418458e-07\n",
      "epoch:15 step:2000 train loss: 2.384, r2: -0.531, rate:0.562, acc:0.312 val loss: 0.735, r2: -0.105, rate:0.688, acc:0.438 lr 5.716905636418458e-07\n",
      "epoch:15 step:2100 train loss: 1.120, r2: -0.004, rate:0.625, acc:0.812 val loss: 0.977, r2: -4.109, rate:0.562, acc:0.375 lr 5.716905636418458e-07\n",
      "epoch:15 step:2200 train loss: 3.743, r2: -4.649, rate:0.250, acc:0.750 val loss: 1.286, r2: -1.296, rate:0.688, acc:0.438 lr 5.716905636418458e-07\n",
      "epoch:15 step:2300 train loss: 0.706, r2: 0.258, rate:0.938, acc:0.500 val loss: 1.243, r2: 0.413, rate:0.750, acc:0.750 lr 5.716905636418458e-07\n",
      "epoch:15 step:2400 train loss: 0.609, r2: 0.384, rate:0.875, acc:0.688 val loss: 1.548, r2: -0.036, rate:0.812, acc:0.688 lr 5.716905636418458e-07\n",
      "epoch:15 step:2500 train loss: 0.889, r2: 0.552, rate:0.562, acc:0.500 val loss: 0.765, r2: -0.124, rate:0.750, acc:0.625 lr 5.716905636418458e-07\n",
      "epoch:15 step:2600 train loss: 7.550, r2: -5.061, rate:0.375, acc:0.312 val loss: 1.334, r2: 0.168, rate:0.875, acc:0.812 lr 5.716905636418458e-07\n",
      "epoch:15 step:2700 train loss: 4.070, r2: -2.992, rate:0.312, acc:0.312 val loss: 0.918, r2: -1.796, rate:0.312, acc:0.500 lr 5.716905636418458e-07\n",
      "epoch:15 step:2800 train loss: 0.842, r2: -2.433, rate:0.312, acc:0.562 val loss: 1.190, r2: -0.196, rate:0.688, acc:0.562 lr 5.716905636418458e-07\n",
      "epoch:15 step:2900 train loss: 0.571, r2: 0.237, rate:0.750, acc:1.000 val loss: 0.864, r2: 0.620, rate:0.562, acc:0.250 lr 5.716905636418458e-07\n",
      "epoch:15 step:3000 train loss: 0.539, r2: 0.658, rate:0.938, acc:0.938 val loss: 1.435, r2: -0.128, rate:0.625, acc:0.500 lr 5.145215072776612e-07\n",
      "epoch:15 step:3100 train loss: 1.118, r2: -0.088, rate:0.688, acc:0.688 val loss: 0.745, r2: 0.315, rate:0.812, acc:0.625 lr 5.145215072776612e-07\n",
      "epoch:15 step:3200 train loss: 1.569, r2: -0.490, rate:0.875, acc:0.562 val loss: 0.790, r2: 0.071, rate:0.438, acc:0.938 lr 5.145215072776612e-07\n",
      "epoch:15 step:3300 train loss: 0.805, r2: -0.680, rate:0.875, acc:0.875 val loss: 0.838, r2: 0.336, rate:0.812, acc:0.812 lr 5.145215072776612e-07\n",
      "epoch:15 step:3400 train loss: 0.545, r2: -0.728, rate:0.812, acc:0.938 val loss: 0.974, r2: -1.902, rate:0.625, acc:0.812 lr 5.145215072776612e-07\n",
      "epoch:15 step:3500 train loss: 1.464, r2: -1.327, rate:0.562, acc:0.750 val loss: 0.884, r2: 0.210, rate:0.438, acc:0.500 lr 5.145215072776612e-07\n",
      "epoch:15 step:3600 train loss: 0.797, r2: -0.015, rate:0.625, acc:0.312 val loss: 1.163, r2: -0.244, rate:0.500, acc:0.688 lr 5.145215072776612e-07\n",
      "epoch:15 step:3700 train loss: 1.312, r2: -1.061, rate:0.250, acc:0.250 val loss: 0.662, r2: -0.440, rate:0.875, acc:0.938 lr 5.145215072776612e-07\n",
      "epoch:15 step:3800 train loss: 0.762, r2: -1.899, rate:0.688, acc:0.625 val loss: 1.141, r2: -0.370, rate:0.625, acc:0.562 lr 5.145215072776612e-07\n",
      "epoch:15 step:3900 train loss: 0.732, r2: -0.644, rate:0.625, acc:0.688 val loss: 0.762, r2: 0.519, rate:0.812, acc:0.812 lr 5.145215072776612e-07\n",
      "epoch:15 step:4000 train loss: 1.398, r2: -0.308, rate:0.625, acc:0.812 val loss: 1.131, r2: 0.408, rate:0.875, acc:0.750 lr 5.145215072776612e-07\n",
      "epoch:15 step:4100 train loss: 0.916, r2: 0.436, rate:0.562, acc:0.375 val loss: 0.852, r2: 0.231, rate:0.750, acc:0.938 lr 4.630693565498951e-07\n",
      "epoch:15 step:4200 train loss: 1.340, r2: -3.539, rate:0.688, acc:0.562 val loss: 1.564, r2: -0.456, rate:0.750, acc:0.375 lr 4.630693565498951e-07\n",
      "epoch:15 step:4300 train loss: 1.323, r2: -0.617, rate:0.562, acc:0.625 val loss: 0.968, r2: -0.887, rate:0.625, acc:0.625 lr 4.630693565498951e-07\n",
      "epoch:15 step:4400 train loss: 2.465, r2: -2.055, rate:0.438, acc:0.375 val loss: 0.771, r2: -0.256, rate:0.625, acc:0.375 lr 4.630693565498951e-07\n",
      "epoch:15 step:4500 train loss: 0.733, r2: -0.214, rate:0.750, acc:1.000 val loss: 1.607, r2: -0.628, rate:0.562, acc:0.812 lr 4.630693565498951e-07\n",
      "epoch:15 step:4600 train loss: 1.846, r2: -1.803, rate:0.625, acc:0.625 val loss: 0.769, r2: -2.320, rate:0.688, acc:1.000 lr 4.630693565498951e-07\n",
      "epoch:15 step:4700 train loss: 0.825, r2: 0.501, rate:0.562, acc:0.562 val loss: 0.721, r2: 0.599, rate:0.750, acc:0.500 lr 4.630693565498951e-07\n",
      "epoch:15 step:4800 train loss: 1.111, r2: -0.401, rate:0.688, acc:0.562 val loss: 0.770, r2: 0.394, rate:0.750, acc:0.750 lr 4.630693565498951e-07\n",
      "epoch:15 step:4900 train loss: 2.219, r2: -17.516, rate:0.812, acc:0.812 val loss: 0.725, r2: -1.625, rate:0.688, acc:0.938 lr 4.630693565498951e-07\n",
      "epoch:15 step:5000 train loss: 0.718, r2: 0.294, rate:0.688, acc:0.625 val loss: 1.082, r2: -0.309, rate:0.688, acc:0.500 lr 4.630693565498951e-07\n",
      "epoch:15 step:5100 train loss: 0.959, r2: -0.892, rate:0.438, acc:0.688 val loss: 2.290, r2: -1.592, rate:0.812, acc:0.750 lr 4.630693565498951e-07\n",
      "epoch:15 step:5200 train loss: 0.463, r2: 0.432, rate:0.875, acc:0.938 val loss: 1.269, r2: 0.393, rate:0.875, acc:0.688 lr 4.167624208949056e-07\n",
      "epoch:15 step:5300 train loss: 1.085, r2: 0.289, rate:0.500, acc:0.688 val loss: 1.134, r2: 0.390, rate:0.625, acc:0.500 lr 4.167624208949056e-07\n",
      "epoch:15 step:5400 train loss: 0.954, r2: 0.453, rate:0.500, acc:0.750 val loss: 0.769, r2: 0.410, rate:0.812, acc:0.812 lr 4.167624208949056e-07\n",
      "epoch:15 step:5500 train loss: 1.503, r2: 0.296, rate:0.750, acc:0.688 val loss: 0.882, r2: 0.358, rate:0.750, acc:0.812 lr 4.167624208949056e-07\n",
      "epoch:15 step:5600 train loss: 0.947, r2: -0.952, rate:0.562, acc:0.812 val loss: 1.318, r2: 0.399, rate:0.688, acc:0.750 lr 4.167624208949056e-07\n",
      "epoch:15 step:5700 train loss: 0.639, r2: 0.344, rate:0.750, acc:0.688 val loss: 1.053, r2: -0.365, rate:0.625, acc:0.500 lr 4.167624208949056e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:5800 train loss: 0.869, r2: 0.489, rate:0.750, acc:0.562 val loss: 2.204, r2: -0.392, rate:0.688, acc:0.812 lr 4.167624208949056e-07\n",
      "epoch:15 step:5900 train loss: 1.155, r2: 0.497, rate:0.750, acc:0.750 val loss: 2.162, r2: -1.034, rate:0.438, acc:0.500 lr 4.167624208949056e-07\n",
      "epoch:15 step:6000 train loss: 1.517, r2: -0.191, rate:0.688, acc:0.625 val loss: 0.773, r2: 0.473, rate:0.688, acc:0.812 lr 4.167624208949056e-07\n",
      "epoch:15 step:6100 train loss: 0.621, r2: 0.118, rate:0.750, acc:0.875 val loss: 0.726, r2: -0.454, rate:0.625, acc:0.875 lr 4.167624208949056e-07\n",
      "epoch:15 step:6200 train loss: 2.780, r2: -16.031, rate:0.875, acc:1.000 val loss: 0.721, r2: 0.130, rate:0.688, acc:0.812 lr 4.167624208949056e-07\n",
      "epoch:15 step:6300 train loss: 0.923, r2: -0.432, rate:0.625, acc:0.438 val loss: 0.752, r2: 0.375, rate:0.875, acc:0.438 lr 3.7508617880541505e-07\n",
      "epoch:15 step:6400 train loss: 1.209, r2: 0.014, rate:0.312, acc:0.562 val loss: 0.556, r2: 0.678, rate:0.875, acc:0.812 lr 3.7508617880541505e-07\n",
      "epoch:15 step:6500 train loss: 4.585, r2: -0.261, rate:0.688, acc:0.500 val loss: 1.888, r2: -1.370, rate:0.750, acc:0.938 lr 3.7508617880541505e-07\n",
      "epoch:15 step:6600 train loss: 0.804, r2: 0.135, rate:0.750, acc:0.562 val loss: 1.012, r2: 0.516, rate:0.812, acc:0.750 lr 3.7508617880541505e-07\n",
      "epoch:15 step:6700 train loss: 1.079, r2: 0.472, rate:0.688, acc:0.312 val loss: 0.784, r2: 0.517, rate:0.812, acc:0.625 lr 3.7508617880541505e-07\n",
      "epoch:15 step:6800 train loss: 0.714, r2: 0.505, rate:0.812, acc:0.875 val loss: 1.400, r2: -1.495, rate:0.562, acc:0.812 lr 3.7508617880541505e-07\n",
      "epoch:15 step:6900 train loss: 1.889, r2: -3.658, rate:0.750, acc:0.688 val loss: 1.050, r2: 0.460, rate:0.812, acc:0.812 lr 3.7508617880541505e-07\n",
      "epoch:15 step:7000 train loss: 0.962, r2: 0.133, rate:0.938, acc:0.938 val loss: 1.691, r2: 0.630, rate:0.688, acc:0.688 lr 3.7508617880541505e-07\n",
      "epoch:15 step:7100 train loss: 1.335, r2: -0.656, rate:0.438, acc:0.375 val loss: 0.785, r2: 0.479, rate:0.750, acc:0.812 lr 3.7508617880541505e-07\n",
      "epoch:15 step:7200 train loss: 0.554, r2: 0.606, rate:0.875, acc:0.875 val loss: 0.677, r2: 0.525, rate:0.875, acc:0.688 lr 3.7508617880541505e-07\n",
      "epoch:15 step:7300 train loss: 1.271, r2: 0.606, rate:0.688, acc:0.688 val loss: 1.549, r2: -0.224, rate:0.750, acc:0.688 lr 3.7508617880541505e-07\n",
      "epoch:15 step:7400 train loss: 1.446, r2: -0.175, rate:0.625, acc:0.875 val loss: 0.651, r2: -1.181, rate:0.625, acc:0.875 lr 3.3757756092487355e-07\n",
      "epoch:15 step:7500 train loss: 0.958, r2: -0.549, rate:0.562, acc:0.688 val loss: 0.840, r2: -0.104, rate:0.812, acc:0.625 lr 3.3757756092487355e-07\n",
      "epoch:15 step:7600 train loss: 2.468, r2: 0.380, rate:0.750, acc:0.625 val loss: 0.733, r2: 0.481, rate:0.750, acc:0.625 lr 3.3757756092487355e-07\n",
      "epoch:15 step:7700 train loss: 0.865, r2: -0.197, rate:0.500, acc:0.500 val loss: 0.833, r2: -1.327, rate:0.750, acc:0.375 lr 3.3757756092487355e-07\n",
      "epoch:15 step:7800 train loss: 0.746, r2: -1.672, rate:0.750, acc:0.625 val loss: 1.326, r2: 0.269, rate:0.750, acc:0.438 lr 3.3757756092487355e-07\n",
      "epoch:15 step:0.327 lr 0.000000\n",
      "epoch:16 step:0 train loss: 0.783, r2: 0.642, rate:0.750, acc:0.375 val loss: 0.738, r2: -3.152, rate:0.688, acc:0.625 lr 3.3757756092487355e-07\n",
      "epoch:16 step:100 train loss: 1.711, r2: -0.116, rate:0.500, acc:0.500 val loss: 0.525, r2: -0.493, rate:0.938, acc:0.688 lr 3.3757756092487355e-07\n",
      "epoch:16 step:200 train loss: 0.884, r2: 0.178, rate:0.625, acc:0.688 val loss: 0.934, r2: -0.564, rate:0.562, acc:0.625 lr 3.3757756092487355e-07\n",
      "epoch:16 step:300 train loss: 0.919, r2: -0.869, rate:0.750, acc:0.750 val loss: 1.069, r2: 0.293, rate:0.625, acc:0.812 lr 3.3757756092487355e-07\n",
      "epoch:16 step:400 train loss: 1.669, r2: -2.023, rate:0.750, acc:0.375 val loss: 1.096, r2: -0.115, rate:0.500, acc:0.750 lr 3.3757756092487355e-07\n",
      "epoch:16 step:500 train loss: 0.951, r2: -1.235, rate:0.375, acc:0.750 val loss: 1.036, r2: -4.291, rate:0.750, acc:1.000 lr 3.3757756092487355e-07\n",
      "epoch:16 step:600 train loss: 2.391, r2: -37.003, rate:0.875, acc:1.000 val loss: 1.289, r2: -3.532, rate:0.500, acc:0.188 lr 3.038198048323862e-07\n",
      "epoch:16 step:700 train loss: 0.758, r2: 0.765, rate:0.688, acc:0.688 val loss: 1.356, r2: -0.011, rate:0.688, acc:0.625 lr 3.038198048323862e-07\n",
      "epoch:16 step:800 train loss: 0.828, r2: -0.019, rate:0.688, acc:0.750 val loss: 0.600, r2: 0.643, rate:0.812, acc:0.500 lr 3.038198048323862e-07\n",
      "epoch:16 step:900 train loss: 1.014, r2: -0.752, rate:0.312, acc:0.625 val loss: 0.589, r2: 0.620, rate:0.750, acc:0.688 lr 3.038198048323862e-07\n",
      "epoch:16 step:1000 train loss: 0.724, r2: 0.591, rate:0.812, acc:0.812 val loss: 1.054, r2: 0.384, rate:0.750, acc:0.688 lr 3.038198048323862e-07\n",
      "epoch:16 step:1100 train loss: 0.941, r2: -0.179, rate:0.625, acc:0.562 val loss: 0.720, r2: 0.741, rate:0.812, acc:0.500 lr 3.038198048323862e-07\n",
      "epoch:16 step:1200 train loss: 1.485, r2: -2.411, rate:0.562, acc:0.812 val loss: 1.082, r2: 0.505, rate:0.562, acc:0.688 lr 3.038198048323862e-07\n",
      "epoch:16 step:1300 train loss: 0.868, r2: 0.470, rate:0.875, acc:0.688 val loss: 1.286, r2: -0.356, rate:0.625, acc:0.562 lr 3.038198048323862e-07\n",
      "epoch:16 step:1400 train loss: 0.832, r2: -4.359, rate:0.688, acc:0.875 val loss: 1.037, r2: -1.813, rate:0.750, acc:0.938 lr 3.038198048323862e-07\n",
      "epoch:16 step:1500 train loss: 0.673, r2: -1.146, rate:0.625, acc:0.625 val loss: 0.750, r2: -2.248, rate:0.688, acc:0.312 lr 3.038198048323862e-07\n",
      "epoch:16 step:1600 train loss: 0.659, r2: 0.005, rate:0.750, acc:0.688 val loss: 0.575, r2: 0.431, rate:0.750, acc:0.688 lr 3.038198048323862e-07\n",
      "epoch:16 step:1700 train loss: 4.200, r2: -3.681, rate:0.750, acc:0.875 val loss: 0.599, r2: 0.699, rate:0.875, acc:0.812 lr 2.734378243491476e-07\n",
      "epoch:16 step:1800 train loss: 0.673, r2: -0.260, rate:0.688, acc:0.938 val loss: 0.668, r2: -0.020, rate:0.938, acc:0.688 lr 2.734378243491476e-07\n",
      "epoch:16 step:1900 train loss: 0.571, r2: 0.051, rate:0.875, acc:0.625 val loss: 0.786, r2: 0.016, rate:0.875, acc:0.812 lr 2.734378243491476e-07\n",
      "epoch:16 step:2000 train loss: 1.167, r2: -0.214, rate:0.750, acc:0.812 val loss: 1.198, r2: -0.688, rate:0.562, acc:0.938 lr 2.734378243491476e-07\n",
      "epoch:16 step:2100 train loss: 0.660, r2: -0.085, rate:0.875, acc:0.688 val loss: 1.774, r2: 0.456, rate:0.750, acc:0.812 lr 2.734378243491476e-07\n",
      "epoch:16 step:2200 train loss: 0.617, r2: 0.165, rate:0.812, acc:0.688 val loss: 0.786, r2: 0.388, rate:0.750, acc:0.562 lr 2.734378243491476e-07\n",
      "epoch:16 step:2300 train loss: 1.557, r2: -1.191, rate:0.438, acc:0.938 val loss: 1.357, r2: -0.989, rate:0.688, acc:0.562 lr 2.734378243491476e-07\n",
      "epoch:16 step:2400 train loss: 0.920, r2: -0.508, rate:0.500, acc:0.625 val loss: 0.897, r2: -0.315, rate:0.562, acc:0.375 lr 2.734378243491476e-07\n",
      "epoch:16 step:2500 train loss: 0.709, r2: -0.067, rate:0.812, acc:0.812 val loss: 1.138, r2: 0.113, rate:0.812, acc:0.625 lr 2.734378243491476e-07\n",
      "epoch:16 step:2600 train loss: 1.872, r2: -0.782, rate:0.812, acc:0.688 val loss: 1.163, r2: -0.626, rate:0.500, acc:0.562 lr 2.734378243491476e-07\n",
      "epoch:16 step:2700 train loss: 1.234, r2: -10.506, rate:0.250, acc:0.500 val loss: 0.605, r2: 0.606, rate:0.812, acc:0.750 lr 2.734378243491476e-07\n",
      "epoch:16 step:2800 train loss: 0.680, r2: 0.692, rate:0.875, acc:0.750 val loss: 0.828, r2: 0.472, rate:0.875, acc:0.812 lr 2.4609404191423283e-07\n",
      "epoch:16 step:2900 train loss: 2.801, r2: -0.714, rate:0.750, acc:0.625 val loss: 1.007, r2: -1.286, rate:0.312, acc:0.812 lr 2.4609404191423283e-07\n",
      "epoch:16 step:3000 train loss: 1.106, r2: -3.207, rate:0.375, acc:0.375 val loss: 1.016, r2: 0.111, rate:0.625, acc:0.750 lr 2.4609404191423283e-07\n",
      "epoch:16 step:3100 train loss: 1.011, r2: 0.158, rate:0.812, acc:0.625 val loss: 0.693, r2: 0.461, rate:0.812, acc:0.750 lr 2.4609404191423283e-07\n",
      "epoch:16 step:3200 train loss: 1.191, r2: -0.507, rate:0.750, acc:0.375 val loss: 0.627, r2: 0.493, rate:0.812, acc:0.688 lr 2.4609404191423283e-07\n",
      "epoch:16 step:3300 train loss: 0.723, r2: -0.628, rate:0.688, acc:0.750 val loss: 0.607, r2: 0.238, rate:0.688, acc:0.750 lr 2.4609404191423283e-07\n",
      "epoch:16 step:3400 train loss: 0.817, r2: 0.560, rate:0.750, acc:0.688 val loss: 0.739, r2: 0.597, rate:0.812, acc:0.625 lr 2.4609404191423283e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:3500 train loss: 1.451, r2: -0.336, rate:0.812, acc:0.812 val loss: 0.854, r2: 0.111, rate:0.750, acc:0.750 lr 2.4609404191423283e-07\n",
      "epoch:16 step:3600 train loss: 0.805, r2: -5.073, rate:0.500, acc:0.625 val loss: 0.661, r2: 0.043, rate:0.688, acc:0.625 lr 2.4609404191423283e-07\n",
      "epoch:16 step:3700 train loss: 1.964, r2: 0.240, rate:0.875, acc:0.812 val loss: 2.160, r2: -2.434, rate:0.688, acc:0.375 lr 2.4609404191423283e-07\n",
      "epoch:16 step:3800 train loss: 0.971, r2: 0.661, rate:0.938, acc:0.938 val loss: 0.731, r2: 0.270, rate:0.625, acc:0.375 lr 2.4609404191423283e-07\n",
      "epoch:16 step:3900 train loss: 0.826, r2: -0.186, rate:0.750, acc:0.625 val loss: 0.765, r2: -0.168, rate:0.938, acc:0.750 lr 2.2148463772280956e-07\n",
      "epoch:16 step:4000 train loss: 1.190, r2: -0.462, rate:0.812, acc:0.750 val loss: 0.630, r2: 0.163, rate:0.812, acc:0.625 lr 2.2148463772280956e-07\n",
      "epoch:16 step:4100 train loss: 0.626, r2: -0.288, rate:0.812, acc:0.750 val loss: 1.304, r2: 0.266, rate:0.688, acc:0.688 lr 2.2148463772280956e-07\n",
      "epoch:16 step:4200 train loss: 0.917, r2: -0.060, rate:0.625, acc:0.250 val loss: 0.686, r2: 0.034, rate:0.812, acc:0.562 lr 2.2148463772280956e-07\n",
      "epoch:16 step:4300 train loss: 0.763, r2: 0.193, rate:0.625, acc:0.812 val loss: 1.134, r2: -0.701, rate:0.688, acc:0.500 lr 2.2148463772280956e-07\n",
      "epoch:16 step:4400 train loss: 1.600, r2: -0.374, rate:0.812, acc:0.562 val loss: 0.573, r2: -0.462, rate:0.812, acc:0.938 lr 2.2148463772280956e-07\n",
      "epoch:16 step:4500 train loss: 0.729, r2: -0.230, rate:0.875, acc:0.438 val loss: 0.893, r2: -0.262, rate:0.750, acc:0.500 lr 2.2148463772280956e-07\n",
      "epoch:16 step:4600 train loss: 1.049, r2: 0.199, rate:0.562, acc:0.812 val loss: 1.148, r2: 0.462, rate:0.812, acc:0.625 lr 2.2148463772280956e-07\n",
      "epoch:16 step:4700 train loss: 0.824, r2: 0.076, rate:0.750, acc:0.750 val loss: 0.864, r2: 0.374, rate:0.750, acc:0.562 lr 2.2148463772280956e-07\n",
      "epoch:16 step:4800 train loss: 1.514, r2: -0.950, rate:0.625, acc:0.875 val loss: 0.776, r2: -0.005, rate:0.562, acc:0.625 lr 2.2148463772280956e-07\n",
      "epoch:16 step:4900 train loss: 1.251, r2: 0.034, rate:0.812, acc:0.875 val loss: 0.790, r2: 0.410, rate:0.625, acc:0.438 lr 2.2148463772280956e-07\n",
      "epoch:16 step:5000 train loss: 0.703, r2: 0.158, rate:0.875, acc:0.750 val loss: 1.200, r2: 0.226, rate:0.562, acc:0.562 lr 1.9933617395052862e-07\n",
      "epoch:16 step:5100 train loss: 0.992, r2: 0.658, rate:0.625, acc:0.500 val loss: 1.133, r2: -0.377, rate:0.500, acc:0.625 lr 1.9933617395052862e-07\n",
      "epoch:16 step:5200 train loss: 0.617, r2: 0.557, rate:0.750, acc:0.750 val loss: 1.363, r2: 0.439, rate:0.562, acc:0.688 lr 1.9933617395052862e-07\n",
      "epoch:16 step:5300 train loss: 2.370, r2: -0.589, rate:1.000, acc:0.812 val loss: 1.234, r2: -1.774, rate:0.562, acc:0.875 lr 1.9933617395052862e-07\n",
      "epoch:16 step:5400 train loss: 0.594, r2: 0.654, rate:0.812, acc:0.625 val loss: 2.223, r2: 0.091, rate:0.375, acc:0.438 lr 1.9933617395052862e-07\n",
      "epoch:16 step:5500 train loss: 1.653, r2: -2.737, rate:0.438, acc:0.438 val loss: 1.229, r2: -0.790, rate:0.688, acc:0.625 lr 1.9933617395052862e-07\n",
      "epoch:16 step:5600 train loss: 1.538, r2: 0.452, rate:0.562, acc:0.625 val loss: 0.612, r2: 0.813, rate:0.812, acc:0.750 lr 1.9933617395052862e-07\n",
      "epoch:16 step:5700 train loss: 0.561, r2: 0.724, rate:0.875, acc:0.688 val loss: 0.803, r2: 0.287, rate:0.812, acc:0.688 lr 1.9933617395052862e-07\n",
      "epoch:16 step:5800 train loss: 0.862, r2: 0.311, rate:0.688, acc:0.625 val loss: 0.680, r2: 0.251, rate:0.812, acc:0.438 lr 1.9933617395052862e-07\n",
      "epoch:16 step:5900 train loss: 0.825, r2: 0.053, rate:0.500, acc:0.625 val loss: 1.524, r2: -0.302, rate:0.625, acc:0.312 lr 1.9933617395052862e-07\n",
      "epoch:16 step:6000 train loss: 0.979, r2: -1.977, rate:0.312, acc:0.125 val loss: 0.651, r2: -0.050, rate:0.750, acc:0.688 lr 1.9933617395052862e-07\n",
      "epoch:16 step:6100 train loss: 0.621, r2: 0.343, rate:0.688, acc:0.812 val loss: 1.356, r2: -1.135, rate:0.438, acc:0.812 lr 1.7940255655547575e-07\n",
      "epoch:16 step:6200 train loss: 1.154, r2: -0.244, rate:0.750, acc:0.625 val loss: 2.372, r2: -0.661, rate:0.688, acc:0.875 lr 1.7940255655547575e-07\n",
      "epoch:16 step:6300 train loss: 1.139, r2: -0.788, rate:0.500, acc:0.688 val loss: 0.926, r2: -0.815, rate:0.562, acc:0.750 lr 1.7940255655547575e-07\n",
      "epoch:16 step:6400 train loss: 1.006, r2: -0.775, rate:0.625, acc:0.438 val loss: 0.711, r2: 0.228, rate:0.688, acc:0.688 lr 1.7940255655547575e-07\n",
      "epoch:16 step:6500 train loss: 2.963, r2: -0.341, rate:0.500, acc:0.500 val loss: 2.706, r2: -1.540, rate:0.625, acc:0.688 lr 1.7940255655547575e-07\n",
      "epoch:16 step:6600 train loss: 0.382, r2: 0.393, rate:1.000, acc:1.000 val loss: 0.844, r2: -0.720, rate:0.812, acc:1.000 lr 1.7940255655547575e-07\n",
      "epoch:16 step:6700 train loss: 1.999, r2: -1.164, rate:0.625, acc:0.875 val loss: 0.961, r2: -0.273, rate:0.750, acc:0.562 lr 1.7940255655547575e-07\n",
      "epoch:16 step:6800 train loss: 0.641, r2: 0.503, rate:0.812, acc:0.625 val loss: 1.862, r2: -0.178, rate:0.562, acc:0.562 lr 1.7940255655547575e-07\n",
      "epoch:16 step:6900 train loss: 0.823, r2: 0.197, rate:0.750, acc:0.750 val loss: 1.094, r2: -0.855, rate:0.562, acc:0.875 lr 1.7940255655547575e-07\n",
      "epoch:16 step:7000 train loss: 0.985, r2: 0.400, rate:0.750, acc:1.000 val loss: 1.024, r2: -0.713, rate:0.625, acc:0.500 lr 1.7940255655547575e-07\n",
      "epoch:16 step:7100 train loss: 2.541, r2: -1.778, rate:0.500, acc:0.438 val loss: 0.790, r2: -0.854, rate:0.750, acc:0.688 lr 1.7940255655547575e-07\n",
      "epoch:16 step:7200 train loss: 0.634, r2: 0.144, rate:0.875, acc:0.812 val loss: 1.047, r2: -4.153, rate:0.562, acc:0.375 lr 1.6146230089992817e-07\n",
      "epoch:16 step:7300 train loss: 0.676, r2: -0.294, rate:0.625, acc:0.812 val loss: 1.869, r2: -6.304, rate:0.312, acc:0.562 lr 1.6146230089992817e-07\n",
      "epoch:16 step:7400 train loss: 1.111, r2: 0.158, rate:0.750, acc:0.938 val loss: 0.588, r2: 0.840, rate:0.812, acc:0.750 lr 1.6146230089992817e-07\n",
      "epoch:16 step:7500 train loss: 1.463, r2: -0.154, rate:0.625, acc:0.750 val loss: 0.894, r2: -1.002, rate:0.312, acc:0.625 lr 1.6146230089992817e-07\n",
      "epoch:16 step:7600 train loss: 1.358, r2: -2.258, rate:0.812, acc:1.000 val loss: 0.628, r2: 0.637, rate:1.000, acc:0.688 lr 1.6146230089992817e-07\n",
      "epoch:16 step:7700 train loss: 0.978, r2: 0.541, rate:0.812, acc:0.625 val loss: 0.808, r2: -0.135, rate:0.500, acc:0.750 lr 1.6146230089992817e-07\n",
      "epoch:16 step:7800 train loss: 0.721, r2: -0.239, rate:0.625, acc:0.812 val loss: 0.741, r2: 0.423, rate:0.688, acc:0.500 lr 1.6146230089992817e-07\n",
      "epoch:16 step:0.327 lr 0.000000\n",
      "epoch:17 step:0 train loss: 0.632, r2: 0.351, rate:0.750, acc:0.750 val loss: 0.999, r2: 0.415, rate:0.750, acc:0.812 lr 1.6146230089992817e-07\n",
      "epoch:17 step:100 train loss: 1.744, r2: -0.810, rate:0.500, acc:0.500 val loss: 1.816, r2: 0.044, rate:0.562, acc:0.750 lr 1.6146230089992817e-07\n",
      "epoch:17 step:200 train loss: 4.919, r2: -2.735, rate:0.875, acc:0.625 val loss: 0.788, r2: 0.248, rate:0.750, acc:0.562 lr 1.6146230089992817e-07\n",
      "epoch:17 step:300 train loss: 0.696, r2: -0.799, rate:0.500, acc:0.938 val loss: 1.531, r2: 0.192, rate:0.688, acc:0.688 lr 1.6146230089992817e-07\n",
      "epoch:17 step:400 train loss: 1.034, r2: 0.436, rate:0.688, acc:0.688 val loss: 0.775, r2: -0.629, rate:0.750, acc:0.938 lr 1.4531607080993536e-07\n",
      "epoch:17 step:500 train loss: 1.139, r2: -0.900, rate:0.812, acc:0.562 val loss: 0.725, r2: 0.088, rate:0.875, acc:0.562 lr 1.4531607080993536e-07\n",
      "epoch:17 step:600 train loss: 0.774, r2: 0.069, rate:0.875, acc:0.812 val loss: 1.438, r2: -0.777, rate:0.312, acc:0.438 lr 1.4531607080993536e-07\n",
      "epoch:17 step:700 train loss: 0.784, r2: -2.161, rate:0.562, acc:0.375 val loss: 0.872, r2: -0.735, rate:0.625, acc:0.875 lr 1.4531607080993536e-07\n",
      "epoch:17 step:800 train loss: 2.092, r2: -2.886, rate:0.438, acc:0.062 val loss: 0.703, r2: -0.746, rate:0.625, acc:0.625 lr 1.4531607080993536e-07\n",
      "epoch:17 step:900 train loss: 0.860, r2: -1.156, rate:0.562, acc:0.500 val loss: 0.795, r2: 0.260, rate:0.625, acc:0.562 lr 1.4531607080993536e-07\n",
      "epoch:17 step:1000 train loss: 0.697, r2: -1.652, rate:0.500, acc:0.500 val loss: 1.513, r2: -1.027, rate:0.688, acc:0.500 lr 1.4531607080993536e-07\n",
      "epoch:17 step:1100 train loss: 1.384, r2: 0.593, rate:0.688, acc:0.750 val loss: 0.771, r2: -0.221, rate:0.500, acc:0.812 lr 1.4531607080993536e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:1200 train loss: 0.906, r2: 0.041, rate:0.625, acc:0.688 val loss: 0.546, r2: 0.633, rate:0.875, acc:0.938 lr 1.4531607080993536e-07\n",
      "epoch:17 step:1300 train loss: 1.848, r2: -5.575, rate:0.938, acc:0.625 val loss: 0.912, r2: -0.780, rate:0.688, acc:0.750 lr 1.4531607080993536e-07\n",
      "epoch:17 step:1400 train loss: 0.915, r2: -1.051, rate:0.750, acc:0.875 val loss: 0.619, r2: 0.699, rate:0.875, acc:0.812 lr 1.4531607080993536e-07\n",
      "epoch:17 step:1500 train loss: 0.583, r2: 0.866, rate:0.812, acc:0.438 val loss: 0.938, r2: 0.494, rate:0.750, acc:0.688 lr 1.3078446372894184e-07\n",
      "epoch:17 step:1600 train loss: 0.674, r2: 0.317, rate:0.688, acc:0.312 val loss: 0.651, r2: 0.407, rate:0.938, acc:0.938 lr 1.3078446372894184e-07\n",
      "epoch:17 step:1700 train loss: 0.677, r2: 0.379, rate:0.750, acc:0.438 val loss: 1.333, r2: -0.488, rate:0.750, acc:0.562 lr 1.3078446372894184e-07\n",
      "epoch:17 step:1800 train loss: 0.912, r2: -1.714, rate:0.500, acc:0.875 val loss: 0.850, r2: 0.329, rate:0.688, acc:0.625 lr 1.3078446372894184e-07\n",
      "epoch:17 step:1900 train loss: 0.656, r2: 0.465, rate:0.750, acc:0.875 val loss: 0.661, r2: -1.387, rate:0.688, acc:0.812 lr 1.3078446372894184e-07\n",
      "epoch:17 step:2000 train loss: 0.596, r2: 0.303, rate:0.750, acc:0.750 val loss: 2.262, r2: -3.096, rate:0.625, acc:0.812 lr 1.3078446372894184e-07\n",
      "epoch:17 step:2100 train loss: 3.518, r2: -4.362, rate:0.750, acc:0.625 val loss: 0.800, r2: -2.769, rate:0.688, acc:0.812 lr 1.3078446372894184e-07\n",
      "epoch:17 step:2200 train loss: 0.807, r2: -0.254, rate:0.688, acc:0.500 val loss: 0.910, r2: -0.338, rate:0.688, acc:0.750 lr 1.3078446372894184e-07\n",
      "epoch:17 step:2300 train loss: 0.758, r2: -1.038, rate:0.625, acc:0.625 val loss: 0.869, r2: 0.418, rate:0.500, acc:0.750 lr 1.3078446372894184e-07\n",
      "epoch:17 step:2400 train loss: 0.990, r2: -2.466, rate:0.500, acc:0.688 val loss: 1.426, r2: 0.331, rate:0.750, acc:0.688 lr 1.3078446372894184e-07\n",
      "epoch:17 step:2500 train loss: 0.910, r2: -2.002, rate:0.500, acc:0.375 val loss: 0.771, r2: -2.549, rate:0.562, acc:0.500 lr 1.3078446372894184e-07\n",
      "epoch:17 step:2600 train loss: 0.637, r2: 0.682, rate:0.875, acc:0.500 val loss: 0.804, r2: 0.335, rate:0.562, acc:0.438 lr 1.1770601735604766e-07\n",
      "epoch:17 step:2700 train loss: 0.760, r2: -1.162, rate:0.625, acc:0.562 val loss: 0.653, r2: 0.265, rate:0.688, acc:0.688 lr 1.1770601735604766e-07\n",
      "epoch:17 step:2800 train loss: 2.553, r2: -0.087, rate:0.875, acc:0.500 val loss: 0.654, r2: -0.023, rate:0.625, acc:0.625 lr 1.1770601735604766e-07\n",
      "epoch:17 step:2900 train loss: 3.357, r2: -1.863, rate:0.312, acc:0.688 val loss: 2.213, r2: -0.034, rate:0.438, acc:0.438 lr 1.1770601735604766e-07\n",
      "epoch:17 step:3000 train loss: 0.984, r2: -1.140, rate:0.875, acc:0.938 val loss: 0.915, r2: 0.406, rate:0.750, acc:0.562 lr 1.1770601735604766e-07\n",
      "epoch:17 step:3100 train loss: 1.807, r2: -3.224, rate:0.875, acc:1.000 val loss: 0.660, r2: 0.657, rate:0.875, acc:0.812 lr 1.1770601735604766e-07\n",
      "epoch:17 step:3200 train loss: 1.165, r2: -0.881, rate:0.688, acc:0.750 val loss: 1.595, r2: -1.086, rate:0.750, acc:0.750 lr 1.1770601735604766e-07\n",
      "epoch:17 step:3300 train loss: 0.754, r2: -0.020, rate:0.688, acc:0.625 val loss: 1.045, r2: -1.059, rate:0.562, acc:0.438 lr 1.1770601735604766e-07\n",
      "epoch:17 step:3400 train loss: 4.667, r2: -4.381, rate:0.438, acc:0.500 val loss: 2.546, r2: -0.032, rate:0.562, acc:0.688 lr 1.1770601735604766e-07\n",
      "epoch:17 step:3500 train loss: 1.657, r2: -1.097, rate:0.625, acc:0.688 val loss: 0.978, r2: -0.266, rate:0.688, acc:0.688 lr 1.1770601735604766e-07\n",
      "epoch:17 step:3600 train loss: 1.343, r2: -1.612, rate:0.312, acc:0.250 val loss: 0.759, r2: 0.284, rate:0.625, acc:0.562 lr 1.1770601735604766e-07\n",
      "epoch:17 step:3700 train loss: 0.864, r2: 0.351, rate:0.625, acc:0.812 val loss: 0.966, r2: -5.201, rate:0.812, acc:0.500 lr 1.059354156204429e-07\n",
      "epoch:17 step:3800 train loss: 1.207, r2: -0.181, rate:0.750, acc:0.438 val loss: 1.060, r2: 0.475, rate:0.625, acc:0.688 lr 1.059354156204429e-07\n",
      "epoch:17 step:3900 train loss: 1.024, r2: 0.622, rate:0.875, acc:0.875 val loss: 0.836, r2: 0.303, rate:0.688, acc:0.625 lr 1.059354156204429e-07\n",
      "epoch:17 step:4000 train loss: 0.921, r2: -0.392, rate:0.812, acc:1.000 val loss: 0.933, r2: 0.433, rate:0.812, acc:0.562 lr 1.059354156204429e-07\n",
      "epoch:17 step:4100 train loss: 1.171, r2: -0.166, rate:0.625, acc:0.688 val loss: 0.885, r2: -0.386, rate:0.625, acc:0.625 lr 1.059354156204429e-07\n",
      "epoch:17 step:4200 train loss: 0.753, r2: 0.001, rate:0.625, acc:0.312 val loss: 0.695, r2: 0.608, rate:0.812, acc:0.875 lr 1.059354156204429e-07\n",
      "epoch:17 step:4300 train loss: 1.771, r2: -0.072, rate:0.562, acc:0.625 val loss: 1.090, r2: -0.505, rate:0.500, acc:0.500 lr 1.059354156204429e-07\n",
      "epoch:17 step:4400 train loss: 0.922, r2: -0.054, rate:0.500, acc:0.500 val loss: 0.964, r2: 0.218, rate:0.688, acc:0.688 lr 1.059354156204429e-07\n",
      "epoch:17 step:4500 train loss: 1.066, r2: 0.186, rate:0.688, acc:0.625 val loss: 0.791, r2: 0.001, rate:0.625, acc:0.625 lr 1.059354156204429e-07\n",
      "epoch:17 step:4600 train loss: 0.830, r2: 0.493, rate:0.625, acc:0.500 val loss: 0.567, r2: 0.341, rate:0.875, acc:0.812 lr 1.059354156204429e-07\n",
      "epoch:17 step:4700 train loss: 0.985, r2: -1.178, rate:0.812, acc:0.312 val loss: 1.477, r2: -1.058, rate:0.438, acc:0.312 lr 1.059354156204429e-07\n",
      "epoch:17 step:4800 train loss: 0.637, r2: 0.314, rate:0.812, acc:0.750 val loss: 0.796, r2: 0.234, rate:0.750, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:17 step:4900 train loss: 0.615, r2: 0.517, rate:0.938, acc:0.688 val loss: 0.745, r2: -0.294, rate:0.750, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:17 step:5000 train loss: 0.831, r2: 0.378, rate:0.625, acc:0.938 val loss: 0.920, r2: -0.002, rate:0.688, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:17 step:5100 train loss: 0.873, r2: -0.335, rate:0.438, acc:0.625 val loss: 0.701, r2: 0.428, rate:0.750, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:17 step:5200 train loss: 1.008, r2: -0.213, rate:0.438, acc:0.438 val loss: 0.913, r2: -0.215, rate:0.688, acc:0.375 lr 9.534187405839861e-08\n",
      "epoch:17 step:5300 train loss: 1.145, r2: -0.137, rate:0.750, acc:0.562 val loss: 0.921, r2: 0.318, rate:0.625, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:17 step:5400 train loss: 0.758, r2: 0.175, rate:0.750, acc:0.750 val loss: 0.891, r2: -4.110, rate:0.562, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:17 step:5500 train loss: 0.605, r2: -0.423, rate:0.688, acc:0.875 val loss: 0.766, r2: 0.038, rate:1.000, acc:1.000 lr 9.534187405839861e-08\n",
      "epoch:17 step:5600 train loss: 1.479, r2: -3.297, rate:1.000, acc:0.812 val loss: 0.848, r2: -2.133, rate:0.562, acc:0.250 lr 9.534187405839861e-08\n",
      "epoch:17 step:5700 train loss: 0.746, r2: -0.146, rate:0.812, acc:0.625 val loss: 3.120, r2: -0.159, rate:0.938, acc:1.000 lr 9.534187405839861e-08\n",
      "epoch:17 step:5800 train loss: 2.056, r2: -0.094, rate:0.625, acc:0.688 val loss: 0.969, r2: -2.868, rate:0.438, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:17 step:5900 train loss: 0.894, r2: -3.594, rate:0.875, acc:0.438 val loss: 1.103, r2: 0.241, rate:0.688, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:17 step:6000 train loss: 0.763, r2: -0.294, rate:0.750, acc:1.000 val loss: 0.631, r2: -0.092, rate:0.750, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:17 step:6100 train loss: 0.927, r2: 0.417, rate:0.562, acc:0.500 val loss: 1.683, r2: 0.255, rate:0.625, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:17 step:6200 train loss: 1.125, r2: 0.599, rate:0.812, acc:0.688 val loss: 1.186, r2: -1.657, rate:0.562, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:17 step:6300 train loss: 0.473, r2: 0.539, rate:0.938, acc:0.812 val loss: 0.758, r2: -0.428, rate:0.562, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:17 step:6400 train loss: 0.772, r2: 0.302, rate:0.812, acc:0.750 val loss: 0.620, r2: 0.747, rate:0.812, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:17 step:6500 train loss: 2.778, r2: -16.324, rate:0.688, acc:1.000 val loss: 0.607, r2: 0.524, rate:0.875, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:17 step:6600 train loss: 1.671, r2: -1.503, rate:0.625, acc:0.500 val loss: 0.589, r2: 0.671, rate:0.750, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:17 step:6700 train loss: 0.684, r2: 0.499, rate:0.812, acc:0.562 val loss: 3.608, r2: 0.132, rate:0.875, acc:0.938 lr 9.534187405839861e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:6800 train loss: 0.673, r2: 0.331, rate:0.688, acc:0.688 val loss: 0.877, r2: 0.556, rate:0.875, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:17 step:6900 train loss: 0.916, r2: -1.950, rate:0.625, acc:0.875 val loss: 1.514, r2: -0.492, rate:0.438, acc:0.312 lr 9.534187405839861e-08\n",
      "epoch:17 step:7000 train loss: 1.068, r2: -2.574, rate:0.562, acc:0.375 val loss: 0.784, r2: -0.065, rate:0.875, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:17 step:7100 train loss: 1.057, r2: -0.922, rate:0.625, acc:0.688 val loss: 0.982, r2: 0.518, rate:0.750, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:17 step:7200 train loss: 0.767, r2: 0.148, rate:0.812, acc:0.375 val loss: 0.914, r2: -0.134, rate:0.625, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:17 step:7300 train loss: 0.741, r2: -0.747, rate:0.750, acc:0.562 val loss: 0.700, r2: 0.261, rate:0.750, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:17 step:7400 train loss: 1.069, r2: -0.642, rate:0.500, acc:0.812 val loss: 0.968, r2: 0.565, rate:0.625, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:17 step:7500 train loss: 0.428, r2: 0.542, rate:1.000, acc:0.938 val loss: 0.673, r2: 0.677, rate:0.750, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:17 step:7600 train loss: 1.355, r2: -0.049, rate:0.438, acc:0.188 val loss: 1.082, r2: -0.710, rate:0.562, acc:0.375 lr 9.534187405839861e-08\n",
      "epoch:17 step:7700 train loss: 1.715, r2: -0.675, rate:0.438, acc:0.375 val loss: 0.776, r2: -0.565, rate:0.500, acc:0.375 lr 9.534187405839861e-08\n",
      "epoch:17 step:7800 train loss: 0.685, r2: 0.365, rate:0.562, acc:0.625 val loss: 1.190, r2: 0.284, rate:0.562, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:17 step:0.327 lr 0.000000\n",
      "epoch:18 step:0 train loss: 0.493, r2: 0.783, rate:0.875, acc:0.812 val loss: 1.585, r2: -4.178, rate:0.875, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:18 step:100 train loss: 0.829, r2: -0.335, rate:0.625, acc:0.812 val loss: 0.891, r2: 0.049, rate:0.625, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:18 step:200 train loss: 2.252, r2: -4.006, rate:0.375, acc:0.375 val loss: 1.301, r2: 0.439, rate:0.750, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:18 step:300 train loss: 1.194, r2: -1.362, rate:0.500, acc:0.688 val loss: 1.848, r2: -0.023, rate:0.688, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:18 step:400 train loss: 0.900, r2: -0.342, rate:0.750, acc:0.938 val loss: 1.107, r2: -0.294, rate:0.500, acc:0.375 lr 9.534187405839861e-08\n",
      "epoch:18 step:500 train loss: 1.019, r2: 0.332, rate:0.562, acc:0.750 val loss: 1.034, r2: -9.305, rate:0.500, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:18 step:600 train loss: 0.821, r2: -0.064, rate:0.562, acc:0.688 val loss: 1.865, r2: -0.569, rate:0.625, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:18 step:700 train loss: 0.785, r2: 0.615, rate:0.562, acc:0.812 val loss: 0.975, r2: 0.481, rate:0.750, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:18 step:800 train loss: 1.272, r2: 0.103, rate:0.438, acc:0.312 val loss: 1.154, r2: 0.039, rate:0.812, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:18 step:900 train loss: 0.889, r2: -0.588, rate:0.812, acc:0.688 val loss: 0.972, r2: 0.145, rate:0.625, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:18 step:1000 train loss: 0.687, r2: 0.639, rate:0.750, acc:0.812 val loss: 1.178, r2: 0.353, rate:0.750, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:18 step:1100 train loss: 0.772, r2: 0.442, rate:0.812, acc:0.625 val loss: 0.652, r2: 0.480, rate:0.875, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:18 step:1200 train loss: 1.330, r2: -1.019, rate:1.000, acc:0.938 val loss: 0.534, r2: 0.766, rate:0.875, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:18 step:1300 train loss: 0.741, r2: -2.153, rate:0.625, acc:0.062 val loss: 0.889, r2: -1.763, rate:0.688, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:18 step:1400 train loss: 0.573, r2: 0.572, rate:0.875, acc:0.812 val loss: 0.608, r2: 0.661, rate:0.938, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:18 step:1500 train loss: 0.765, r2: 0.223, rate:0.688, acc:0.312 val loss: 0.970, r2: 0.407, rate:0.750, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:18 step:1600 train loss: 0.738, r2: 0.492, rate:0.688, acc:0.312 val loss: 0.843, r2: 0.044, rate:0.812, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:18 step:1700 train loss: 1.129, r2: 0.023, rate:0.625, acc:0.625 val loss: 0.865, r2: -3.913, rate:0.688, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:18 step:1800 train loss: 0.682, r2: 0.188, rate:0.688, acc:0.562 val loss: 0.788, r2: -3.827, rate:0.750, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:18 step:1900 train loss: 0.866, r2: -0.189, rate:0.562, acc:0.562 val loss: 0.936, r2: -1.571, rate:0.812, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:18 step:2000 train loss: 0.493, r2: -0.433, rate:0.938, acc:1.000 val loss: 0.861, r2: 0.149, rate:0.812, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:18 step:2100 train loss: 1.111, r2: -1.811, rate:0.812, acc:0.375 val loss: 0.648, r2: 0.492, rate:0.688, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:18 step:2200 train loss: 0.729, r2: 0.715, rate:1.000, acc:0.750 val loss: 1.647, r2: -0.108, rate:0.562, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:18 step:2300 train loss: 0.649, r2: 0.330, rate:0.750, acc:0.688 val loss: 0.700, r2: 0.496, rate:0.875, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:18 step:2400 train loss: 0.637, r2: 0.011, rate:0.938, acc:0.875 val loss: 0.729, r2: -2.527, rate:0.688, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:18 step:2500 train loss: 0.792, r2: 0.817, rate:0.625, acc:0.625 val loss: 0.711, r2: -4.416, rate:0.625, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:18 step:2600 train loss: 1.417, r2: -0.137, rate:0.500, acc:0.500 val loss: 0.824, r2: -0.251, rate:0.625, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:18 step:2700 train loss: 0.985, r2: -0.009, rate:0.875, acc:1.000 val loss: 0.776, r2: 0.322, rate:0.625, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:18 step:2800 train loss: 0.608, r2: 0.467, rate:0.750, acc:0.688 val loss: 1.307, r2: 0.270, rate:0.875, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:18 step:2900 train loss: 0.784, r2: 0.738, rate:0.750, acc:0.750 val loss: 1.568, r2: -0.579, rate:0.625, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:18 step:3000 train loss: 0.667, r2: 0.267, rate:0.688, acc:0.750 val loss: 1.158, r2: 0.356, rate:0.812, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:18 step:3100 train loss: 1.445, r2: -1.997, rate:0.375, acc:0.500 val loss: 1.321, r2: -1.243, rate:0.375, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:18 step:3200 train loss: 0.618, r2: 0.433, rate:0.812, acc:0.750 val loss: 0.625, r2: -1.789, rate:0.875, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:18 step:3300 train loss: 0.628, r2: 0.753, rate:0.750, acc:0.562 val loss: 1.170, r2: -1.471, rate:0.500, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:18 step:3400 train loss: 1.017, r2: -2.059, rate:0.375, acc:0.625 val loss: 0.791, r2: -1.234, rate:0.812, acc:1.000 lr 9.534187405839861e-08\n",
      "epoch:18 step:3500 train loss: 0.687, r2: 0.065, rate:0.812, acc:0.438 val loss: 0.882, r2: 0.386, rate:0.812, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:18 step:3600 train loss: 0.745, r2: 0.090, rate:0.625, acc:0.500 val loss: 1.128, r2: -0.465, rate:0.750, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:18 step:3700 train loss: 2.273, r2: 0.196, rate:0.625, acc:0.688 val loss: 0.949, r2: -0.236, rate:0.812, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:18 step:3800 train loss: 0.719, r2: 0.694, rate:0.812, acc:0.688 val loss: 0.879, r2: 0.433, rate:0.750, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:18 step:3900 train loss: 0.551, r2: -0.652, rate:0.875, acc:0.875 val loss: 2.008, r2: -0.009, rate:0.688, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:18 step:4000 train loss: 0.864, r2: 0.198, rate:0.875, acc:0.750 val loss: 0.979, r2: -0.988, rate:0.750, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:18 step:4100 train loss: 0.952, r2: 0.346, rate:0.812, acc:0.812 val loss: 0.767, r2: 0.269, rate:0.625, acc:0.375 lr 9.534187405839861e-08\n",
      "epoch:18 step:4200 train loss: 0.717, r2: -0.875, rate:0.438, acc:0.375 val loss: 0.623, r2: 0.514, rate:0.875, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:18 step:4300 train loss: 0.692, r2: 0.013, rate:0.688, acc:0.562 val loss: 0.692, r2: 0.405, rate:0.875, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:18 step:4400 train loss: 0.757, r2: 0.156, rate:0.812, acc:0.562 val loss: 0.987, r2: 0.231, rate:0.625, acc:0.812 lr 9.534187405839861e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:4500 train loss: 0.680, r2: -0.174, rate:0.688, acc:0.625 val loss: 0.663, r2: 0.464, rate:1.000, acc:1.000 lr 9.534187405839861e-08\n",
      "epoch:18 step:4600 train loss: 0.842, r2: -0.054, rate:0.625, acc:0.688 val loss: 1.747, r2: 0.324, rate:0.500, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:18 step:4700 train loss: 1.988, r2: -1.324, rate:0.625, acc:1.000 val loss: 0.710, r2: 0.039, rate:0.500, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:18 step:4800 train loss: 0.585, r2: 0.768, rate:0.875, acc:0.625 val loss: 0.852, r2: 0.088, rate:0.688, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:18 step:4900 train loss: 0.977, r2: 0.575, rate:0.875, acc:0.938 val loss: 0.907, r2: -3.523, rate:0.375, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:18 step:5000 train loss: 1.198, r2: -0.098, rate:0.500, acc:0.562 val loss: 0.933, r2: 0.060, rate:0.688, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:18 step:5100 train loss: 1.068, r2: -5.534, rate:0.375, acc:0.250 val loss: 2.187, r2: 0.004, rate:0.562, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:18 step:5200 train loss: 1.007, r2: -1.437, rate:0.875, acc:0.188 val loss: 1.328, r2: -1.436, rate:0.562, acc:0.375 lr 9.534187405839861e-08\n",
      "epoch:18 step:5300 train loss: 0.817, r2: -0.393, rate:0.625, acc:0.812 val loss: 1.394, r2: 0.080, rate:0.562, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:18 step:5400 train loss: 0.707, r2: 0.228, rate:0.688, acc:0.625 val loss: 0.656, r2: -0.088, rate:0.812, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:18 step:5500 train loss: 1.334, r2: -0.226, rate:0.625, acc:0.688 val loss: 0.814, r2: 0.586, rate:0.750, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:18 step:5600 train loss: 0.970, r2: 0.046, rate:0.438, acc:0.750 val loss: 1.446, r2: -0.204, rate:0.562, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:18 step:5700 train loss: 1.139, r2: -0.180, rate:0.750, acc:0.438 val loss: 1.347, r2: -0.116, rate:0.375, acc:0.375 lr 9.534187405839861e-08\n",
      "epoch:18 step:5800 train loss: 1.138, r2: -1.472, rate:0.500, acc:0.312 val loss: 0.663, r2: -7.243, rate:0.812, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:18 step:5900 train loss: 1.772, r2: -0.679, rate:0.625, acc:0.625 val loss: 1.050, r2: -0.014, rate:0.500, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:18 step:6000 train loss: 0.606, r2: -2.253, rate:0.875, acc:0.938 val loss: 0.685, r2: -1.122, rate:0.750, acc:0.312 lr 9.534187405839861e-08\n",
      "epoch:18 step:6100 train loss: 0.701, r2: 0.399, rate:0.625, acc:0.562 val loss: 0.796, r2: 0.445, rate:0.812, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:18 step:6200 train loss: 1.435, r2: 0.378, rate:0.812, acc:0.875 val loss: 1.398, r2: -0.154, rate:0.875, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:18 step:6300 train loss: 1.060, r2: -1.639, rate:0.500, acc:0.875 val loss: 0.820, r2: 0.178, rate:0.750, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:18 step:6400 train loss: 0.751, r2: 0.679, rate:0.688, acc:0.688 val loss: 0.890, r2: 0.307, rate:0.750, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:18 step:6500 train loss: 0.839, r2: 0.534, rate:0.688, acc:0.625 val loss: 0.565, r2: -0.463, rate:0.812, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:18 step:6600 train loss: 0.873, r2: 0.394, rate:0.500, acc:0.812 val loss: 1.119, r2: 0.295, rate:0.625, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:18 step:6700 train loss: 1.154, r2: 0.153, rate:0.375, acc:0.688 val loss: 0.910, r2: 0.298, rate:0.812, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:18 step:6800 train loss: 0.742, r2: 0.003, rate:0.750, acc:0.625 val loss: 2.998, r2: -1.129, rate:0.562, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:18 step:6900 train loss: 0.550, r2: 0.621, rate:0.750, acc:0.812 val loss: 0.814, r2: 0.091, rate:0.500, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:18 step:7000 train loss: 1.775, r2: -5.569, rate:0.500, acc:0.938 val loss: 2.174, r2: -0.215, rate:0.625, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:18 step:7100 train loss: 1.347, r2: 0.236, rate:0.438, acc:0.375 val loss: 0.982, r2: 0.453, rate:0.750, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:18 step:7200 train loss: 0.827, r2: -0.182, rate:0.688, acc:0.688 val loss: 1.129, r2: 0.237, rate:0.625, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:18 step:7300 train loss: 1.060, r2: -1.559, rate:0.188, acc:0.562 val loss: 0.766, r2: 0.410, rate:0.750, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:18 step:7400 train loss: 1.139, r2: 0.457, rate:0.875, acc:0.500 val loss: 0.530, r2: 0.591, rate:0.875, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:18 step:7500 train loss: 3.227, r2: -0.643, rate:0.688, acc:0.812 val loss: 0.602, r2: 0.019, rate:0.875, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:18 step:7600 train loss: 0.774, r2: -0.053, rate:0.812, acc:0.875 val loss: 0.839, r2: -0.742, rate:0.562, acc:0.375 lr 9.534187405839861e-08\n",
      "epoch:18 step:7700 train loss: 1.137, r2: -0.139, rate:0.688, acc:0.500 val loss: 1.123, r2: 0.298, rate:0.750, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:18 step:7800 train loss: 0.692, r2: 0.070, rate:0.562, acc:0.875 val loss: 0.714, r2: -0.051, rate:0.812, acc:1.000 lr 9.534187405839861e-08\n",
      "epoch:18 step:0.327 lr 0.000000\n",
      "epoch:19 step:0 train loss: 0.897, r2: -2.990, rate:0.375, acc:0.750 val loss: 1.053, r2: 0.466, rate:0.750, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:19 step:100 train loss: 1.051, r2: -1.336, rate:0.438, acc:0.312 val loss: 0.652, r2: -0.555, rate:0.812, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:19 step:200 train loss: 0.794, r2: 0.588, rate:0.625, acc:0.625 val loss: 0.907, r2: -3.746, rate:0.562, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:19 step:300 train loss: 5.122, r2: -3.237, rate:0.625, acc:0.688 val loss: 1.357, r2: -0.327, rate:0.500, acc:0.312 lr 9.534187405839861e-08\n",
      "epoch:19 step:400 train loss: 0.927, r2: -2.045, rate:0.625, acc:0.688 val loss: 0.635, r2: 0.433, rate:0.938, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:19 step:500 train loss: 2.011, r2: -2.151, rate:0.812, acc:0.438 val loss: 1.396, r2: 0.333, rate:0.688, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:19 step:600 train loss: 0.932, r2: -0.754, rate:0.688, acc:0.438 val loss: 0.702, r2: -0.464, rate:0.500, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:19 step:700 train loss: 1.036, r2: -0.356, rate:0.375, acc:0.312 val loss: 0.616, r2: 0.464, rate:0.688, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:19 step:800 train loss: 0.537, r2: -0.427, rate:0.875, acc:0.812 val loss: 0.562, r2: 0.050, rate:0.875, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:19 step:900 train loss: 0.923, r2: -1.495, rate:0.625, acc:0.938 val loss: 0.545, r2: -0.278, rate:0.875, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:19 step:1000 train loss: 1.029, r2: -0.325, rate:0.875, acc:0.750 val loss: 1.064, r2: 0.034, rate:0.688, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:19 step:1100 train loss: 0.804, r2: 0.217, rate:0.688, acc:0.688 val loss: 0.674, r2: 0.736, rate:0.750, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:19 step:1200 train loss: 0.938, r2: -0.090, rate:0.688, acc:0.812 val loss: 0.716, r2: 0.409, rate:0.688, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:19 step:1300 train loss: 0.691, r2: -0.352, rate:0.750, acc:0.625 val loss: 0.677, r2: 0.483, rate:0.688, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:19 step:1400 train loss: 1.222, r2: -1.254, rate:0.750, acc:0.938 val loss: 0.512, r2: 0.772, rate:0.750, acc:1.000 lr 9.534187405839861e-08\n",
      "epoch:19 step:1500 train loss: 0.863, r2: -1.735, rate:0.875, acc:1.000 val loss: 0.882, r2: -0.669, rate:0.812, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:19 step:1600 train loss: 0.863, r2: -2.818, rate:0.875, acc:0.688 val loss: 1.663, r2: -0.943, rate:0.688, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:19 step:1700 train loss: 0.503, r2: 0.555, rate:1.000, acc:1.000 val loss: 1.335, r2: -0.357, rate:0.438, acc:0.188 lr 9.534187405839861e-08\n",
      "epoch:19 step:1800 train loss: 0.829, r2: -1.422, rate:0.438, acc:0.500 val loss: 1.180, r2: -0.119, rate:0.562, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:19 step:1900 train loss: 0.729, r2: 0.636, rate:0.562, acc:0.688 val loss: 1.064, r2: -0.471, rate:0.688, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:19 step:2000 train loss: 1.157, r2: 0.430, rate:0.812, acc:0.750 val loss: 1.190, r2: -0.728, rate:0.500, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:19 step:2100 train loss: 0.621, r2: 0.056, rate:0.875, acc:0.938 val loss: 0.911, r2: -1.003, rate:0.562, acc:0.500 lr 9.534187405839861e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:2200 train loss: 1.259, r2: -1.768, rate:0.500, acc:0.188 val loss: 0.632, r2: 0.661, rate:0.750, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:19 step:2300 train loss: 1.183, r2: 0.064, rate:0.562, acc:0.375 val loss: 1.623, r2: -0.459, rate:0.750, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:19 step:2400 train loss: 0.927, r2: -1.752, rate:0.625, acc:0.250 val loss: 0.582, r2: 0.145, rate:0.812, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:19 step:2500 train loss: 1.133, r2: -0.164, rate:0.438, acc:0.312 val loss: 1.499, r2: -6.327, rate:0.625, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:19 step:2600 train loss: 0.874, r2: 0.398, rate:0.625, acc:0.625 val loss: 1.463, r2: -0.367, rate:0.688, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:19 step:2700 train loss: 0.968, r2: 0.303, rate:0.812, acc:0.812 val loss: 1.012, r2: 0.177, rate:0.750, acc:0.375 lr 9.534187405839861e-08\n",
      "epoch:19 step:2800 train loss: 3.500, r2: -1.162, rate:0.375, acc:0.688 val loss: 0.721, r2: 0.183, rate:0.688, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:19 step:2900 train loss: 1.038, r2: 0.220, rate:0.938, acc:0.500 val loss: 0.459, r2: 0.814, rate:0.938, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:19 step:3000 train loss: 0.841, r2: -1.227, rate:0.438, acc:0.812 val loss: 0.943, r2: 0.320, rate:0.812, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:19 step:3100 train loss: 0.688, r2: -0.308, rate:0.750, acc:0.875 val loss: 1.115, r2: -2.854, rate:0.438, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:19 step:3200 train loss: 1.386, r2: 0.094, rate:0.438, acc:0.625 val loss: 1.521, r2: -1.217, rate:0.562, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:19 step:3300 train loss: 0.662, r2: -0.057, rate:0.750, acc:0.562 val loss: 1.250, r2: -0.032, rate:0.688, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:19 step:3400 train loss: 0.678, r2: -0.720, rate:0.562, acc:0.500 val loss: 0.630, r2: 0.718, rate:0.938, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:19 step:3500 train loss: 1.146, r2: -3.099, rate:0.562, acc:0.250 val loss: 1.063, r2: 0.210, rate:0.625, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:19 step:3600 train loss: 1.053, r2: -1.103, rate:0.250, acc:0.812 val loss: 1.876, r2: -0.005, rate:0.562, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:19 step:3700 train loss: 1.909, r2: -7.985, rate:0.750, acc:1.000 val loss: 0.677, r2: 0.334, rate:0.750, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:19 step:3800 train loss: 0.968, r2: 0.266, rate:0.625, acc:0.500 val loss: 1.327, r2: 0.468, rate:1.000, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:19 step:3900 train loss: 1.027, r2: -0.112, rate:0.688, acc:0.438 val loss: 0.941, r2: -0.175, rate:0.500, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:19 step:4000 train loss: 0.974, r2: 0.090, rate:0.938, acc:0.750 val loss: 1.301, r2: -0.151, rate:0.750, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:19 step:4100 train loss: 0.780, r2: 0.228, rate:0.625, acc:0.562 val loss: 0.845, r2: -0.153, rate:0.438, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:19 step:4200 train loss: 0.689, r2: -1.619, rate:0.688, acc:0.500 val loss: 2.100, r2: 0.225, rate:0.625, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:19 step:4300 train loss: 1.099, r2: 0.350, rate:0.562, acc:0.625 val loss: 0.822, r2: 0.282, rate:0.625, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:19 step:4400 train loss: 0.910, r2: -1.102, rate:0.750, acc:0.938 val loss: 0.652, r2: 0.103, rate:0.750, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:19 step:4500 train loss: 0.618, r2: 0.218, rate:0.562, acc:0.750 val loss: 1.284, r2: -0.339, rate:0.688, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:19 step:4600 train loss: 1.038, r2: -0.529, rate:0.812, acc:0.812 val loss: 0.761, r2: -0.961, rate:0.688, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:19 step:4700 train loss: 1.401, r2: -0.356, rate:0.688, acc:0.812 val loss: 0.730, r2: -1.378, rate:0.625, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:19 step:4800 train loss: 0.843, r2: 0.022, rate:0.688, acc:0.562 val loss: 2.369, r2: -0.302, rate:0.438, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:19 step:4900 train loss: 0.911, r2: 0.384, rate:0.750, acc:0.562 val loss: 0.683, r2: -19.442, rate:0.750, acc:0.188 lr 9.534187405839861e-08\n",
      "epoch:19 step:5000 train loss: 0.806, r2: -1.202, rate:0.625, acc:0.375 val loss: 0.721, r2: 0.350, rate:0.750, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:19 step:5100 train loss: 0.911, r2: -0.106, rate:0.688, acc:0.750 val loss: 0.651, r2: 0.139, rate:1.000, acc:1.000 lr 9.534187405839861e-08\n",
      "epoch:19 step:5200 train loss: 0.754, r2: -0.030, rate:0.500, acc:0.688 val loss: 0.814, r2: 0.495, rate:0.625, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:19 step:5300 train loss: 0.818, r2: 0.703, rate:1.000, acc:1.000 val loss: 0.589, r2: -0.045, rate:0.750, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:19 step:5400 train loss: 0.733, r2: -0.545, rate:0.750, acc:0.562 val loss: 2.058, r2: -0.032, rate:0.562, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:19 step:5500 train loss: 1.065, r2: -0.875, rate:0.750, acc:0.875 val loss: 1.438, r2: -0.075, rate:0.438, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:19 step:5600 train loss: 1.245, r2: -0.558, rate:0.938, acc:1.000 val loss: 0.889, r2: 0.003, rate:0.750, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:19 step:5700 train loss: 2.025, r2: -0.010, rate:0.625, acc:0.500 val loss: 1.059, r2: 0.177, rate:0.688, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:19 step:5800 train loss: 1.142, r2: -0.051, rate:0.562, acc:0.438 val loss: 0.708, r2: 0.511, rate:0.750, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:19 step:5900 train loss: 0.984, r2: -2.361, rate:0.688, acc:0.812 val loss: 1.174, r2: -0.122, rate:0.688, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:19 step:6000 train loss: 0.753, r2: 0.538, rate:0.688, acc:0.688 val loss: 1.082, r2: -0.134, rate:0.688, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:19 step:6100 train loss: 1.096, r2: -0.320, rate:0.625, acc:0.375 val loss: 1.041, r2: -0.617, rate:0.688, acc:0.375 lr 9.534187405839861e-08\n",
      "epoch:19 step:6200 train loss: 1.254, r2: -1.325, rate:0.875, acc:0.875 val loss: 1.417, r2: -0.103, rate:0.500, acc:0.312 lr 9.534187405839861e-08\n",
      "epoch:19 step:6300 train loss: 0.652, r2: 0.102, rate:0.750, acc:0.688 val loss: 0.712, r2: 0.183, rate:0.750, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:19 step:6400 train loss: 1.245, r2: 0.304, rate:0.750, acc:0.938 val loss: 1.011, r2: -0.273, rate:0.500, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:19 step:6500 train loss: 0.759, r2: 0.680, rate:0.812, acc:0.750 val loss: 0.994, r2: -0.251, rate:0.500, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:19 step:6600 train loss: 0.779, r2: 0.143, rate:0.562, acc:0.500 val loss: 1.227, r2: 0.000, rate:0.562, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:19 step:6700 train loss: 1.264, r2: -1.002, rate:0.875, acc:0.688 val loss: 0.953, r2: 0.182, rate:0.625, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:19 step:6800 train loss: 0.643, r2: 0.516, rate:0.938, acc:0.938 val loss: 1.488, r2: -0.780, rate:0.625, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:19 step:6900 train loss: 1.356, r2: -0.406, rate:0.688, acc:0.188 val loss: 0.813, r2: 0.433, rate:0.625, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:19 step:7000 train loss: 0.826, r2: 0.162, rate:0.688, acc:0.438 val loss: 0.849, r2: -0.059, rate:0.562, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:19 step:7100 train loss: 0.698, r2: 0.156, rate:0.875, acc:0.812 val loss: 1.550, r2: -0.208, rate:0.938, acc:1.000 lr 9.534187405839861e-08\n",
      "epoch:19 step:7200 train loss: 0.650, r2: 0.330, rate:0.875, acc:0.812 val loss: 0.826, r2: -0.179, rate:0.562, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:19 step:7300 train loss: 1.982, r2: -1.891, rate:0.562, acc:0.312 val loss: 2.188, r2: 0.297, rate:0.688, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:19 step:7400 train loss: 0.838, r2: -0.404, rate:0.562, acc:0.812 val loss: 1.165, r2: -1.984, rate:0.688, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:19 step:7500 train loss: 0.659, r2: 0.525, rate:0.812, acc:0.562 val loss: 1.703, r2: -1.778, rate:0.438, acc:0.125 lr 9.534187405839861e-08\n",
      "epoch:19 step:7600 train loss: 1.288, r2: -3.311, rate:0.625, acc:0.938 val loss: 0.626, r2: 0.008, rate:0.625, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:19 step:7700 train loss: 1.129, r2: 0.574, rate:0.375, acc:0.750 val loss: 1.269, r2: -0.197, rate:0.500, acc:0.688 lr 9.534187405839861e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:7800 train loss: 0.746, r2: -0.780, rate:0.812, acc:0.625 val loss: 0.649, r2: 0.570, rate:0.812, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:19 step:0.327 lr 0.000000\n",
      "epoch:20 step:0 train loss: 1.192, r2: -3.220, rate:0.688, acc:0.875 val loss: 0.784, r2: 0.188, rate:0.500, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:20 step:100 train loss: 0.917, r2: -3.750, rate:0.500, acc:0.375 val loss: 0.860, r2: 0.365, rate:0.750, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:20 step:200 train loss: 1.336, r2: -0.300, rate:0.625, acc:0.500 val loss: 0.662, r2: -0.474, rate:0.875, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:20 step:300 train loss: 0.795, r2: 0.065, rate:0.562, acc:0.625 val loss: 0.773, r2: 0.597, rate:0.812, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:20 step:400 train loss: 1.048, r2: -0.283, rate:0.562, acc:0.562 val loss: 1.749, r2: -0.633, rate:0.500, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:20 step:500 train loss: 1.010, r2: -0.199, rate:0.625, acc:0.688 val loss: 0.670, r2: 0.336, rate:0.875, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:20 step:600 train loss: 0.810, r2: 0.409, rate:0.625, acc:0.562 val loss: 0.628, r2: 0.516, rate:0.812, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:20 step:700 train loss: 0.775, r2: -1.599, rate:0.625, acc:0.812 val loss: 1.331, r2: 0.229, rate:0.875, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:20 step:800 train loss: 0.576, r2: 0.781, rate:0.938, acc:0.938 val loss: 0.881, r2: -0.271, rate:0.750, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:20 step:900 train loss: 0.689, r2: -0.345, rate:0.688, acc:0.625 val loss: 1.072, r2: -0.760, rate:0.562, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:20 step:1000 train loss: 0.709, r2: -13.692, rate:0.688, acc:0.875 val loss: 0.774, r2: 0.588, rate:0.938, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:20 step:1100 train loss: 0.505, r2: 0.694, rate:1.000, acc:0.875 val loss: 1.268, r2: 0.217, rate:0.688, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:20 step:1200 train loss: 0.907, r2: -0.520, rate:0.688, acc:0.688 val loss: 1.095, r2: 0.024, rate:0.625, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:20 step:1300 train loss: 1.047, r2: -3.232, rate:0.375, acc:0.500 val loss: 0.715, r2: 0.066, rate:0.688, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:20 step:1400 train loss: 0.877, r2: -0.907, rate:0.688, acc:0.375 val loss: 0.756, r2: -1.344, rate:0.625, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:20 step:1500 train loss: 0.743, r2: -2.433, rate:0.438, acc:0.750 val loss: 1.005, r2: -0.067, rate:0.938, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:20 step:1600 train loss: 0.748, r2: -3.318, rate:0.438, acc:0.875 val loss: 1.021, r2: -0.076, rate:0.562, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:20 step:1700 train loss: 0.783, r2: 0.493, rate:0.812, acc:0.750 val loss: 1.204, r2: -0.899, rate:0.688, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:20 step:1800 train loss: 0.626, r2: 0.695, rate:0.688, acc:0.875 val loss: 0.826, r2: 0.153, rate:0.688, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:20 step:1900 train loss: 1.327, r2: -0.232, rate:0.750, acc:0.562 val loss: 1.386, r2: 0.382, rate:0.750, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:20 step:2000 train loss: 1.025, r2: -3.465, rate:0.438, acc:0.562 val loss: 0.994, r2: 0.279, rate:0.438, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:20 step:2100 train loss: 0.823, r2: 0.640, rate:0.375, acc:0.438 val loss: 0.740, r2: 0.607, rate:0.812, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:20 step:2200 train loss: 0.748, r2: 0.630, rate:0.812, acc:0.875 val loss: 1.139, r2: -1.071, rate:0.688, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:20 step:2300 train loss: 1.068, r2: -0.882, rate:0.688, acc:0.500 val loss: 1.676, r2: -0.155, rate:0.750, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:20 step:2400 train loss: 0.818, r2: 0.286, rate:0.812, acc:0.750 val loss: 1.587, r2: -0.197, rate:0.625, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:20 step:2500 train loss: 1.116, r2: -1.715, rate:0.500, acc:0.500 val loss: 1.119, r2: -0.527, rate:0.438, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:20 step:2600 train loss: 1.107, r2: -1.031, rate:0.750, acc:0.188 val loss: 1.683, r2: -0.088, rate:0.625, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:20 step:2700 train loss: 0.803, r2: -2.244, rate:0.500, acc:0.625 val loss: 1.022, r2: -0.402, rate:0.688, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:20 step:2800 train loss: 1.356, r2: -0.838, rate:0.688, acc:0.500 val loss: 1.092, r2: -0.066, rate:0.625, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:20 step:2900 train loss: 0.787, r2: 0.159, rate:0.688, acc:0.500 val loss: 0.645, r2: 0.392, rate:0.812, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:20 step:3000 train loss: 1.343, r2: -0.728, rate:0.625, acc:0.500 val loss: 1.095, r2: -1.382, rate:0.625, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:20 step:3100 train loss: 0.906, r2: -2.078, rate:0.688, acc:0.438 val loss: 0.841, r2: -1.698, rate:0.688, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:20 step:3200 train loss: 3.983, r2: -1.773, rate:0.562, acc:0.375 val loss: 1.099, r2: -0.315, rate:0.625, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:20 step:3300 train loss: 1.225, r2: -3.146, rate:0.750, acc:0.562 val loss: 1.411, r2: -3.068, rate:0.562, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:20 step:3400 train loss: 0.838, r2: -1.950, rate:0.500, acc:0.875 val loss: 1.519, r2: -0.340, rate:0.750, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:20 step:3500 train loss: 1.596, r2: -0.297, rate:0.750, acc:0.875 val loss: 1.474, r2: -2.542, rate:0.562, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:20 step:3600 train loss: 1.046, r2: 0.287, rate:0.875, acc:0.875 val loss: 0.950, r2: -0.932, rate:0.688, acc:0.250 lr 9.534187405839861e-08\n",
      "epoch:20 step:3700 train loss: 0.723, r2: 0.127, rate:0.562, acc:0.438 val loss: 1.391, r2: -0.943, rate:0.500, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:20 step:3800 train loss: 1.269, r2: -0.283, rate:0.562, acc:0.438 val loss: 1.119, r2: 0.268, rate:0.750, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:20 step:3900 train loss: 0.861, r2: -2.843, rate:0.688, acc:0.562 val loss: 1.411, r2: -1.884, rate:0.688, acc:0.312 lr 9.534187405839861e-08\n",
      "epoch:20 step:4000 train loss: 2.606, r2: -5.123, rate:0.562, acc:0.812 val loss: 0.854, r2: 0.280, rate:0.688, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:20 step:4100 train loss: 1.405, r2: 0.457, rate:0.938, acc:0.938 val loss: 0.919, r2: 0.305, rate:0.750, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:20 step:4200 train loss: 1.569, r2: -0.143, rate:0.750, acc:0.875 val loss: 2.448, r2: -0.657, rate:0.625, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:20 step:4300 train loss: 0.670, r2: 0.117, rate:0.688, acc:0.312 val loss: 1.002, r2: 0.340, rate:0.688, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:20 step:4400 train loss: 1.414, r2: -1.331, rate:0.562, acc:0.438 val loss: 1.597, r2: 0.605, rate:0.750, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:20 step:4500 train loss: 1.054, r2: -0.400, rate:0.438, acc:0.625 val loss: 0.766, r2: 0.360, rate:0.875, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:20 step:4600 train loss: 1.079, r2: -0.193, rate:0.625, acc:0.750 val loss: 1.004, r2: -0.335, rate:0.688, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:20 step:4700 train loss: 2.244, r2: -12.483, rate:0.750, acc:0.875 val loss: 0.915, r2: -0.144, rate:0.625, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:20 step:4800 train loss: 1.363, r2: 0.442, rate:0.812, acc:0.812 val loss: 0.680, r2: -1.079, rate:0.688, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:20 step:4900 train loss: 0.899, r2: 0.577, rate:0.812, acc:0.688 val loss: 1.106, r2: 0.463, rate:0.688, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:20 step:5000 train loss: 2.835, r2: -12.712, rate:0.938, acc:0.938 val loss: 1.611, r2: -0.283, rate:0.688, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:20 step:5100 train loss: 0.762, r2: -2.961, rate:0.625, acc:0.250 val loss: 0.603, r2: 0.805, rate:0.688, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:20 step:5200 train loss: 0.734, r2: -1.875, rate:0.562, acc:0.188 val loss: 1.733, r2: -0.512, rate:0.375, acc:0.375 lr 9.534187405839861e-08\n",
      "epoch:20 step:5300 train loss: 1.087, r2: -1.508, rate:0.625, acc:0.312 val loss: 0.529, r2: 0.225, rate:0.625, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:20 step:5400 train loss: 0.774, r2: -1.165, rate:0.438, acc:0.312 val loss: 0.593, r2: 0.480, rate:0.875, acc:0.500 lr 9.534187405839861e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:5500 train loss: 1.318, r2: -0.377, rate:0.688, acc:0.875 val loss: 1.043, r2: -4.003, rate:0.688, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:20 step:5600 train loss: 3.151, r2: 0.367, rate:0.688, acc:0.750 val loss: 2.098, r2: 0.021, rate:0.625, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:20 step:5700 train loss: 1.419, r2: -1.046, rate:0.875, acc:0.875 val loss: 0.795, r2: -0.180, rate:0.812, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:20 step:5800 train loss: 1.198, r2: -0.019, rate:0.688, acc:0.750 val loss: 0.943, r2: 0.280, rate:0.688, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:20 step:5900 train loss: 0.904, r2: -0.817, rate:0.562, acc:0.562 val loss: 0.790, r2: 0.197, rate:0.688, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:20 step:6000 train loss: 0.965, r2: -0.760, rate:0.625, acc:0.500 val loss: 0.767, r2: 0.349, rate:0.688, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:20 step:6100 train loss: 0.671, r2: 0.107, rate:0.688, acc:0.750 val loss: 1.609, r2: -5.485, rate:0.688, acc:1.000 lr 9.534187405839861e-08\n",
      "epoch:20 step:6200 train loss: 1.040, r2: -0.553, rate:0.562, acc:0.750 val loss: 0.988, r2: 0.631, rate:0.938, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:20 step:6300 train loss: 0.628, r2: -0.198, rate:0.688, acc:0.750 val loss: 1.058, r2: -0.433, rate:0.562, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:20 step:6400 train loss: 0.822, r2: -2.765, rate:0.750, acc:0.688 val loss: 0.798, r2: 0.143, rate:0.500, acc:0.375 lr 9.534187405839861e-08\n",
      "epoch:20 step:6500 train loss: 4.048, r2: -0.022, rate:0.312, acc:0.188 val loss: 1.071, r2: -0.899, rate:0.500, acc:0.375 lr 9.534187405839861e-08\n",
      "epoch:20 step:6600 train loss: 1.077, r2: 0.479, rate:0.875, acc:0.812 val loss: 1.038, r2: -0.509, rate:0.688, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:20 step:6700 train loss: 0.791, r2: 0.462, rate:0.812, acc:0.625 val loss: 1.871, r2: -0.307, rate:0.875, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:20 step:6800 train loss: 0.713, r2: 0.483, rate:0.812, acc:0.875 val loss: 0.840, r2: -1.819, rate:0.500, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:20 step:6900 train loss: 0.586, r2: 0.571, rate:0.875, acc:0.750 val loss: 1.087, r2: 0.449, rate:0.688, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:20 step:7000 train loss: 0.677, r2: 0.583, rate:0.750, acc:0.562 val loss: 0.621, r2: -0.523, rate:0.688, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:20 step:7100 train loss: 2.026, r2: -0.500, rate:0.875, acc:1.000 val loss: 0.866, r2: 0.219, rate:0.500, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:20 step:7200 train loss: 1.121, r2: -1.841, rate:0.500, acc:0.188 val loss: 1.018, r2: 0.081, rate:0.438, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:20 step:7300 train loss: 0.684, r2: 0.356, rate:0.812, acc:0.875 val loss: 0.798, r2: 0.461, rate:0.562, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:20 step:7400 train loss: 0.634, r2: 0.327, rate:0.750, acc:0.625 val loss: 0.665, r2: -1.064, rate:0.625, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:20 step:7500 train loss: 0.746, r2: 0.328, rate:0.812, acc:0.688 val loss: 1.134, r2: -0.858, rate:0.688, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:20 step:7600 train loss: 0.773, r2: 0.472, rate:0.438, acc:0.500 val loss: 0.884, r2: -0.050, rate:0.625, acc:0.375 lr 9.534187405839861e-08\n",
      "epoch:20 step:7700 train loss: 1.577, r2: -0.691, rate:0.812, acc:0.688 val loss: 0.672, r2: -0.065, rate:0.750, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:20 step:7800 train loss: 1.083, r2: -0.224, rate:0.688, acc:0.750 val loss: 1.003, r2: -0.200, rate:0.625, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:20 step:0.327 lr 0.000000\n",
      "epoch:21 step:0 train loss: 1.153, r2: -0.150, rate:0.812, acc:0.500 val loss: 0.771, r2: 0.686, rate:0.688, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:21 step:100 train loss: 1.109, r2: -0.765, rate:0.625, acc:0.750 val loss: 0.652, r2: -0.087, rate:0.875, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:21 step:200 train loss: 1.325, r2: -0.889, rate:0.875, acc:1.000 val loss: 0.508, r2: 0.840, rate:1.000, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:21 step:300 train loss: 0.713, r2: 0.343, rate:0.750, acc:0.750 val loss: 0.789, r2: 0.085, rate:0.625, acc:1.000 lr 9.534187405839861e-08\n",
      "epoch:21 step:400 train loss: 1.194, r2: 0.363, rate:0.562, acc:0.562 val loss: 1.029, r2: -0.322, rate:0.562, acc:0.312 lr 9.534187405839861e-08\n",
      "epoch:21 step:500 train loss: 3.709, r2: -3.040, rate:0.938, acc:0.875 val loss: 0.672, r2: 0.535, rate:0.750, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:21 step:600 train loss: 0.762, r2: 0.522, rate:0.625, acc:0.625 val loss: 0.812, r2: 0.274, rate:0.688, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:21 step:700 train loss: 1.455, r2: -0.128, rate:0.750, acc:0.750 val loss: 1.308, r2: -0.473, rate:0.562, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:21 step:800 train loss: 0.929, r2: -2.293, rate:0.625, acc:0.375 val loss: 0.678, r2: 0.551, rate:0.625, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:21 step:900 train loss: 1.458, r2: 0.463, rate:0.750, acc:0.812 val loss: 1.769, r2: -3.451, rate:0.562, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:21 step:1000 train loss: 0.732, r2: -0.199, rate:0.562, acc:0.688 val loss: 1.862, r2: -0.753, rate:0.875, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:21 step:1100 train loss: 1.420, r2: 0.132, rate:0.625, acc:0.625 val loss: 0.713, r2: 0.637, rate:0.812, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:21 step:1200 train loss: 0.722, r2: -0.282, rate:0.625, acc:0.625 val loss: 0.749, r2: 0.423, rate:0.688, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:21 step:1300 train loss: 1.333, r2: -0.417, rate:0.812, acc:0.750 val loss: 1.443, r2: 0.138, rate:0.625, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:21 step:1400 train loss: 0.740, r2: 0.719, rate:0.938, acc:0.750 val loss: 1.949, r2: -2.493, rate:0.688, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:21 step:1500 train loss: 0.609, r2: -0.077, rate:0.688, acc:0.938 val loss: 0.997, r2: 0.344, rate:0.750, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:21 step:1600 train loss: 1.007, r2: 0.354, rate:0.625, acc:0.562 val loss: 1.542, r2: 0.082, rate:0.750, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:21 step:1700 train loss: 0.620, r2: 0.665, rate:0.812, acc:0.938 val loss: 0.756, r2: -0.285, rate:0.938, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:21 step:1800 train loss: 0.820, r2: 0.232, rate:0.562, acc:0.688 val loss: 0.901, r2: -0.714, rate:0.562, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:21 step:1900 train loss: 0.528, r2: 0.766, rate:0.938, acc:0.938 val loss: 0.858, r2: -0.049, rate:0.938, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:21 step:2000 train loss: 0.964, r2: -2.352, rate:0.875, acc:1.000 val loss: 0.987, r2: -0.536, rate:0.500, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:21 step:2100 train loss: 0.938, r2: -4.244, rate:0.688, acc:0.625 val loss: 0.795, r2: 0.501, rate:0.688, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:21 step:2200 train loss: 0.934, r2: -0.615, rate:0.500, acc:0.375 val loss: 0.598, r2: 0.530, rate:0.812, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:21 step:2300 train loss: 0.937, r2: -0.764, rate:0.562, acc:0.688 val loss: 1.019, r2: -1.430, rate:0.688, acc:0.250 lr 9.534187405839861e-08\n",
      "epoch:21 step:2400 train loss: 0.661, r2: -0.935, rate:0.812, acc:0.812 val loss: 0.575, r2: 0.793, rate:0.938, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:21 step:2500 train loss: 1.217, r2: -0.752, rate:0.688, acc:0.688 val loss: 0.881, r2: 0.302, rate:0.688, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:21 step:2600 train loss: 2.597, r2: -2.771, rate:0.562, acc:0.250 val loss: 1.098, r2: -0.372, rate:0.750, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:21 step:2700 train loss: 1.011, r2: -0.525, rate:0.562, acc:0.375 val loss: 1.003, r2: -4.829, rate:0.625, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:21 step:2800 train loss: 0.736, r2: -0.125, rate:0.875, acc:0.938 val loss: 0.921, r2: -1.985, rate:0.875, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:21 step:2900 train loss: 1.273, r2: -4.298, rate:0.812, acc:0.938 val loss: 0.988, r2: -0.281, rate:0.688, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:21 step:3000 train loss: 1.419, r2: -0.255, rate:0.250, acc:0.312 val loss: 1.414, r2: 0.124, rate:0.688, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:21 step:3100 train loss: 0.935, r2: 0.434, rate:0.750, acc:0.688 val loss: 1.049, r2: 0.224, rate:0.688, acc:0.812 lr 9.534187405839861e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:3200 train loss: 1.125, r2: -3.670, rate:0.562, acc:0.750 val loss: 0.924, r2: 0.216, rate:0.562, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:21 step:3300 train loss: 0.754, r2: -0.296, rate:0.500, acc:0.812 val loss: 1.058, r2: -0.241, rate:0.750, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:21 step:3400 train loss: 0.750, r2: -1.314, rate:0.438, acc:0.625 val loss: 2.191, r2: -0.636, rate:0.562, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:21 step:3500 train loss: 3.899, r2: -4.320, rate:0.812, acc:0.938 val loss: 1.135, r2: 0.043, rate:0.500, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:21 step:3600 train loss: 0.961, r2: 0.299, rate:0.875, acc:0.688 val loss: 1.336, r2: 0.025, rate:0.625, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:21 step:3700 train loss: 1.173, r2: -3.015, rate:0.812, acc:0.625 val loss: 1.609, r2: -0.403, rate:0.562, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:21 step:3800 train loss: 0.698, r2: -0.167, rate:0.750, acc:0.750 val loss: 0.783, r2: 0.462, rate:0.562, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:21 step:3900 train loss: 1.824, r2: -2.507, rate:0.562, acc:0.688 val loss: 0.981, r2: 0.100, rate:0.750, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:21 step:4000 train loss: 1.226, r2: -1.937, rate:0.312, acc:0.062 val loss: 0.785, r2: -0.248, rate:0.625, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:21 step:4100 train loss: 0.841, r2: -0.495, rate:0.562, acc:0.875 val loss: 1.708, r2: -0.167, rate:0.688, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:21 step:4200 train loss: 0.788, r2: -2.817, rate:0.500, acc:0.750 val loss: 1.194, r2: 0.316, rate:0.688, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:21 step:4300 train loss: 0.692, r2: -0.562, rate:0.750, acc:0.375 val loss: 0.805, r2: 0.581, rate:0.625, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:21 step:4400 train loss: 2.244, r2: -1.731, rate:0.188, acc:0.625 val loss: 1.306, r2: 0.118, rate:0.500, acc:0.312 lr 9.534187405839861e-08\n",
      "epoch:21 step:4500 train loss: 1.001, r2: -0.019, rate:0.812, acc:0.375 val loss: 0.717, r2: 0.623, rate:0.938, acc:1.000 lr 9.534187405839861e-08\n",
      "epoch:21 step:4600 train loss: 1.657, r2: -0.086, rate:0.625, acc:0.938 val loss: 0.878, r2: 0.414, rate:0.688, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:21 step:4700 train loss: 2.948, r2: 0.099, rate:0.375, acc:0.562 val loss: 1.385, r2: -0.426, rate:0.500, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:21 step:4800 train loss: 0.956, r2: 0.503, rate:0.625, acc:0.875 val loss: 0.731, r2: -0.758, rate:0.812, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:21 step:4900 train loss: 1.671, r2: -4.955, rate:0.812, acc:0.250 val loss: 0.911, r2: -0.657, rate:0.562, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:21 step:5000 train loss: 0.925, r2: 0.391, rate:0.812, acc:0.750 val loss: 1.781, r2: -6.595, rate:0.438, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:21 step:5100 train loss: 0.679, r2: 0.500, rate:0.812, acc:0.625 val loss: 1.014, r2: -2.862, rate:0.625, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:21 step:5200 train loss: 0.806, r2: -1.520, rate:0.375, acc:0.750 val loss: 8.549, r2: -9.821, rate:0.562, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:21 step:5300 train loss: 1.064, r2: -1.728, rate:0.500, acc:0.438 val loss: 0.914, r2: -1.663, rate:0.688, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:21 step:5400 train loss: 1.055, r2: -0.245, rate:0.812, acc:0.688 val loss: 1.090, r2: -0.079, rate:0.812, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:21 step:5500 train loss: 0.611, r2: -1.614, rate:0.938, acc:1.000 val loss: 0.649, r2: 0.384, rate:0.875, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:21 step:5600 train loss: 1.394, r2: -1.661, rate:0.562, acc:0.562 val loss: 0.852, r2: 0.461, rate:0.625, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:21 step:5700 train loss: 2.047, r2: -3.479, rate:0.750, acc:0.625 val loss: 0.922, r2: -0.074, rate:0.625, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:21 step:5800 train loss: 0.589, r2: 0.670, rate:0.688, acc:0.875 val loss: 1.088, r2: 0.587, rate:0.625, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:21 step:5900 train loss: 0.992, r2: -1.172, rate:0.688, acc:0.562 val loss: 0.545, r2: 0.642, rate:0.938, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:21 step:6000 train loss: 0.679, r2: 0.097, rate:0.625, acc:0.625 val loss: 0.993, r2: -0.065, rate:0.562, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:21 step:6100 train loss: 0.888, r2: -2.444, rate:0.625, acc:0.875 val loss: 1.531, r2: -1.131, rate:0.562, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:21 step:6200 train loss: 0.913, r2: 0.101, rate:0.750, acc:0.438 val loss: 1.178, r2: -0.362, rate:0.688, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:21 step:6300 train loss: 0.726, r2: 0.353, rate:0.750, acc:0.625 val loss: 0.993, r2: 0.022, rate:0.625, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:21 step:6400 train loss: 0.780, r2: -1.712, rate:0.438, acc:0.750 val loss: 5.296, r2: -1.326, rate:0.812, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:21 step:6500 train loss: 3.409, r2: -0.763, rate:0.438, acc:0.438 val loss: 0.759, r2: 0.195, rate:0.750, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:21 step:6600 train loss: 0.755, r2: -0.751, rate:0.438, acc:0.625 val loss: 0.832, r2: -1.211, rate:0.375, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:21 step:6700 train loss: 0.718, r2: 0.586, rate:0.688, acc:0.688 val loss: 1.008, r2: 0.075, rate:1.000, acc:1.000 lr 9.534187405839861e-08\n",
      "epoch:21 step:6800 train loss: 2.207, r2: -1.289, rate:0.812, acc:0.750 val loss: 0.854, r2: -1.919, rate:0.688, acc:0.312 lr 9.534187405839861e-08\n",
      "epoch:21 step:6900 train loss: 0.574, r2: 0.618, rate:0.938, acc:0.375 val loss: 0.647, r2: -0.934, rate:0.438, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:21 step:7000 train loss: 0.838, r2: -0.702, rate:0.625, acc:0.500 val loss: 1.115, r2: -1.266, rate:0.438, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:21 step:7100 train loss: 3.089, r2: 0.448, rate:0.875, acc:0.875 val loss: 0.641, r2: 0.679, rate:0.875, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:21 step:7200 train loss: 0.900, r2: -0.261, rate:0.562, acc:0.438 val loss: 2.070, r2: -1.073, rate:0.562, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:21 step:7300 train loss: 0.969, r2: -0.836, rate:0.625, acc:0.938 val loss: 0.627, r2: 0.536, rate:0.750, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:21 step:7400 train loss: 0.946, r2: -0.397, rate:0.938, acc:0.562 val loss: 1.012, r2: 0.080, rate:0.500, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:21 step:7500 train loss: 2.000, r2: -1.496, rate:1.000, acc:0.938 val loss: 0.857, r2: 0.298, rate:0.625, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:21 step:7600 train loss: 1.624, r2: -0.981, rate:0.812, acc:0.625 val loss: 0.777, r2: 0.178, rate:0.688, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:21 step:7700 train loss: 2.163, r2: -1.946, rate:0.375, acc:0.500 val loss: 0.681, r2: 0.157, rate:0.500, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:21 step:7800 train loss: 1.215, r2: -1.118, rate:0.688, acc:0.875 val loss: 0.556, r2: 0.337, rate:0.812, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:21 step:0.327 lr 0.000000\n",
      "epoch:22 step:0 train loss: 0.665, r2: -1.617, rate:0.625, acc:0.812 val loss: 0.773, r2: -1.341, rate:0.875, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:22 step:100 train loss: 0.873, r2: 0.386, rate:0.500, acc:0.438 val loss: 0.943, r2: -0.198, rate:0.812, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:22 step:200 train loss: 0.977, r2: 0.601, rate:0.750, acc:0.438 val loss: 0.586, r2: -1.511, rate:0.750, acc:1.000 lr 9.534187405839861e-08\n",
      "epoch:22 step:300 train loss: 1.095, r2: -0.721, rate:0.688, acc:0.625 val loss: 1.008, r2: -0.303, rate:0.750, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:22 step:400 train loss: 1.013, r2: -0.525, rate:0.500, acc:0.750 val loss: 1.118, r2: -0.625, rate:0.625, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:22 step:500 train loss: 0.942, r2: -0.839, rate:0.625, acc:0.688 val loss: 1.313, r2: 0.105, rate:0.750, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:22 step:600 train loss: 0.911, r2: -1.578, rate:0.500, acc:0.625 val loss: 1.578, r2: -3.700, rate:0.375, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:22 step:700 train loss: 1.297, r2: -2.122, rate:0.625, acc:0.812 val loss: 1.022, r2: -0.130, rate:0.875, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:22 step:800 train loss: 0.830, r2: -1.107, rate:0.625, acc:1.000 val loss: 4.453, r2: 0.394, rate:0.625, acc:0.562 lr 9.534187405839861e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:900 train loss: 0.621, r2: -0.608, rate:0.812, acc:0.625 val loss: 0.565, r2: 0.546, rate:0.938, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:22 step:1000 train loss: 0.942, r2: -0.143, rate:0.500, acc:0.750 val loss: 0.836, r2: 0.557, rate:0.875, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:22 step:1100 train loss: 1.716, r2: -8.733, rate:0.188, acc:0.688 val loss: 0.603, r2: 0.706, rate:0.938, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:22 step:1200 train loss: 0.776, r2: 0.191, rate:0.500, acc:0.562 val loss: 0.936, r2: -0.307, rate:0.562, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:22 step:1300 train loss: 2.142, r2: -1.493, rate:0.688, acc:0.375 val loss: 1.092, r2: -0.348, rate:0.812, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:22 step:1400 train loss: 0.720, r2: 0.655, rate:0.625, acc:0.750 val loss: 0.747, r2: -0.706, rate:0.688, acc:1.000 lr 9.534187405839861e-08\n",
      "epoch:22 step:1500 train loss: 0.596, r2: 0.724, rate:0.812, acc:0.688 val loss: 0.897, r2: 0.410, rate:0.625, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:22 step:1600 train loss: 0.923, r2: -1.807, rate:0.500, acc:0.812 val loss: 0.733, r2: -2.135, rate:0.562, acc:0.250 lr 9.534187405839861e-08\n",
      "epoch:22 step:1700 train loss: 1.327, r2: -1.371, rate:0.875, acc:1.000 val loss: 2.619, r2: -1.318, rate:0.625, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:22 step:1800 train loss: 0.812, r2: -0.803, rate:0.625, acc:0.625 val loss: 0.957, r2: -0.735, rate:0.812, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:22 step:1900 train loss: 0.958, r2: -1.297, rate:0.562, acc:0.312 val loss: 0.510, r2: 0.400, rate:0.938, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:22 step:2000 train loss: 2.931, r2: -2.373, rate:0.438, acc:0.688 val loss: 0.485, r2: -1.278, rate:1.000, acc:1.000 lr 9.534187405839861e-08\n",
      "epoch:22 step:2100 train loss: 0.783, r2: 0.020, rate:0.688, acc:0.562 val loss: 0.862, r2: 0.429, rate:0.750, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:22 step:2200 train loss: 2.713, r2: -3.973, rate:0.375, acc:0.312 val loss: 0.965, r2: 0.476, rate:0.750, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:22 step:2300 train loss: 0.673, r2: 0.426, rate:0.812, acc:0.625 val loss: 1.658, r2: 0.258, rate:0.750, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:22 step:2400 train loss: 0.593, r2: 0.335, rate:0.688, acc:0.750 val loss: 1.101, r2: -0.722, rate:0.812, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:22 step:2500 train loss: 1.046, r2: -1.957, rate:0.312, acc:0.250 val loss: 0.704, r2: -0.287, rate:0.750, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:22 step:2600 train loss: 0.568, r2: 0.564, rate:0.812, acc:0.812 val loss: 1.034, r2: -0.915, rate:0.438, acc:0.188 lr 9.534187405839861e-08\n",
      "epoch:22 step:2700 train loss: 0.838, r2: -1.927, rate:0.438, acc:0.688 val loss: 2.851, r2: -0.357, rate:0.750, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:22 step:2800 train loss: 0.930, r2: 0.146, rate:0.562, acc:0.688 val loss: 3.313, r2: 0.220, rate:0.688, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:22 step:2900 train loss: 0.962, r2: -0.743, rate:0.562, acc:0.188 val loss: 0.818, r2: 0.427, rate:0.812, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:22 step:3000 train loss: 0.679, r2: 0.128, rate:0.875, acc:0.812 val loss: 1.010, r2: -0.209, rate:0.688, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:22 step:3100 train loss: 1.101, r2: 0.281, rate:0.750, acc:0.562 val loss: 1.023, r2: 0.471, rate:0.875, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:22 step:3200 train loss: 0.759, r2: 0.121, rate:0.750, acc:0.875 val loss: 0.804, r2: 0.436, rate:0.875, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:22 step:3300 train loss: 0.637, r2: -0.353, rate:0.688, acc:0.812 val loss: 0.868, r2: -0.011, rate:0.875, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:22 step:3400 train loss: 0.670, r2: 0.451, rate:0.750, acc:0.562 val loss: 1.473, r2: -0.934, rate:0.750, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:22 step:3500 train loss: 0.754, r2: -0.132, rate:0.625, acc:0.438 val loss: 0.697, r2: 0.519, rate:0.812, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:22 step:3600 train loss: 0.805, r2: -1.226, rate:0.438, acc:0.500 val loss: 0.960, r2: -0.368, rate:0.562, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:22 step:3700 train loss: 0.993, r2: 0.445, rate:0.688, acc:0.750 val loss: 0.786, r2: 0.469, rate:0.750, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:22 step:3800 train loss: 2.013, r2: 0.481, rate:0.875, acc:0.875 val loss: 0.884, r2: 0.460, rate:0.750, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:22 step:3900 train loss: 0.522, r2: 0.100, rate:0.750, acc:0.875 val loss: 0.826, r2: -1.091, rate:0.625, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:22 step:4000 train loss: 0.878, r2: -0.160, rate:0.688, acc:0.562 val loss: 0.740, r2: 0.054, rate:0.812, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:22 step:4100 train loss: 1.298, r2: -2.957, rate:0.500, acc:0.250 val loss: 0.931, r2: 0.237, rate:0.625, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:22 step:4200 train loss: 0.636, r2: 0.016, rate:0.750, acc:0.750 val loss: 0.805, r2: 0.566, rate:0.812, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:22 step:4300 train loss: 0.812, r2: -0.436, rate:0.688, acc:0.875 val loss: 0.649, r2: 0.446, rate:0.688, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:22 step:4400 train loss: 0.851, r2: 0.511, rate:0.750, acc:0.500 val loss: 0.800, r2: -0.023, rate:0.562, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:22 step:4500 train loss: 0.634, r2: 0.463, rate:0.750, acc:0.375 val loss: 1.049, r2: 0.022, rate:0.688, acc:0.375 lr 9.534187405839861e-08\n",
      "epoch:22 step:4600 train loss: 0.435, r2: 0.464, rate:0.938, acc:0.938 val loss: 0.917, r2: 0.141, rate:0.688, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:22 step:4700 train loss: 0.945, r2: 0.696, rate:0.812, acc:0.688 val loss: 1.079, r2: 0.210, rate:0.562, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:22 step:4800 train loss: 0.766, r2: 0.373, rate:0.812, acc:0.500 val loss: 1.428, r2: -0.325, rate:0.688, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:22 step:4900 train loss: 1.360, r2: 0.012, rate:0.750, acc:0.875 val loss: 0.743, r2: 0.055, rate:0.812, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:22 step:5000 train loss: 0.526, r2: 0.695, rate:1.000, acc:0.875 val loss: 1.293, r2: 0.156, rate:0.500, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:22 step:5100 train loss: 0.645, r2: 0.177, rate:0.750, acc:0.812 val loss: 1.063, r2: -0.915, rate:0.562, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:22 step:5200 train loss: 0.496, r2: 0.706, rate:0.875, acc:0.875 val loss: 0.775, r2: -1.226, rate:0.688, acc:0.375 lr 9.534187405839861e-08\n",
      "epoch:22 step:5300 train loss: 0.936, r2: -2.227, rate:0.312, acc:0.375 val loss: 1.653, r2: -0.925, rate:0.688, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:22 step:5400 train loss: 0.707, r2: 0.530, rate:0.562, acc:0.562 val loss: 1.082, r2: 0.264, rate:0.750, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:22 step:5500 train loss: 0.753, r2: -1.093, rate:0.438, acc:0.750 val loss: 1.092, r2: 0.045, rate:0.500, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:22 step:5600 train loss: 1.001, r2: -0.591, rate:0.812, acc:0.250 val loss: 1.448, r2: -2.123, rate:0.875, acc:1.000 lr 9.534187405839861e-08\n",
      "epoch:22 step:5700 train loss: 0.655, r2: 0.442, rate:0.812, acc:0.688 val loss: 0.784, r2: 0.216, rate:0.938, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:22 step:5800 train loss: 0.613, r2: -2.524, rate:0.812, acc:0.875 val loss: 2.091, r2: 0.302, rate:0.688, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:22 step:5900 train loss: 0.598, r2: 0.065, rate:0.875, acc:0.375 val loss: 0.804, r2: 0.080, rate:0.688, acc:0.375 lr 9.534187405839861e-08\n",
      "epoch:22 step:6000 train loss: 0.615, r2: -0.284, rate:0.812, acc:0.812 val loss: 0.683, r2: 0.572, rate:0.812, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:22 step:6100 train loss: 0.591, r2: 0.387, rate:0.688, acc:0.688 val loss: 0.857, r2: 0.082, rate:0.688, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:22 step:6200 train loss: 0.700, r2: 0.538, rate:0.625, acc:0.438 val loss: 0.655, r2: 0.503, rate:0.750, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:22 step:6300 train loss: 0.732, r2: -3.521, rate:0.625, acc:0.438 val loss: 1.027, r2: -0.070, rate:0.688, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:22 step:6400 train loss: 0.717, r2: 0.416, rate:0.938, acc:0.625 val loss: 0.907, r2: -2.358, rate:0.625, acc:0.938 lr 9.534187405839861e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:6500 train loss: 0.866, r2: 0.451, rate:0.812, acc:0.688 val loss: 1.041, r2: -0.015, rate:0.500, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:22 step:6600 train loss: 0.455, r2: 0.761, rate:0.938, acc:0.812 val loss: 1.033, r2: 0.483, rate:0.562, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:22 step:6700 train loss: 1.447, r2: 0.076, rate:0.750, acc:0.750 val loss: 0.933, r2: -0.578, rate:0.562, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:22 step:6800 train loss: 1.098, r2: -1.017, rate:0.625, acc:0.812 val loss: 1.117, r2: -0.081, rate:0.688, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:22 step:6900 train loss: 0.628, r2: 0.555, rate:0.562, acc:0.625 val loss: 2.328, r2: 0.452, rate:0.875, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:22 step:7000 train loss: 0.644, r2: -0.266, rate:0.688, acc:0.688 val loss: 0.957, r2: 0.277, rate:0.688, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:22 step:7100 train loss: 1.023, r2: -15.246, rate:0.750, acc:0.938 val loss: 2.289, r2: -12.498, rate:0.688, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:22 step:7200 train loss: 0.680, r2: -3.307, rate:0.750, acc:0.562 val loss: 1.425, r2: -0.405, rate:0.438, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:22 step:7300 train loss: 1.550, r2: 0.012, rate:0.812, acc:0.812 val loss: 0.760, r2: 0.002, rate:0.562, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:22 step:7400 train loss: 1.084, r2: -0.658, rate:0.688, acc:0.500 val loss: 0.688, r2: -0.946, rate:0.625, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:22 step:7500 train loss: 0.829, r2: -0.156, rate:0.625, acc:0.750 val loss: 1.145, r2: -0.561, rate:0.500, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:22 step:7600 train loss: 1.004, r2: 0.256, rate:0.500, acc:0.812 val loss: 0.685, r2: 0.502, rate:0.688, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:22 step:7700 train loss: 1.968, r2: -3.907, rate:0.938, acc:1.000 val loss: 1.542, r2: -0.532, rate:0.500, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:22 step:7800 train loss: 0.678, r2: -1.514, rate:0.562, acc:0.625 val loss: 1.484, r2: -0.200, rate:0.500, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:22 step:0.327 lr 0.000000\n",
      "epoch:23 step:0 train loss: 0.661, r2: 0.003, rate:0.812, acc:0.625 val loss: 1.182, r2: -3.573, rate:0.500, acc:0.250 lr 9.534187405839861e-08\n",
      "epoch:23 step:100 train loss: 0.761, r2: -0.053, rate:0.812, acc:0.375 val loss: 0.948, r2: 0.190, rate:0.812, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:23 step:200 train loss: 1.284, r2: -1.090, rate:0.562, acc:0.125 val loss: 1.074, r2: 0.000, rate:0.688, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:23 step:300 train loss: 0.754, r2: 0.386, rate:0.875, acc:0.625 val loss: 0.749, r2: -0.393, rate:0.688, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:23 step:400 train loss: 1.095, r2: -1.071, rate:0.750, acc:0.875 val loss: 0.962, r2: 0.168, rate:0.750, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:23 step:500 train loss: 0.611, r2: 0.174, rate:0.875, acc:0.688 val loss: 0.830, r2: -2.116, rate:0.375, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:23 step:600 train loss: 0.717, r2: 0.221, rate:0.750, acc:0.312 val loss: 1.186, r2: -0.646, rate:0.688, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:23 step:700 train loss: 0.623, r2: 0.173, rate:0.812, acc:0.750 val loss: 1.121, r2: -0.529, rate:0.625, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:23 step:800 train loss: 1.398, r2: -4.082, rate:0.312, acc:0.188 val loss: 0.584, r2: 0.442, rate:0.938, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:23 step:900 train loss: 0.752, r2: -0.115, rate:0.562, acc:0.625 val loss: 0.795, r2: 0.017, rate:0.562, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:23 step:1000 train loss: 0.629, r2: -0.057, rate:0.938, acc:0.875 val loss: 1.750, r2: -0.719, rate:0.562, acc:0.312 lr 9.534187405839861e-08\n",
      "epoch:23 step:1100 train loss: 1.074, r2: -2.982, rate:0.438, acc:0.250 val loss: 0.679, r2: 0.266, rate:0.500, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:23 step:1200 train loss: 1.198, r2: -0.898, rate:0.688, acc:0.938 val loss: 1.238, r2: -0.647, rate:0.750, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:23 step:1300 train loss: 4.667, r2: -1.405, rate:0.250, acc:0.500 val loss: 0.532, r2: 0.846, rate:0.812, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:23 step:1400 train loss: 0.697, r2: -1.029, rate:0.750, acc:0.688 val loss: 0.591, r2: -0.136, rate:0.750, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:23 step:1500 train loss: 0.740, r2: -3.359, rate:0.750, acc:0.750 val loss: 0.734, r2: 0.499, rate:0.688, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:23 step:1600 train loss: 0.572, r2: -0.563, rate:1.000, acc:0.500 val loss: 0.929, r2: -0.663, rate:0.562, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:23 step:1700 train loss: 0.638, r2: 0.021, rate:0.562, acc:0.750 val loss: 1.878, r2: -0.315, rate:0.812, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:23 step:1800 train loss: 3.694, r2: -1.131, rate:0.812, acc:0.875 val loss: 1.164, r2: 0.513, rate:0.875, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:23 step:1900 train loss: 1.151, r2: -1.550, rate:0.375, acc:0.250 val loss: 0.693, r2: -1.312, rate:0.688, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:23 step:2000 train loss: 0.923, r2: -0.049, rate:0.562, acc:0.625 val loss: 0.783, r2: 0.051, rate:0.688, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:23 step:2100 train loss: 0.678, r2: -2.420, rate:0.500, acc:0.938 val loss: 1.798, r2: 0.170, rate:0.562, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:23 step:2200 train loss: 1.196, r2: -1.147, rate:0.688, acc:0.500 val loss: 0.862, r2: 0.035, rate:0.688, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:23 step:2300 train loss: 0.739, r2: 0.534, rate:0.938, acc:0.875 val loss: 0.762, r2: 0.335, rate:0.625, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:23 step:2400 train loss: 0.694, r2: 0.366, rate:0.562, acc:0.688 val loss: 1.065, r2: 0.314, rate:0.812, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:23 step:2500 train loss: 0.493, r2: -0.304, rate:0.938, acc:0.812 val loss: 1.464, r2: 0.365, rate:0.750, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:23 step:2600 train loss: 0.932, r2: -0.294, rate:0.375, acc:0.312 val loss: 1.416, r2: -1.594, rate:0.625, acc:0.188 lr 9.534187405839861e-08\n",
      "epoch:23 step:2700 train loss: 0.611, r2: 0.505, rate:1.000, acc:0.812 val loss: 0.768, r2: -0.541, rate:0.875, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:23 step:2800 train loss: 0.744, r2: -0.918, rate:0.562, acc:0.438 val loss: 1.171, r2: -0.963, rate:0.562, acc:0.375 lr 9.534187405839861e-08\n",
      "epoch:23 step:2900 train loss: 0.839, r2: -0.123, rate:0.562, acc:0.500 val loss: 0.611, r2: 0.595, rate:0.812, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:23 step:3000 train loss: 0.742, r2: 0.421, rate:0.625, acc:0.688 val loss: 0.736, r2: 0.230, rate:0.625, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:23 step:3100 train loss: 0.693, r2: 0.549, rate:0.750, acc:0.688 val loss: 1.247, r2: 0.132, rate:0.812, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:23 step:3200 train loss: 0.735, r2: -3.027, rate:0.312, acc:0.562 val loss: 0.878, r2: -1.119, rate:0.812, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:23 step:3300 train loss: 0.672, r2: -0.478, rate:0.625, acc:0.625 val loss: 0.886, r2: 0.387, rate:0.688, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:23 step:3400 train loss: 0.706, r2: -0.740, rate:0.500, acc:0.875 val loss: 1.474, r2: 0.129, rate:0.750, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:23 step:3500 train loss: 1.098, r2: -0.614, rate:0.625, acc:0.375 val loss: 0.674, r2: 0.538, rate:0.938, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:23 step:3600 train loss: 2.058, r2: 0.325, rate:0.812, acc:0.750 val loss: 1.822, r2: -0.724, rate:0.562, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:23 step:3700 train loss: 0.906, r2: 0.131, rate:0.750, acc:0.812 val loss: 0.909, r2: -0.129, rate:0.438, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:23 step:3800 train loss: 0.689, r2: 0.469, rate:0.688, acc:0.750 val loss: 0.636, r2: 0.619, rate:0.875, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:23 step:3900 train loss: 0.637, r2: 0.274, rate:0.875, acc:0.562 val loss: 0.636, r2: 0.609, rate:1.000, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:23 step:4000 train loss: 1.079, r2: 0.497, rate:0.812, acc:0.625 val loss: 0.769, r2: 0.250, rate:0.750, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:23 step:4100 train loss: 2.030, r2: -5.035, rate:0.750, acc:0.500 val loss: 1.127, r2: -0.593, rate:0.375, acc:0.312 lr 9.534187405839861e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:4200 train loss: 0.842, r2: -1.721, rate:0.438, acc:0.500 val loss: 1.357, r2: 0.393, rate:0.750, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:23 step:4300 train loss: 0.775, r2: 0.437, rate:0.875, acc:0.875 val loss: 0.926, r2: -1.059, rate:0.688, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:23 step:4400 train loss: 1.087, r2: -1.020, rate:0.688, acc:0.938 val loss: 1.000, r2: -1.143, rate:0.750, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:23 step:4500 train loss: 0.891, r2: 0.381, rate:0.500, acc:0.688 val loss: 0.726, r2: 0.611, rate:0.750, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:23 step:4600 train loss: 1.293, r2: -4.866, rate:0.250, acc:0.438 val loss: 1.103, r2: -6.578, rate:0.562, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:23 step:4700 train loss: 0.805, r2: 0.369, rate:1.000, acc:0.875 val loss: 2.197, r2: -2.485, rate:0.750, acc:1.000 lr 9.534187405839861e-08\n",
      "epoch:23 step:4800 train loss: 0.624, r2: 0.648, rate:0.875, acc:0.438 val loss: 0.878, r2: -0.473, rate:0.688, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:23 step:4900 train loss: 6.146, r2: -28.456, rate:0.938, acc:1.000 val loss: 0.529, r2: 0.640, rate:1.000, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:23 step:5000 train loss: 1.016, r2: -1.099, rate:0.750, acc:0.938 val loss: 1.220, r2: 0.099, rate:0.625, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:23 step:5100 train loss: 0.910, r2: -2.218, rate:0.688, acc:0.312 val loss: 0.987, r2: 0.358, rate:0.750, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:23 step:5200 train loss: 0.674, r2: 0.138, rate:0.750, acc:0.688 val loss: 0.961, r2: -0.718, rate:0.625, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:23 step:5300 train loss: 1.865, r2: -0.992, rate:0.375, acc:0.438 val loss: 0.835, r2: -0.275, rate:0.750, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:23 step:5400 train loss: 0.604, r2: 0.378, rate:0.688, acc:0.688 val loss: 0.883, r2: 0.527, rate:0.562, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:23 step:5500 train loss: 0.367, r2: -0.403, rate:1.000, acc:1.000 val loss: 0.954, r2: 0.177, rate:0.688, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:23 step:5600 train loss: 0.822, r2: -0.625, rate:0.688, acc:0.812 val loss: 1.197, r2: -0.412, rate:0.500, acc:0.375 lr 9.534187405839861e-08\n",
      "epoch:23 step:5700 train loss: 0.770, r2: 0.128, rate:0.625, acc:0.562 val loss: 0.668, r2: 0.621, rate:0.875, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:23 step:5800 train loss: 2.053, r2: -12.658, rate:1.000, acc:1.000 val loss: 0.708, r2: 0.507, rate:0.875, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:23 step:5900 train loss: 0.727, r2: 0.703, rate:0.812, acc:0.562 val loss: 0.639, r2: 0.221, rate:0.625, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:23 step:6000 train loss: 3.036, r2: -3.314, rate:0.750, acc:0.938 val loss: 0.926, r2: -0.153, rate:0.688, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:23 step:6100 train loss: 0.842, r2: -0.212, rate:0.438, acc:0.750 val loss: 1.987, r2: 0.373, rate:0.875, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:23 step:6200 train loss: 0.540, r2: 0.465, rate:1.000, acc:0.938 val loss: 1.153, r2: -0.755, rate:0.438, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:23 step:6300 train loss: 0.604, r2: 0.739, rate:0.812, acc:0.625 val loss: 0.730, r2: 0.378, rate:0.750, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:23 step:6400 train loss: 1.626, r2: 0.009, rate:0.562, acc:0.750 val loss: 0.777, r2: 0.468, rate:0.750, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:23 step:6500 train loss: 9.948, r2: -8.817, rate:0.688, acc:0.875 val loss: 0.672, r2: -0.617, rate:0.688, acc:1.000 lr 9.534187405839861e-08\n",
      "epoch:23 step:6600 train loss: 0.703, r2: -4.759, rate:0.438, acc:0.562 val loss: 1.474, r2: -0.339, rate:0.562, acc:0.375 lr 9.534187405839861e-08\n",
      "epoch:23 step:6700 train loss: 3.141, r2: -1.376, rate:0.688, acc:1.000 val loss: 0.875, r2: -0.869, rate:0.750, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:23 step:6800 train loss: 0.749, r2: 0.139, rate:0.750, acc:0.750 val loss: 6.177, r2: 0.019, rate:0.625, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:23 step:6900 train loss: 0.930, r2: -0.904, rate:0.812, acc:0.875 val loss: 0.854, r2: -0.647, rate:0.500, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:23 step:7000 train loss: 0.622, r2: -0.870, rate:0.688, acc:0.812 val loss: 0.913, r2: 0.237, rate:0.562, acc:0.375 lr 9.534187405839861e-08\n",
      "epoch:23 step:7100 train loss: 1.475, r2: 0.153, rate:0.562, acc:0.625 val loss: 0.873, r2: 0.096, rate:0.688, acc:0.312 lr 9.534187405839861e-08\n",
      "epoch:23 step:7200 train loss: 0.566, r2: 0.036, rate:0.812, acc:0.938 val loss: 0.881, r2: -0.143, rate:0.938, acc:1.000 lr 9.534187405839861e-08\n",
      "epoch:23 step:7300 train loss: 1.002, r2: -0.096, rate:0.625, acc:0.438 val loss: 0.766, r2: -0.773, rate:0.625, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:23 step:7400 train loss: 1.148, r2: 0.027, rate:0.625, acc:0.312 val loss: 1.460, r2: 0.160, rate:0.688, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:23 step:7500 train loss: 0.635, r2: -0.024, rate:0.938, acc:0.688 val loss: 1.001, r2: 0.064, rate:0.688, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:23 step:7600 train loss: 0.778, r2: 0.699, rate:0.750, acc:0.625 val loss: 0.652, r2: 0.741, rate:0.938, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:23 step:7700 train loss: 1.082, r2: -0.200, rate:0.312, acc:0.438 val loss: 0.907, r2: 0.341, rate:0.875, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:23 step:7800 train loss: 0.695, r2: -0.446, rate:0.625, acc:0.750 val loss: 0.874, r2: 0.434, rate:0.812, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:23 step:0.327 lr 0.000000\n",
      "epoch:24 step:0 train loss: 0.561, r2: -0.166, rate:0.938, acc:0.750 val loss: 0.743, r2: 0.291, rate:0.812, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:24 step:100 train loss: 1.239, r2: -0.058, rate:0.688, acc:0.688 val loss: 2.580, r2: 0.344, rate:0.562, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:24 step:200 train loss: 2.520, r2: -1.297, rate:0.875, acc:0.750 val loss: 0.552, r2: 0.722, rate:0.750, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:24 step:300 train loss: 0.849, r2: 0.502, rate:0.750, acc:0.750 val loss: 1.756, r2: 0.284, rate:0.688, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:24 step:400 train loss: 1.471, r2: 0.318, rate:0.875, acc:0.938 val loss: 0.820, r2: 0.289, rate:0.938, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:24 step:500 train loss: 0.993, r2: -0.292, rate:0.688, acc:0.438 val loss: 0.716, r2: 0.157, rate:0.688, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:24 step:600 train loss: 1.104, r2: -0.632, rate:0.500, acc:0.750 val loss: 0.778, r2: -2.008, rate:0.562, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:24 step:700 train loss: 0.823, r2: -0.320, rate:0.812, acc:0.938 val loss: 0.557, r2: 0.487, rate:0.875, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:24 step:800 train loss: 1.360, r2: -0.866, rate:0.375, acc:0.312 val loss: 1.171, r2: 0.300, rate:0.625, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:24 step:900 train loss: 0.699, r2: -0.137, rate:0.750, acc:0.500 val loss: 2.645, r2: 0.041, rate:0.562, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:24 step:1000 train loss: 0.768, r2: 0.071, rate:0.750, acc:0.688 val loss: 0.946, r2: -1.468, rate:0.438, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:24 step:1100 train loss: 1.156, r2: 0.523, rate:0.812, acc:0.562 val loss: 0.789, r2: 0.465, rate:0.625, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:24 step:1200 train loss: 1.328, r2: -10.327, rate:0.688, acc:0.375 val loss: 1.096, r2: -0.097, rate:0.688, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:24 step:1300 train loss: 0.751, r2: 0.451, rate:0.812, acc:0.750 val loss: 1.106, r2: -0.782, rate:0.688, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:24 step:1400 train loss: 0.801, r2: -0.041, rate:0.438, acc:0.750 val loss: 0.600, r2: 0.471, rate:0.750, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:24 step:1500 train loss: 0.516, r2: -0.626, rate:1.000, acc:0.562 val loss: 0.735, r2: 0.461, rate:0.812, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:24 step:1600 train loss: 0.882, r2: -0.986, rate:0.625, acc:0.500 val loss: 0.874, r2: -0.839, rate:0.688, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:24 step:1700 train loss: 0.814, r2: 0.467, rate:0.688, acc:0.688 val loss: 0.882, r2: 0.115, rate:0.750, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:24 step:1800 train loss: 1.422, r2: -3.089, rate:0.750, acc:0.938 val loss: 0.924, r2: -0.227, rate:0.375, acc:0.812 lr 9.534187405839861e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:1900 train loss: 1.027, r2: -0.076, rate:0.500, acc:0.625 val loss: 1.133, r2: 0.345, rate:0.625, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:24 step:2000 train loss: 1.846, r2: -2.108, rate:0.562, acc:0.188 val loss: 0.836, r2: -0.403, rate:0.625, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:24 step:2100 train loss: 0.661, r2: 0.168, rate:0.875, acc:0.812 val loss: 1.096, r2: 0.402, rate:0.812, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:24 step:2200 train loss: 0.791, r2: 0.727, rate:0.625, acc:0.500 val loss: 1.282, r2: -0.288, rate:0.688, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:24 step:2300 train loss: 0.781, r2: -0.161, rate:0.562, acc:0.500 val loss: 0.936, r2: -1.255, rate:0.562, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:24 step:2400 train loss: 0.411, r2: 0.820, rate:1.000, acc:0.812 val loss: 1.649, r2: -0.788, rate:0.688, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:24 step:2500 train loss: 0.757, r2: -2.008, rate:0.875, acc:0.812 val loss: 0.697, r2: 0.175, rate:0.750, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:24 step:2600 train loss: 0.712, r2: 0.443, rate:0.625, acc:0.812 val loss: 1.254, r2: 0.007, rate:0.812, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:24 step:2700 train loss: 0.520, r2: 0.771, rate:0.938, acc:0.812 val loss: 0.937, r2: -0.363, rate:0.875, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:24 step:2800 train loss: 0.802, r2: -3.363, rate:0.562, acc:0.625 val loss: 1.140, r2: -0.547, rate:0.562, acc:0.375 lr 9.534187405839861e-08\n",
      "epoch:24 step:2900 train loss: 0.528, r2: -0.432, rate:0.875, acc:1.000 val loss: 2.372, r2: -1.760, rate:0.625, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:24 step:3000 train loss: 0.640, r2: 0.433, rate:0.812, acc:0.812 val loss: 1.547, r2: -1.411, rate:0.562, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:24 step:3100 train loss: 0.670, r2: 0.177, rate:0.688, acc:0.688 val loss: 0.671, r2: 0.402, rate:0.875, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:24 step:3200 train loss: 1.603, r2: -0.726, rate:0.562, acc:0.750 val loss: 0.739, r2: 0.001, rate:0.750, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:24 step:3300 train loss: 0.658, r2: -0.476, rate:0.750, acc:0.750 val loss: 1.868, r2: 0.278, rate:0.688, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:24 step:3400 train loss: 0.714, r2: -0.149, rate:0.688, acc:0.562 val loss: 0.961, r2: 0.063, rate:0.750, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:24 step:3500 train loss: 0.919, r2: -4.231, rate:0.688, acc:0.375 val loss: 2.138, r2: -1.047, rate:0.438, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:24 step:3600 train loss: 0.707, r2: -0.724, rate:0.750, acc:0.625 val loss: 1.780, r2: 0.142, rate:0.500, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:24 step:3700 train loss: 1.236, r2: -1.887, rate:0.562, acc:0.375 val loss: 0.923, r2: 0.082, rate:0.688, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:24 step:3800 train loss: 0.714, r2: 0.343, rate:0.875, acc:0.688 val loss: 0.956, r2: 0.028, rate:0.625, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:24 step:3900 train loss: 0.607, r2: 0.900, rate:0.750, acc:0.438 val loss: 0.747, r2: 0.661, rate:0.750, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:24 step:4000 train loss: 0.893, r2: -0.388, rate:0.562, acc:0.812 val loss: 0.929, r2: 0.224, rate:0.750, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:24 step:4100 train loss: 0.559, r2: -1.046, rate:0.812, acc:0.875 val loss: 2.481, r2: 0.035, rate:0.438, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:24 step:4200 train loss: 0.756, r2: -0.426, rate:0.812, acc:0.875 val loss: 0.916, r2: 0.212, rate:0.625, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:24 step:4300 train loss: 0.650, r2: 0.565, rate:0.688, acc:0.875 val loss: 1.333, r2: 0.488, rate:0.750, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:24 step:4400 train loss: 0.991, r2: -0.195, rate:0.250, acc:0.250 val loss: 2.320, r2: 0.119, rate:0.562, acc:0.312 lr 9.534187405839861e-08\n",
      "epoch:24 step:4500 train loss: 0.646, r2: 0.606, rate:0.875, acc:0.688 val loss: 0.902, r2: 0.190, rate:0.562, acc:0.375 lr 9.534187405839861e-08\n",
      "epoch:24 step:4600 train loss: 0.936, r2: -0.449, rate:0.750, acc:0.625 val loss: 0.888, r2: 0.192, rate:0.750, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:24 step:4700 train loss: 1.144, r2: -0.611, rate:0.438, acc:0.188 val loss: 1.694, r2: 0.225, rate:0.500, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:24 step:4800 train loss: 0.761, r2: -0.635, rate:0.750, acc:0.438 val loss: 0.895, r2: -0.001, rate:0.688, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:24 step:4900 train loss: 0.912, r2: 0.061, rate:0.750, acc:0.750 val loss: 1.005, r2: -0.116, rate:0.500, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:24 step:5000 train loss: 0.965, r2: -3.279, rate:0.562, acc:0.688 val loss: 1.255, r2: -0.134, rate:0.438, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:24 step:5100 train loss: 0.800, r2: 0.454, rate:0.625, acc:0.562 val loss: 0.988, r2: 0.242, rate:0.875, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:24 step:5200 train loss: 0.850, r2: -1.523, rate:0.375, acc:0.500 val loss: 1.294, r2: 0.204, rate:0.625, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:24 step:5300 train loss: 0.666, r2: 0.172, rate:0.750, acc:0.875 val loss: 0.612, r2: 0.452, rate:0.812, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:24 step:5400 train loss: 0.604, r2: -1.239, rate:0.750, acc:0.875 val loss: 0.971, r2: 0.459, rate:0.688, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:24 step:5500 train loss: 1.004, r2: -3.629, rate:0.812, acc:0.375 val loss: 1.044, r2: -0.378, rate:0.625, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:24 step:5600 train loss: 0.938, r2: 0.377, rate:0.625, acc:0.688 val loss: 1.422, r2: -0.663, rate:0.438, acc:0.375 lr 9.534187405839861e-08\n",
      "epoch:24 step:5700 train loss: 0.604, r2: 0.408, rate:0.875, acc:0.875 val loss: 0.754, r2: 0.382, rate:0.562, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:24 step:5800 train loss: 0.942, r2: 0.554, rate:0.562, acc:0.812 val loss: 1.740, r2: -0.774, rate:0.688, acc:0.375 lr 9.534187405839861e-08\n",
      "epoch:24 step:5900 train loss: 1.186, r2: -0.025, rate:0.688, acc:0.500 val loss: 0.683, r2: 0.613, rate:0.750, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:24 step:6000 train loss: 0.568, r2: 0.634, rate:0.812, acc:0.688 val loss: 0.686, r2: 0.511, rate:0.938, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:24 step:6100 train loss: 1.185, r2: -0.298, rate:0.750, acc:0.438 val loss: 0.624, r2: -0.377, rate:0.938, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:24 step:6200 train loss: 1.619, r2: -1.213, rate:0.750, acc:0.875 val loss: 1.915, r2: -1.436, rate:0.438, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:24 step:6300 train loss: 1.041, r2: -4.714, rate:0.312, acc:0.375 val loss: 1.079, r2: -0.034, rate:0.312, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:24 step:6400 train loss: 1.258, r2: -2.387, rate:0.500, acc:0.375 val loss: 0.872, r2: 0.301, rate:0.812, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:24 step:6500 train loss: 1.189, r2: 0.057, rate:0.500, acc:0.750 val loss: 0.536, r2: 0.681, rate:0.938, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:24 step:6600 train loss: 0.652, r2: -7.841, rate:0.625, acc:0.812 val loss: 2.053, r2: -0.203, rate:0.688, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:24 step:6700 train loss: 1.112, r2: -1.978, rate:0.688, acc:0.312 val loss: 1.058, r2: 0.445, rate:0.812, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:24 step:6800 train loss: 0.701, r2: -0.103, rate:0.750, acc:0.562 val loss: 0.733, r2: 0.361, rate:0.750, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:24 step:6900 train loss: 0.909, r2: -0.530, rate:0.500, acc:0.250 val loss: 1.058, r2: -0.375, rate:0.562, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:24 step:7000 train loss: 0.930, r2: -0.668, rate:0.500, acc:0.500 val loss: 0.737, r2: -0.125, rate:0.750, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:24 step:7100 train loss: 1.271, r2: -3.802, rate:0.750, acc:0.438 val loss: 1.005, r2: -0.628, rate:0.750, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:24 step:7200 train loss: 0.553, r2: 0.474, rate:0.875, acc:0.625 val loss: 1.262, r2: -0.086, rate:0.688, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:24 step:7300 train loss: 0.802, r2: 0.342, rate:0.688, acc:0.688 val loss: 0.621, r2: 0.773, rate:0.688, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:24 step:7400 train loss: 1.679, r2: 0.135, rate:0.750, acc:0.875 val loss: 1.349, r2: 0.133, rate:0.812, acc:0.500 lr 9.534187405839861e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:7500 train loss: 0.995, r2: -1.029, rate:0.500, acc:0.500 val loss: 1.244, r2: 0.034, rate:0.500, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:24 step:7600 train loss: 0.842, r2: 0.624, rate:0.812, acc:0.750 val loss: 1.257, r2: -0.300, rate:0.875, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:24 step:7700 train loss: 1.387, r2: -0.858, rate:0.688, acc:0.812 val loss: 1.591, r2: -0.088, rate:0.688, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:24 step:7800 train loss: 0.662, r2: -0.679, rate:0.750, acc:0.812 val loss: 1.071, r2: -1.176, rate:0.375, acc:0.125 lr 9.534187405839861e-08\n",
      "epoch:24 step:0.327 lr 0.000000\n",
      "epoch:25 step:0 train loss: 0.579, r2: 0.471, rate:0.750, acc:0.938 val loss: 0.692, r2: 0.572, rate:0.750, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:25 step:100 train loss: 0.802, r2: 0.600, rate:0.688, acc:0.562 val loss: 0.904, r2: 0.303, rate:0.875, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:25 step:200 train loss: 0.909, r2: -1.101, rate:0.500, acc:0.312 val loss: 0.972, r2: -0.229, rate:0.812, acc:0.375 lr 9.534187405839861e-08\n",
      "epoch:25 step:300 train loss: 2.931, r2: -10.686, rate:0.625, acc:0.375 val loss: 0.856, r2: 0.135, rate:0.562, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:25 step:400 train loss: 0.574, r2: 0.571, rate:0.938, acc:0.812 val loss: 0.768, r2: -0.253, rate:0.625, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:25 step:500 train loss: 1.221, r2: -2.098, rate:0.438, acc:0.438 val loss: 0.675, r2: 0.655, rate:0.812, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:25 step:600 train loss: 0.627, r2: -0.550, rate:0.688, acc:0.875 val loss: 1.006, r2: 0.499, rate:0.750, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:25 step:700 train loss: 0.778, r2: 0.660, rate:0.750, acc:0.625 val loss: 0.742, r2: -0.211, rate:0.625, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:25 step:800 train loss: 0.723, r2: 0.148, rate:0.750, acc:0.438 val loss: 1.719, r2: 0.073, rate:0.625, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:25 step:900 train loss: 0.750, r2: -3.882, rate:0.562, acc:1.000 val loss: 0.868, r2: 0.204, rate:0.812, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:25 step:1000 train loss: 0.691, r2: 0.430, rate:0.375, acc:0.438 val loss: 1.035, r2: -1.609, rate:0.625, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:25 step:1100 train loss: 0.988, r2: 0.595, rate:0.625, acc:0.562 val loss: 0.652, r2: -0.126, rate:0.688, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:25 step:1200 train loss: 0.834, r2: 0.650, rate:0.875, acc:0.875 val loss: 1.128, r2: -1.125, rate:0.625, acc:0.312 lr 9.534187405839861e-08\n",
      "epoch:25 step:1300 train loss: 1.142, r2: 0.071, rate:0.688, acc:0.812 val loss: 1.022, r2: -0.362, rate:0.875, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:25 step:1400 train loss: 0.677, r2: 0.025, rate:0.688, acc:0.562 val loss: 2.353, r2: -1.149, rate:0.500, acc:0.375 lr 9.534187405839861e-08\n",
      "epoch:25 step:1500 train loss: 0.707, r2: -1.770, rate:0.688, acc:0.750 val loss: 1.724, r2: 0.384, rate:0.750, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:25 step:1600 train loss: 0.629, r2: 0.457, rate:0.750, acc:0.625 val loss: 0.773, r2: 0.320, rate:0.812, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:25 step:1700 train loss: 0.761, r2: -0.083, rate:0.500, acc:0.562 val loss: 0.803, r2: 0.497, rate:0.625, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:25 step:1800 train loss: 0.566, r2: 0.806, rate:0.688, acc:0.812 val loss: 0.867, r2: 0.118, rate:0.562, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:25 step:1900 train loss: 0.761, r2: -0.685, rate:0.750, acc:0.812 val loss: 1.713, r2: 0.495, rate:0.875, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:25 step:2000 train loss: 1.477, r2: 0.344, rate:0.312, acc:0.250 val loss: 0.886, r2: 0.155, rate:0.750, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:25 step:2100 train loss: 1.220, r2: 0.359, rate:0.812, acc:0.812 val loss: 1.089, r2: -1.253, rate:0.688, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:25 step:2200 train loss: 0.726, r2: 0.159, rate:0.688, acc:0.750 val loss: 1.057, r2: -0.358, rate:0.500, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:25 step:2300 train loss: 1.059, r2: -1.206, rate:0.875, acc:0.938 val loss: 0.737, r2: 0.361, rate:0.750, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:25 step:2400 train loss: 0.711, r2: 0.449, rate:0.562, acc:0.875 val loss: 0.776, r2: -0.066, rate:0.938, acc:1.000 lr 9.534187405839861e-08\n",
      "epoch:25 step:2500 train loss: 0.637, r2: 0.049, rate:0.625, acc:0.812 val loss: 0.990, r2: 0.198, rate:0.875, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:25 step:2600 train loss: 0.811, r2: -0.346, rate:0.875, acc:1.000 val loss: 1.210, r2: -0.885, rate:0.688, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:25 step:2700 train loss: 0.840, r2: 0.216, rate:0.812, acc:0.750 val loss: 0.857, r2: 0.565, rate:0.562, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:25 step:2800 train loss: 0.929, r2: -0.696, rate:0.625, acc:0.562 val loss: 0.735, r2: 0.199, rate:0.562, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:25 step:2900 train loss: 0.906, r2: 0.086, rate:0.625, acc:0.500 val loss: 0.811, r2: -0.490, rate:0.500, acc:0.375 lr 9.534187405839861e-08\n",
      "epoch:25 step:3000 train loss: 0.722, r2: -0.232, rate:0.688, acc:0.875 val loss: 0.626, r2: 0.278, rate:0.812, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:25 step:3100 train loss: 0.814, r2: -0.403, rate:0.438, acc:0.438 val loss: 1.856, r2: 0.165, rate:0.688, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:25 step:3200 train loss: 0.886, r2: 0.060, rate:0.562, acc:0.500 val loss: 0.918, r2: 0.300, rate:0.562, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:25 step:3300 train loss: 0.788, r2: -0.814, rate:0.500, acc:0.312 val loss: 1.500, r2: -0.018, rate:0.562, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:25 step:3400 train loss: 0.696, r2: 0.375, rate:0.688, acc:0.812 val loss: 0.788, r2: 0.043, rate:0.625, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:25 step:3500 train loss: 1.077, r2: 0.032, rate:0.562, acc:0.562 val loss: 0.659, r2: 0.632, rate:0.750, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:25 step:3600 train loss: 2.283, r2: -3.994, rate:0.562, acc:0.250 val loss: 0.958, r2: 0.307, rate:0.688, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:25 step:3700 train loss: 0.915, r2: -0.468, rate:0.562, acc:0.562 val loss: 2.597, r2: 0.312, rate:0.812, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:25 step:3800 train loss: 0.889, r2: 0.261, rate:0.688, acc:0.750 val loss: 0.897, r2: -1.514, rate:0.688, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:25 step:3900 train loss: 1.170, r2: -0.601, rate:0.375, acc:0.375 val loss: 2.881, r2: -3.596, rate:0.625, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:25 step:4000 train loss: 5.046, r2: -9.257, rate:0.625, acc:0.312 val loss: 1.289, r2: -0.287, rate:0.750, acc:1.000 lr 9.534187405839861e-08\n",
      "epoch:25 step:4100 train loss: 1.696, r2: -3.045, rate:0.562, acc:0.875 val loss: 3.619, r2: -0.048, rate:0.812, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:25 step:4200 train loss: 0.800, r2: 0.018, rate:0.562, acc:0.500 val loss: 0.648, r2: 0.163, rate:0.688, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:25 step:4300 train loss: 1.173, r2: -0.041, rate:0.188, acc:0.312 val loss: 2.096, r2: 0.241, rate:0.625, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:25 step:4400 train loss: 1.032, r2: -0.899, rate:0.500, acc:0.312 val loss: 1.415, r2: -1.881, rate:0.375, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:25 step:4500 train loss: 1.101, r2: 0.601, rate:0.812, acc:0.875 val loss: 0.763, r2: -2.024, rate:0.625, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:25 step:4600 train loss: 1.380, r2: -5.716, rate:0.812, acc:1.000 val loss: 1.080, r2: -0.549, rate:0.812, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:25 step:4700 train loss: 0.804, r2: -0.846, rate:0.562, acc:0.625 val loss: 0.901, r2: -1.985, rate:0.500, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:25 step:4800 train loss: 0.780, r2: 0.287, rate:0.688, acc:0.375 val loss: 0.751, r2: -0.997, rate:0.750, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:25 step:4900 train loss: 1.192, r2: -1.481, rate:0.750, acc:1.000 val loss: 0.857, r2: 0.045, rate:0.750, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:25 step:5000 train loss: 1.396, r2: -1.386, rate:0.688, acc:0.500 val loss: 1.643, r2: 0.043, rate:0.562, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:25 step:5100 train loss: 0.846, r2: -1.188, rate:0.438, acc:0.375 val loss: 0.821, r2: -0.215, rate:0.438, acc:0.500 lr 9.534187405839861e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:5200 train loss: 0.724, r2: -0.903, rate:0.500, acc:0.438 val loss: 1.103, r2: -0.139, rate:0.875, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:25 step:5300 train loss: 0.571, r2: -0.659, rate:0.812, acc:0.875 val loss: 0.506, r2: 0.489, rate:0.875, acc:1.000 lr 9.534187405839861e-08\n",
      "epoch:25 step:5400 train loss: 1.114, r2: 0.096, rate:0.875, acc:0.625 val loss: 0.673, r2: 0.368, rate:0.812, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:25 step:5500 train loss: 1.185, r2: 0.288, rate:0.688, acc:0.812 val loss: 1.061, r2: 0.436, rate:0.812, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:25 step:5600 train loss: 1.280, r2: 0.254, rate:0.812, acc:0.688 val loss: 0.964, r2: 0.325, rate:0.625, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:25 step:5700 train loss: 0.636, r2: 0.278, rate:0.875, acc:0.812 val loss: 0.952, r2: -0.473, rate:0.438, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:25 step:5800 train loss: 1.240, r2: -0.943, rate:0.688, acc:0.250 val loss: 0.763, r2: -0.632, rate:0.812, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:25 step:5900 train loss: 0.750, r2: 0.570, rate:0.750, acc:0.688 val loss: 1.271, r2: -1.709, rate:0.562, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:25 step:6000 train loss: 0.895, r2: -1.924, rate:0.438, acc:1.000 val loss: 0.948, r2: 0.124, rate:0.688, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:25 step:6100 train loss: 0.791, r2: 0.648, rate:0.750, acc:0.875 val loss: 0.928, r2: 0.309, rate:0.750, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:25 step:6200 train loss: 1.413, r2: -1.506, rate:0.625, acc:0.938 val loss: 1.029, r2: -1.070, rate:0.500, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:25 step:6300 train loss: 0.812, r2: -1.195, rate:0.688, acc:0.938 val loss: 0.685, r2: 0.559, rate:0.688, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:25 step:6400 train loss: 0.622, r2: -0.130, rate:0.750, acc:0.875 val loss: 0.884, r2: 0.510, rate:0.812, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:25 step:6500 train loss: 0.890, r2: -0.712, rate:0.812, acc:0.562 val loss: 2.314, r2: 0.214, rate:0.625, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:25 step:6600 train loss: 0.971, r2: 0.037, rate:0.750, acc:0.812 val loss: 0.958, r2: -1.780, rate:0.688, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:25 step:6700 train loss: 1.306, r2: -0.288, rate:0.688, acc:0.312 val loss: 1.055, r2: 0.098, rate:0.625, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:25 step:6800 train loss: 0.771, r2: -0.490, rate:0.812, acc:0.812 val loss: 0.816, r2: 0.036, rate:0.688, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:25 step:6900 train loss: 0.824, r2: -0.232, rate:0.688, acc:0.500 val loss: 0.811, r2: -0.166, rate:0.688, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:25 step:7000 train loss: 0.679, r2: 0.131, rate:0.688, acc:0.562 val loss: 0.624, r2: 0.553, rate:0.688, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:25 step:7100 train loss: 0.983, r2: 0.221, rate:0.500, acc:0.500 val loss: 2.271, r2: -3.849, rate:0.750, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:25 step:7200 train loss: 0.794, r2: 0.090, rate:0.625, acc:0.625 val loss: 0.917, r2: 0.021, rate:0.438, acc:0.312 lr 9.534187405839861e-08\n",
      "epoch:25 step:7300 train loss: 0.840, r2: 0.244, rate:0.750, acc:0.438 val loss: 0.744, r2: 0.227, rate:0.688, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:25 step:7400 train loss: 1.325, r2: 0.371, rate:0.625, acc:0.438 val loss: 0.777, r2: -1.472, rate:0.500, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:25 step:7500 train loss: 0.755, r2: -0.258, rate:0.750, acc:0.562 val loss: 0.792, r2: 0.318, rate:0.812, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:25 step:7600 train loss: 1.262, r2: -0.172, rate:0.875, acc:0.812 val loss: 0.870, r2: -0.045, rate:0.625, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:25 step:7700 train loss: 2.215, r2: -4.553, rate:0.625, acc:0.500 val loss: 0.837, r2: 0.446, rate:0.812, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:25 step:7800 train loss: 0.612, r2: -0.660, rate:0.812, acc:0.938 val loss: 0.963, r2: -0.477, rate:0.562, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:25 step:0.327 lr 0.000000\n",
      "epoch:26 step:0 train loss: 5.344, r2: -5.255, rate:0.688, acc:0.625 val loss: 0.603, r2: 0.714, rate:0.750, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:26 step:100 train loss: 0.665, r2: 0.585, rate:0.875, acc:0.750 val loss: 1.634, r2: -1.878, rate:0.625, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:26 step:200 train loss: 0.675, r2: 0.054, rate:0.750, acc:0.438 val loss: 0.632, r2: -0.255, rate:0.812, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:26 step:300 train loss: 1.948, r2: -0.123, rate:0.750, acc:0.625 val loss: 0.956, r2: 0.275, rate:0.812, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:26 step:400 train loss: 0.929, r2: -0.207, rate:0.688, acc:0.875 val loss: 0.614, r2: 0.563, rate:0.875, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:26 step:500 train loss: 1.038, r2: 0.210, rate:0.875, acc:0.875 val loss: 1.762, r2: -0.736, rate:0.625, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:26 step:600 train loss: 0.499, r2: -0.969, rate:0.812, acc:0.938 val loss: 0.915, r2: 0.373, rate:0.250, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:26 step:700 train loss: 0.745, r2: -3.796, rate:0.562, acc:0.375 val loss: 2.023, r2: 0.391, rate:0.688, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:26 step:800 train loss: 0.656, r2: -0.642, rate:0.938, acc:0.312 val loss: 0.848, r2: 0.268, rate:0.938, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:26 step:900 train loss: 8.757, r2: -5.890, rate:0.750, acc:0.875 val loss: 1.640, r2: 0.417, rate:1.000, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:26 step:1000 train loss: 0.931, r2: -0.661, rate:0.562, acc:0.375 val loss: 0.953, r2: -4.175, rate:0.438, acc:0.375 lr 9.534187405839861e-08\n",
      "epoch:26 step:1100 train loss: 0.981, r2: -1.036, rate:0.750, acc:0.438 val loss: 0.963, r2: 0.123, rate:0.625, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:26 step:1200 train loss: 1.096, r2: -2.946, rate:0.500, acc:0.625 val loss: 1.075, r2: -0.155, rate:0.688, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:26 step:1300 train loss: 1.125, r2: -0.611, rate:0.938, acc:0.688 val loss: 1.528, r2: -0.900, rate:0.500, acc:0.250 lr 9.534187405839861e-08\n",
      "epoch:26 step:1400 train loss: 1.962, r2: -3.323, rate:0.312, acc:0.250 val loss: 2.727, r2: -0.108, rate:0.500, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:26 step:1500 train loss: 0.944, r2: -1.464, rate:0.438, acc:0.312 val loss: 1.035, r2: -0.022, rate:0.562, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:26 step:1600 train loss: 0.703, r2: -0.885, rate:0.688, acc:0.500 val loss: 0.672, r2: 0.598, rate:0.562, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:26 step:1700 train loss: 0.641, r2: 0.454, rate:0.812, acc:0.750 val loss: 0.566, r2: 0.632, rate:0.875, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:26 step:1800 train loss: 0.915, r2: -0.830, rate:0.562, acc:0.938 val loss: 1.095, r2: -0.758, rate:0.812, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:26 step:1900 train loss: 0.807, r2: 0.330, rate:0.625, acc:0.625 val loss: 0.943, r2: 0.110, rate:0.562, acc:0.375 lr 9.534187405839861e-08\n",
      "epoch:26 step:2000 train loss: 2.653, r2: -0.196, rate:0.875, acc:0.875 val loss: 0.563, r2: -1.937, rate:0.938, acc:1.000 lr 9.534187405839861e-08\n",
      "epoch:26 step:2100 train loss: 0.600, r2: -0.658, rate:0.938, acc:0.938 val loss: 0.611, r2: -1.135, rate:0.812, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:26 step:2200 train loss: 1.226, r2: -0.117, rate:0.688, acc:0.875 val loss: 1.001, r2: -0.228, rate:0.562, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:26 step:2300 train loss: 1.505, r2: -1.203, rate:0.562, acc:0.500 val loss: 1.026, r2: 0.479, rate:0.938, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:26 step:2400 train loss: 0.749, r2: -0.707, rate:0.438, acc:0.250 val loss: 0.903, r2: 0.366, rate:0.875, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:26 step:2500 train loss: 0.726, r2: -4.372, rate:0.500, acc:0.812 val loss: 1.707, r2: 0.048, rate:0.625, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:26 step:2600 train loss: 0.710, r2: 0.250, rate:0.562, acc:0.625 val loss: 1.425, r2: -0.440, rate:0.688, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:26 step:2700 train loss: 0.710, r2: 0.148, rate:0.562, acc:0.562 val loss: 0.948, r2: -3.858, rate:0.562, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:26 step:2800 train loss: 4.277, r2: -23.806, rate:1.000, acc:1.000 val loss: 0.634, r2: 0.634, rate:0.750, acc:0.812 lr 9.534187405839861e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:2900 train loss: 0.894, r2: 0.106, rate:0.812, acc:0.562 val loss: 1.759, r2: -0.771, rate:0.562, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:26 step:3000 train loss: 0.618, r2: 0.475, rate:0.812, acc:0.688 val loss: 1.020, r2: -0.754, rate:0.500, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:26 step:3100 train loss: 1.365, r2: 0.196, rate:0.812, acc:0.500 val loss: 0.930, r2: 0.387, rate:0.625, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:26 step:3200 train loss: 0.767, r2: -2.278, rate:0.750, acc:0.250 val loss: 1.010, r2: -0.443, rate:0.375, acc:0.312 lr 9.534187405839861e-08\n",
      "epoch:26 step:3300 train loss: 0.664, r2: -0.065, rate:0.875, acc:0.562 val loss: 0.995, r2: -0.425, rate:0.625, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:26 step:3400 train loss: 0.789, r2: -0.765, rate:0.625, acc:0.312 val loss: 0.804, r2: 0.404, rate:0.688, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:26 step:3500 train loss: 0.727, r2: -0.808, rate:0.688, acc:0.250 val loss: 0.819, r2: -0.305, rate:0.562, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:26 step:3600 train loss: 2.118, r2: -2.998, rate:0.438, acc:0.688 val loss: 0.573, r2: 0.473, rate:0.812, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:26 step:3700 train loss: 0.980, r2: -0.438, rate:0.625, acc:0.750 val loss: 0.906, r2: 0.051, rate:0.750, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:26 step:3800 train loss: 1.245, r2: -0.491, rate:0.812, acc:0.812 val loss: 1.037, r2: -0.117, rate:0.625, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:26 step:3900 train loss: 1.175, r2: 0.262, rate:0.688, acc:0.688 val loss: 1.301, r2: -3.448, rate:0.688, acc:0.188 lr 9.534187405839861e-08\n",
      "epoch:26 step:4000 train loss: 0.779, r2: -0.357, rate:0.562, acc:0.500 val loss: 1.594, r2: 0.192, rate:0.625, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:26 step:4100 train loss: 1.265, r2: 0.211, rate:0.750, acc:0.312 val loss: 1.167, r2: -0.051, rate:0.750, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:26 step:4200 train loss: 0.607, r2: 0.838, rate:0.875, acc:0.562 val loss: 0.557, r2: 0.693, rate:0.875, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:26 step:4300 train loss: 0.941, r2: -1.107, rate:0.438, acc:0.812 val loss: 0.544, r2: 0.714, rate:0.812, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:26 step:4400 train loss: 0.785, r2: 0.640, rate:0.625, acc:0.500 val loss: 0.746, r2: -0.312, rate:0.625, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:26 step:4500 train loss: 2.440, r2: -0.599, rate:0.625, acc:0.375 val loss: 2.376, r2: -4.300, rate:0.625, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:26 step:4600 train loss: 1.147, r2: -1.090, rate:0.750, acc:0.562 val loss: 0.504, r2: 0.708, rate:0.812, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:26 step:4700 train loss: 1.001, r2: -0.992, rate:0.562, acc:0.500 val loss: 0.765, r2: 0.287, rate:0.688, acc:0.375 lr 9.534187405839861e-08\n",
      "epoch:26 step:4800 train loss: 0.512, r2: -0.115, rate:0.938, acc:1.000 val loss: 0.984, r2: -4.541, rate:0.812, acc:1.000 lr 9.534187405839861e-08\n",
      "epoch:26 step:4900 train loss: 1.488, r2: 0.517, rate:0.625, acc:0.750 val loss: 0.726, r2: 0.614, rate:0.750, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:26 step:5000 train loss: 0.927, r2: -4.228, rate:0.562, acc:0.750 val loss: 1.797, r2: -0.440, rate:0.688, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:26 step:5100 train loss: 0.996, r2: -1.965, rate:0.625, acc:0.375 val loss: 1.218, r2: -0.173, rate:0.562, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:26 step:5200 train loss: 1.137, r2: 0.367, rate:0.562, acc:0.812 val loss: 0.827, r2: 0.732, rate:0.938, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:26 step:5300 train loss: 0.756, r2: -0.085, rate:0.312, acc:0.625 val loss: 0.792, r2: 0.512, rate:0.750, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:26 step:5400 train loss: 0.782, r2: 0.187, rate:0.688, acc:0.875 val loss: 0.702, r2: -0.479, rate:0.688, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:26 step:5500 train loss: 0.755, r2: 0.099, rate:0.438, acc:0.375 val loss: 1.334, r2: 0.378, rate:0.625, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:26 step:5600 train loss: 0.862, r2: 0.414, rate:0.750, acc:0.562 val loss: 0.781, r2: 0.147, rate:0.812, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:26 step:5700 train loss: 1.072, r2: -0.105, rate:0.625, acc:0.625 val loss: 1.163, r2: -2.948, rate:0.562, acc:0.375 lr 9.534187405839861e-08\n",
      "epoch:26 step:5800 train loss: 0.740, r2: 0.286, rate:0.625, acc:0.562 val loss: 0.680, r2: -0.144, rate:0.812, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:26 step:5900 train loss: 1.311, r2: -3.950, rate:0.562, acc:0.250 val loss: 1.575, r2: -2.985, rate:0.688, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:26 step:6000 train loss: 1.389, r2: -0.918, rate:0.375, acc:0.312 val loss: 1.002, r2: 0.304, rate:0.688, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:26 step:6100 train loss: 0.889, r2: -1.004, rate:0.688, acc:0.375 val loss: 0.651, r2: 0.597, rate:0.812, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:26 step:6200 train loss: 1.033, r2: -0.853, rate:0.562, acc:0.375 val loss: 0.837, r2: 0.205, rate:0.562, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:26 step:6300 train loss: 1.088, r2: -1.678, rate:0.562, acc:0.188 val loss: 2.377, r2: 0.439, rate:0.812, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:26 step:6400 train loss: 0.860, r2: 0.553, rate:0.812, acc:0.500 val loss: 0.652, r2: 0.213, rate:0.562, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:26 step:6500 train loss: 0.528, r2: 0.020, rate:0.812, acc:0.938 val loss: 0.929, r2: -0.023, rate:0.625, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:26 step:6600 train loss: 0.715, r2: 0.098, rate:0.750, acc:0.688 val loss: 0.764, r2: 0.314, rate:0.750, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:26 step:6700 train loss: 1.036, r2: -0.212, rate:0.750, acc:0.500 val loss: 0.815, r2: 0.262, rate:0.812, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:26 step:6800 train loss: 1.242, r2: -0.096, rate:0.750, acc:0.750 val loss: 1.112, r2: -1.435, rate:0.438, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:26 step:6900 train loss: 0.747, r2: 0.148, rate:0.438, acc:0.438 val loss: 1.164, r2: 0.164, rate:0.625, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:26 step:7000 train loss: 1.349, r2: 0.520, rate:0.938, acc:0.938 val loss: 1.082, r2: 0.360, rate:0.688, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:26 step:7100 train loss: 0.770, r2: -2.592, rate:0.562, acc:0.750 val loss: 0.798, r2: -0.968, rate:0.625, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:26 step:7200 train loss: 2.445, r2: -0.409, rate:0.312, acc:0.875 val loss: 0.712, r2: 0.714, rate:0.750, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:26 step:7300 train loss: 0.662, r2: 0.373, rate:0.812, acc:0.688 val loss: 0.947, r2: 0.306, rate:0.688, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:26 step:7400 train loss: 2.392, r2: -14.539, rate:0.500, acc:0.250 val loss: 2.124, r2: -0.860, rate:0.625, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:26 step:7500 train loss: 1.982, r2: -8.259, rate:0.750, acc:0.250 val loss: 1.462, r2: -0.425, rate:0.375, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:26 step:7600 train loss: 0.549, r2: -2.284, rate:0.812, acc:0.875 val loss: 0.677, r2: 0.313, rate:0.812, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:26 step:7700 train loss: 0.853, r2: 0.186, rate:0.750, acc:0.750 val loss: 0.659, r2: 0.534, rate:0.875, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:26 step:7800 train loss: 1.505, r2: -1.385, rate:0.375, acc:0.188 val loss: 2.073, r2: -9.968, rate:0.625, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:26 step:0.327 lr 0.000000\n",
      "epoch:27 step:0 train loss: 3.433, r2: -2.294, rate:0.688, acc:0.812 val loss: 0.659, r2: 0.265, rate:0.812, acc:0.375 lr 9.534187405839861e-08\n",
      "epoch:27 step:100 train loss: 0.923, r2: -0.111, rate:0.562, acc:0.500 val loss: 0.877, r2: 0.326, rate:0.625, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:27 step:200 train loss: 0.935, r2: -0.392, rate:0.812, acc:0.875 val loss: 0.511, r2: 0.834, rate:1.000, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:27 step:300 train loss: 0.792, r2: 0.182, rate:0.438, acc:0.688 val loss: 1.239, r2: -0.458, rate:0.562, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:27 step:400 train loss: 0.870, r2: 0.155, rate:0.812, acc:0.812 val loss: 0.873, r2: 0.639, rate:0.812, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:27 step:500 train loss: 1.059, r2: 0.268, rate:0.688, acc:0.562 val loss: 0.809, r2: -0.095, rate:0.625, acc:0.625 lr 9.534187405839861e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:600 train loss: 0.706, r2: 0.016, rate:0.625, acc:0.812 val loss: 1.604, r2: -0.343, rate:0.625, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:27 step:700 train loss: 0.936, r2: -2.325, rate:0.438, acc:0.438 val loss: 0.852, r2: -0.874, rate:0.625, acc:0.375 lr 9.534187405839861e-08\n",
      "epoch:27 step:800 train loss: 0.650, r2: 0.452, rate:0.688, acc:0.688 val loss: 0.675, r2: 0.216, rate:0.875, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:27 step:900 train loss: 1.707, r2: -1.869, rate:0.438, acc:0.312 val loss: 0.797, r2: 0.455, rate:0.750, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:27 step:1000 train loss: 1.072, r2: 0.194, rate:0.625, acc:0.750 val loss: 0.692, r2: 0.286, rate:0.750, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:27 step:1100 train loss: 1.076, r2: -0.723, rate:0.500, acc:0.750 val loss: 0.688, r2: 0.522, rate:0.750, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:27 step:1200 train loss: 0.836, r2: -0.635, rate:0.688, acc:0.812 val loss: 1.328, r2: -0.875, rate:0.562, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:27 step:1300 train loss: 1.163, r2: 0.116, rate:0.625, acc:0.562 val loss: 0.990, r2: 0.649, rate:0.625, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:27 step:1400 train loss: 2.403, r2: -1.130, rate:0.688, acc:0.375 val loss: 1.207, r2: -3.885, rate:0.750, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:27 step:1500 train loss: 0.667, r2: -0.253, rate:0.625, acc:0.625 val loss: 0.565, r2: 0.718, rate:0.875, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:27 step:1600 train loss: 0.748, r2: -1.137, rate:0.625, acc:0.875 val loss: 1.063, r2: 0.234, rate:0.625, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:27 step:1700 train loss: 0.822, r2: 0.038, rate:0.500, acc:0.188 val loss: 3.549, r2: -1.962, rate:0.375, acc:0.312 lr 9.534187405839861e-08\n",
      "epoch:27 step:1800 train loss: 1.087, r2: -0.390, rate:0.500, acc:0.500 val loss: 0.545, r2: 0.696, rate:0.938, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:27 step:1900 train loss: 0.699, r2: -5.492, rate:0.750, acc:0.500 val loss: 0.947, r2: -0.132, rate:0.625, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:27 step:2000 train loss: 2.862, r2: -0.535, rate:0.875, acc:0.562 val loss: 1.054, r2: 0.071, rate:0.688, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:27 step:2100 train loss: 0.971, r2: -1.123, rate:0.938, acc:0.938 val loss: 0.800, r2: -0.211, rate:0.562, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:27 step:2200 train loss: 1.562, r2: -1.810, rate:0.312, acc:0.688 val loss: 0.773, r2: 0.674, rate:0.562, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:27 step:2300 train loss: 2.231, r2: -2.693, rate:0.438, acc:0.500 val loss: 0.634, r2: 0.447, rate:0.938, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:27 step:2400 train loss: 0.803, r2: -0.279, rate:0.562, acc:0.500 val loss: 0.693, r2: 0.518, rate:0.875, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:27 step:2500 train loss: 1.662, r2: -0.375, rate:0.812, acc:0.625 val loss: 1.523, r2: 0.172, rate:0.562, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:27 step:2600 train loss: 0.651, r2: -0.001, rate:0.812, acc:0.812 val loss: 0.643, r2: 0.482, rate:0.625, acc:0.375 lr 9.534187405839861e-08\n",
      "epoch:27 step:2700 train loss: 1.889, r2: -2.234, rate:0.938, acc:0.938 val loss: 0.716, r2: -0.252, rate:0.812, acc:1.000 lr 9.534187405839861e-08\n",
      "epoch:27 step:2800 train loss: 0.518, r2: 0.790, rate:0.875, acc:0.688 val loss: 1.055, r2: -0.654, rate:0.500, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:27 step:2900 train loss: 0.588, r2: 0.611, rate:0.750, acc:0.375 val loss: 1.376, r2: -0.347, rate:0.500, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:27 step:3000 train loss: 0.858, r2: -0.284, rate:0.750, acc:0.500 val loss: 1.264, r2: -0.153, rate:0.562, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:27 step:3100 train loss: 1.614, r2: 0.050, rate:0.625, acc:0.438 val loss: 0.945, r2: -7.201, rate:0.500, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:27 step:3200 train loss: 0.737, r2: -9.250, rate:0.562, acc:0.625 val loss: 0.635, r2: 0.065, rate:0.750, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:27 step:3300 train loss: 0.836, r2: -2.727, rate:0.750, acc:0.750 val loss: 0.664, r2: -0.140, rate:0.688, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:27 step:3400 train loss: 0.742, r2: -0.394, rate:0.875, acc:0.812 val loss: 0.856, r2: -0.077, rate:0.625, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:27 step:3500 train loss: 1.310, r2: -4.217, rate:0.750, acc:0.312 val loss: 1.616, r2: -5.183, rate:0.500, acc:0.312 lr 9.534187405839861e-08\n",
      "epoch:27 step:3600 train loss: 1.156, r2: -7.379, rate:0.438, acc:0.250 val loss: 1.338, r2: 0.410, rate:0.750, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:27 step:3700 train loss: 0.773, r2: -0.258, rate:0.750, acc:0.688 val loss: 1.617, r2: -1.154, rate:0.750, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:27 step:3800 train loss: 0.778, r2: 0.612, rate:0.688, acc:0.562 val loss: 0.631, r2: 0.662, rate:0.875, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:27 step:3900 train loss: 0.612, r2: -0.254, rate:0.875, acc:0.938 val loss: 0.880, r2: 0.261, rate:0.562, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:27 step:4000 train loss: 0.770, r2: 0.335, rate:0.812, acc:0.562 val loss: 0.816, r2: 0.181, rate:0.625, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:27 step:4100 train loss: 0.461, r2: -0.701, rate:1.000, acc:1.000 val loss: 0.743, r2: 0.511, rate:0.875, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:27 step:4200 train loss: 0.747, r2: -0.750, rate:0.625, acc:0.688 val loss: 0.693, r2: 0.525, rate:0.812, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:27 step:4300 train loss: 0.877, r2: -0.179, rate:0.438, acc:0.438 val loss: 0.836, r2: -0.522, rate:0.688, acc:0.375 lr 9.534187405839861e-08\n",
      "epoch:27 step:4400 train loss: 0.854, r2: -1.115, rate:0.438, acc:0.438 val loss: 0.901, r2: 0.347, rate:0.750, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:27 step:4500 train loss: 0.875, r2: 0.704, rate:0.812, acc:0.750 val loss: 1.008, r2: 0.272, rate:0.688, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:27 step:4600 train loss: 0.783, r2: -0.339, rate:0.625, acc:0.875 val loss: 0.985, r2: 0.643, rate:0.750, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:27 step:4700 train loss: 0.916, r2: -0.342, rate:0.812, acc:0.812 val loss: 0.663, r2: 0.550, rate:1.000, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:27 step:4800 train loss: 1.125, r2: -1.356, rate:0.562, acc:0.562 val loss: 0.663, r2: 0.438, rate:0.812, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:27 step:4900 train loss: 1.741, r2: 0.101, rate:0.500, acc:0.562 val loss: 1.014, r2: 0.169, rate:0.812, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:27 step:5000 train loss: 1.058, r2: -0.584, rate:0.438, acc:0.500 val loss: 1.439, r2: -0.434, rate:0.625, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:27 step:5100 train loss: 0.770, r2: -1.753, rate:0.625, acc:0.750 val loss: 0.904, r2: 0.413, rate:0.812, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:27 step:5200 train loss: 1.241, r2: -0.560, rate:0.625, acc:0.500 val loss: 1.123, r2: -0.217, rate:0.625, acc:0.312 lr 9.534187405839861e-08\n",
      "epoch:27 step:5300 train loss: 0.883, r2: 0.652, rate:0.812, acc:0.750 val loss: 0.805, r2: 0.057, rate:0.750, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:27 step:5400 train loss: 1.350, r2: 0.338, rate:0.625, acc:0.625 val loss: 1.277, r2: -0.235, rate:0.438, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:27 step:5500 train loss: 0.603, r2: -0.183, rate:0.812, acc:0.438 val loss: 1.209, r2: -0.198, rate:0.562, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:27 step:5600 train loss: 1.098, r2: -2.343, rate:1.000, acc:0.688 val loss: 1.324, r2: 0.307, rate:0.500, acc:0.188 lr 9.534187405839861e-08\n",
      "epoch:27 step:5700 train loss: 0.605, r2: 0.816, rate:0.812, acc:0.625 val loss: 0.943, r2: 0.246, rate:0.750, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:27 step:5800 train loss: 2.834, r2: -0.871, rate:0.625, acc:0.562 val loss: 1.197, r2: -1.097, rate:0.750, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:27 step:5900 train loss: 0.844, r2: 0.425, rate:0.812, acc:0.875 val loss: 0.652, r2: 0.654, rate:0.938, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:27 step:6000 train loss: 1.991, r2: -1.780, rate:0.500, acc:0.312 val loss: 0.827, r2: -0.399, rate:0.688, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:27 step:6100 train loss: 0.755, r2: 0.402, rate:0.875, acc:0.688 val loss: 1.091, r2: -3.273, rate:0.625, acc:0.625 lr 9.534187405839861e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:6200 train loss: 0.919, r2: -1.829, rate:0.688, acc:0.750 val loss: 1.050, r2: 0.558, rate:0.750, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:27 step:6300 train loss: 0.972, r2: -2.145, rate:0.625, acc:0.438 val loss: 0.950, r2: -0.387, rate:0.750, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:27 step:6400 train loss: 0.846, r2: -3.749, rate:0.438, acc:0.812 val loss: 0.774, r2: -1.168, rate:0.625, acc:0.188 lr 9.534187405839861e-08\n",
      "epoch:27 step:6500 train loss: 1.208, r2: -0.194, rate:0.812, acc:0.562 val loss: 0.776, r2: 0.393, rate:0.562, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:27 step:6600 train loss: 1.152, r2: 0.078, rate:0.750, acc:0.625 val loss: 3.183, r2: -1.050, rate:0.688, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:27 step:6700 train loss: 2.425, r2: -3.009, rate:0.500, acc:0.625 val loss: 0.914, r2: 0.013, rate:0.625, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:27 step:6800 train loss: 0.736, r2: -0.091, rate:1.000, acc:0.562 val loss: 0.818, r2: 0.458, rate:0.812, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:27 step:6900 train loss: 0.950, r2: 0.336, rate:0.875, acc:0.875 val loss: 1.792, r2: 0.190, rate:0.500, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:27 step:7000 train loss: 0.655, r2: 0.234, rate:0.812, acc:0.688 val loss: 0.863, r2: -0.435, rate:0.562, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:27 step:7100 train loss: 1.303, r2: -0.661, rate:0.375, acc:0.688 val loss: 0.654, r2: 0.655, rate:0.750, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:27 step:7200 train loss: 0.532, r2: 0.888, rate:0.812, acc:0.812 val loss: 1.151, r2: -1.496, rate:0.750, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:27 step:7300 train loss: 0.481, r2: 0.092, rate:0.938, acc:0.750 val loss: 0.727, r2: 0.332, rate:0.688, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:27 step:7400 train loss: 0.771, r2: -0.219, rate:0.750, acc:0.625 val loss: 1.630, r2: -3.274, rate:0.312, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:27 step:7500 train loss: 1.246, r2: -1.587, rate:0.438, acc:0.438 val loss: 1.216, r2: -1.072, rate:0.438, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:27 step:7600 train loss: 1.153, r2: -0.519, rate:0.438, acc:0.562 val loss: 0.850, r2: 0.390, rate:0.750, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:27 step:7700 train loss: 1.209, r2: 0.370, rate:0.750, acc:0.562 val loss: 1.807, r2: 0.025, rate:0.500, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:27 step:7800 train loss: 4.018, r2: -7.349, rate:0.875, acc:0.625 val loss: 0.732, r2: 0.180, rate:0.812, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:27 step:0.327 lr 0.000000\n",
      "epoch:28 step:0 train loss: 0.666, r2: 0.604, rate:0.750, acc:0.500 val loss: 1.085, r2: 0.262, rate:0.750, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:28 step:100 train loss: 1.791, r2: -1.808, rate:0.500, acc:0.375 val loss: 1.275, r2: -0.182, rate:0.688, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:28 step:200 train loss: 0.822, r2: 0.367, rate:0.438, acc:0.875 val loss: 1.547, r2: -0.102, rate:0.500, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:28 step:300 train loss: 0.735, r2: -2.049, rate:0.625, acc:0.875 val loss: 0.898, r2: 0.492, rate:0.875, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:28 step:400 train loss: 1.133, r2: 0.539, rate:0.750, acc:0.625 val loss: 0.890, r2: -0.308, rate:0.750, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:28 step:500 train loss: 1.034, r2: 0.170, rate:0.750, acc:0.750 val loss: 1.974, r2: -1.574, rate:0.688, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:28 step:600 train loss: 0.944, r2: -0.565, rate:0.500, acc:0.438 val loss: 0.728, r2: 0.384, rate:0.812, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:28 step:700 train loss: 0.741, r2: -0.417, rate:0.562, acc:0.438 val loss: 1.029, r2: 0.241, rate:0.750, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:28 step:800 train loss: 0.799, r2: -0.770, rate:0.500, acc:0.688 val loss: 1.446, r2: -0.063, rate:0.688, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:28 step:900 train loss: 0.548, r2: -0.889, rate:0.875, acc:0.875 val loss: 0.746, r2: -1.827, rate:0.750, acc:0.312 lr 9.534187405839861e-08\n",
      "epoch:28 step:1000 train loss: 0.865, r2: -3.631, rate:0.562, acc:0.750 val loss: 0.804, r2: -0.238, rate:0.688, acc:0.312 lr 9.534187405839861e-08\n",
      "epoch:28 step:1100 train loss: 4.129, r2: -0.910, rate:0.562, acc:0.750 val loss: 1.173, r2: -0.628, rate:0.562, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:28 step:1200 train loss: 1.432, r2: -0.732, rate:0.438, acc:0.688 val loss: 0.805, r2: 0.329, rate:0.688, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:28 step:1300 train loss: 0.612, r2: 0.624, rate:0.875, acc:0.812 val loss: 0.888, r2: 0.245, rate:0.812, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:28 step:1400 train loss: 0.669, r2: 0.332, rate:0.688, acc:0.875 val loss: 0.700, r2: -1.320, rate:0.875, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:28 step:1500 train loss: 1.343, r2: -0.326, rate:0.562, acc:0.438 val loss: 1.509, r2: -0.551, rate:0.500, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:28 step:1600 train loss: 0.781, r2: -0.945, rate:0.500, acc:0.812 val loss: 0.692, r2: -3.271, rate:0.625, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:28 step:1700 train loss: 1.397, r2: -1.433, rate:0.688, acc:0.250 val loss: 0.859, r2: 0.734, rate:0.812, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:28 step:1800 train loss: 0.700, r2: 0.757, rate:0.812, acc:0.875 val loss: 0.777, r2: -0.016, rate:0.750, acc:1.000 lr 9.534187405839861e-08\n",
      "epoch:28 step:1900 train loss: 0.893, r2: -0.263, rate:0.938, acc:0.500 val loss: 1.277, r2: -0.732, rate:0.500, acc:0.312 lr 9.534187405839861e-08\n",
      "epoch:28 step:2000 train loss: 1.966, r2: 0.535, rate:0.875, acc:0.750 val loss: 0.515, r2: 0.789, rate:0.938, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:28 step:2100 train loss: 0.779, r2: 0.133, rate:0.562, acc:0.812 val loss: 2.060, r2: -0.093, rate:0.688, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:28 step:2200 train loss: 0.653, r2: 0.665, rate:0.875, acc:0.812 val loss: 1.481, r2: -0.273, rate:0.562, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:28 step:2300 train loss: 1.184, r2: -0.360, rate:0.375, acc:0.625 val loss: 0.735, r2: -1.735, rate:0.812, acc:1.000 lr 9.534187405839861e-08\n",
      "epoch:28 step:2400 train loss: 1.315, r2: -0.519, rate:0.375, acc:0.250 val loss: 0.659, r2: 0.542, rate:0.812, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:28 step:2500 train loss: 1.651, r2: -0.869, rate:0.812, acc:0.438 val loss: 1.848, r2: 0.099, rate:0.625, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:28 step:2600 train loss: 0.750, r2: 0.393, rate:0.688, acc:0.688 val loss: 0.793, r2: -0.324, rate:0.688, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:28 step:2700 train loss: 1.087, r2: 0.333, rate:0.625, acc:0.812 val loss: 1.171, r2: -3.558, rate:0.500, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:28 step:2800 train loss: 0.806, r2: 0.359, rate:0.688, acc:0.625 val loss: 1.456, r2: -0.104, rate:0.812, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:28 step:2900 train loss: 0.948, r2: -1.474, rate:0.500, acc:0.250 val loss: 0.818, r2: -3.601, rate:0.688, acc:0.250 lr 9.534187405839861e-08\n",
      "epoch:28 step:3000 train loss: 0.574, r2: -0.083, rate:0.812, acc:0.750 val loss: 0.723, r2: 0.210, rate:0.625, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:28 step:3100 train loss: 1.269, r2: -2.834, rate:0.750, acc:1.000 val loss: 0.977, r2: 0.032, rate:0.750, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:28 step:3200 train loss: 1.113, r2: -0.048, rate:0.625, acc:0.375 val loss: 0.863, r2: 0.259, rate:0.688, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:28 step:3300 train loss: 0.793, r2: -1.359, rate:0.625, acc:0.438 val loss: 2.108, r2: -2.133, rate:0.688, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:28 step:3400 train loss: 0.839, r2: -4.104, rate:0.625, acc:0.938 val loss: 0.708, r2: -1.006, rate:0.562, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:28 step:3500 train loss: 2.904, r2: -11.580, rate:0.375, acc:0.125 val loss: 0.712, r2: 0.685, rate:0.750, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:28 step:3600 train loss: 0.890, r2: -1.487, rate:0.500, acc:0.375 val loss: 0.895, r2: 0.147, rate:0.750, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:28 step:3700 train loss: 0.595, r2: 0.303, rate:0.875, acc:0.688 val loss: 0.804, r2: -0.565, rate:0.500, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:28 step:3800 train loss: 0.614, r2: 0.700, rate:0.812, acc:0.812 val loss: 1.481, r2: -4.384, rate:0.438, acc:0.500 lr 9.534187405839861e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:3900 train loss: 0.994, r2: 0.504, rate:0.875, acc:0.812 val loss: 1.042, r2: -0.209, rate:0.750, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:28 step:4000 train loss: 1.130, r2: -1.762, rate:0.812, acc:0.750 val loss: 0.955, r2: 0.130, rate:0.812, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:28 step:4100 train loss: 0.766, r2: -0.693, rate:0.875, acc:0.750 val loss: 0.935, r2: -0.311, rate:0.500, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:28 step:4200 train loss: 0.770, r2: -0.744, rate:0.688, acc:0.500 val loss: 1.685, r2: -0.639, rate:0.625, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:28 step:4300 train loss: 0.926, r2: -0.780, rate:0.625, acc:0.562 val loss: 0.828, r2: 0.144, rate:0.750, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:28 step:4400 train loss: 0.900, r2: -8.765, rate:0.312, acc:0.938 val loss: 1.428, r2: 0.282, rate:0.625, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:28 step:4500 train loss: 0.677, r2: 0.617, rate:0.812, acc:0.562 val loss: 0.955, r2: 0.448, rate:0.812, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:28 step:4600 train loss: 0.674, r2: -0.084, rate:0.875, acc:0.812 val loss: 0.680, r2: 0.506, rate:0.750, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:28 step:4700 train loss: 1.121, r2: -0.784, rate:0.625, acc:0.625 val loss: 0.769, r2: 0.342, rate:0.750, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:28 step:4800 train loss: 1.396, r2: -0.967, rate:0.625, acc:0.750 val loss: 1.289, r2: -1.103, rate:0.625, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:28 step:4900 train loss: 0.694, r2: 0.262, rate:0.688, acc:0.750 val loss: 0.564, r2: 0.337, rate:0.812, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:28 step:5000 train loss: 0.685, r2: -0.699, rate:0.812, acc:0.812 val loss: 0.676, r2: 0.175, rate:0.875, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:28 step:5100 train loss: 1.224, r2: 0.002, rate:0.625, acc:0.688 val loss: 1.380, r2: 0.508, rate:0.938, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:28 step:5200 train loss: 0.647, r2: 0.792, rate:0.750, acc:0.750 val loss: 2.132, r2: -0.603, rate:0.312, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:28 step:5300 train loss: 1.024, r2: -0.367, rate:0.688, acc:0.750 val loss: 0.808, r2: -0.153, rate:0.500, acc:0.312 lr 9.534187405839861e-08\n",
      "epoch:28 step:5400 train loss: 0.630, r2: -1.263, rate:0.812, acc:0.875 val loss: 1.574, r2: 0.231, rate:0.688, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:28 step:5500 train loss: 0.574, r2: -0.243, rate:0.875, acc:0.500 val loss: 1.823, r2: -0.949, rate:0.375, acc:0.188 lr 9.534187405839861e-08\n",
      "epoch:28 step:5600 train loss: 0.607, r2: 0.511, rate:0.812, acc:0.812 val loss: 3.856, r2: -0.138, rate:0.938, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:28 step:5700 train loss: 0.726, r2: 0.217, rate:0.625, acc:0.562 val loss: 0.586, r2: -3.719, rate:0.812, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:28 step:5800 train loss: 0.753, r2: 0.315, rate:0.688, acc:0.812 val loss: 0.705, r2: 0.009, rate:0.688, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:28 step:5900 train loss: 1.435, r2: -0.747, rate:0.750, acc:0.938 val loss: 0.877, r2: -3.439, rate:0.500, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:28 step:6000 train loss: 2.136, r2: -4.043, rate:0.500, acc:0.625 val loss: 2.143, r2: -0.648, rate:0.625, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:28 step:6100 train loss: 0.692, r2: 0.584, rate:0.688, acc:0.562 val loss: 1.763, r2: -0.672, rate:0.375, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:28 step:6200 train loss: 0.785, r2: -1.327, rate:0.750, acc:0.312 val loss: 0.710, r2: 0.260, rate:0.812, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:28 step:6300 train loss: 1.185, r2: 0.210, rate:0.875, acc:0.875 val loss: 0.718, r2: 0.017, rate:0.812, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:28 step:6400 train loss: 0.847, r2: -1.610, rate:0.438, acc:0.625 val loss: 0.646, r2: 0.182, rate:0.750, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:28 step:6500 train loss: 2.433, r2: -1.891, rate:0.625, acc:0.438 val loss: 0.687, r2: 0.484, rate:0.812, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:28 step:6600 train loss: 0.874, r2: -1.067, rate:0.312, acc:0.938 val loss: 0.736, r2: 0.157, rate:0.750, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:28 step:6700 train loss: 0.830, r2: 0.409, rate:0.938, acc:0.750 val loss: 0.812, r2: 0.253, rate:0.562, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:28 step:6800 train loss: 0.728, r2: -0.792, rate:0.500, acc:0.625 val loss: 2.141, r2: 0.503, rate:0.938, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:28 step:6900 train loss: 0.857, r2: -2.357, rate:0.500, acc:0.688 val loss: 0.800, r2: -0.111, rate:0.750, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:28 step:7000 train loss: 0.996, r2: -4.663, rate:0.688, acc:0.562 val loss: 0.779, r2: -0.372, rate:0.562, acc:0.375 lr 9.534187405839861e-08\n",
      "epoch:28 step:7100 train loss: 0.599, r2: 0.390, rate:0.875, acc:0.688 val loss: 1.005, r2: 0.510, rate:0.625, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:28 step:7200 train loss: 2.151, r2: -1.004, rate:0.688, acc:0.625 val loss: 0.949, r2: -0.191, rate:0.750, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:28 step:7300 train loss: 1.105, r2: -0.115, rate:0.562, acc:0.438 val loss: 0.823, r2: -0.993, rate:0.562, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:28 step:7400 train loss: 0.942, r2: 0.604, rate:0.688, acc:0.688 val loss: 0.807, r2: 0.505, rate:0.625, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:28 step:7500 train loss: 0.450, r2: 0.590, rate:0.938, acc:0.875 val loss: 0.741, r2: 0.530, rate:0.812, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:28 step:7600 train loss: 1.514, r2: 0.277, rate:0.688, acc:0.812 val loss: 2.851, r2: -0.209, rate:0.562, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:28 step:7700 train loss: 1.055, r2: -0.286, rate:0.375, acc:0.438 val loss: 0.779, r2: -1.195, rate:0.562, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:28 step:7800 train loss: 0.995, r2: 0.042, rate:0.688, acc:0.438 val loss: 1.262, r2: -1.223, rate:0.750, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:28 step:0.327 lr 0.000000\n",
      "epoch:29 step:0 train loss: 0.964, r2: -0.273, rate:0.250, acc:0.562 val loss: 4.581, r2: -0.528, rate:0.875, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:29 step:100 train loss: 1.787, r2: -2.573, rate:0.500, acc:0.312 val loss: 1.647, r2: 0.153, rate:0.625, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:29 step:200 train loss: 1.135, r2: 0.530, rate:0.688, acc:0.500 val loss: 1.410, r2: 0.023, rate:0.688, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:29 step:300 train loss: 0.741, r2: 0.529, rate:0.750, acc:0.625 val loss: 0.561, r2: 0.108, rate:0.875, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:29 step:400 train loss: 1.946, r2: -2.134, rate:0.750, acc:0.938 val loss: 1.718, r2: 0.289, rate:0.750, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:29 step:500 train loss: 1.099, r2: 0.352, rate:0.688, acc:0.750 val loss: 0.906, r2: -3.112, rate:0.688, acc:0.375 lr 9.534187405839861e-08\n",
      "epoch:29 step:600 train loss: 0.701, r2: -2.060, rate:0.562, acc:0.750 val loss: 0.588, r2: 0.670, rate:0.938, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:29 step:700 train loss: 0.793, r2: 0.035, rate:0.625, acc:0.500 val loss: 0.808, r2: 0.118, rate:0.625, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:29 step:800 train loss: 0.854, r2: 0.038, rate:1.000, acc:0.875 val loss: 1.474, r2: -0.306, rate:0.688, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:29 step:900 train loss: 1.895, r2: -0.908, rate:0.750, acc:0.750 val loss: 0.636, r2: 0.357, rate:0.875, acc:1.000 lr 9.534187405839861e-08\n",
      "epoch:29 step:1000 train loss: 0.771, r2: 0.195, rate:0.875, acc:0.812 val loss: 0.772, r2: -0.230, rate:0.750, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:29 step:1100 train loss: 0.846, r2: -2.006, rate:0.750, acc:1.000 val loss: 1.054, r2: 0.197, rate:0.625, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:29 step:1200 train loss: 0.501, r2: -0.121, rate:1.000, acc:0.938 val loss: 0.795, r2: -2.336, rate:0.750, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:29 step:1300 train loss: 1.631, r2: -1.074, rate:0.688, acc:0.312 val loss: 0.701, r2: -0.326, rate:0.750, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:29 step:1400 train loss: 1.361, r2: -1.116, rate:0.938, acc:0.938 val loss: 0.703, r2: 0.293, rate:0.625, acc:0.375 lr 9.534187405839861e-08\n",
      "epoch:29 step:1500 train loss: 0.679, r2: 0.073, rate:0.812, acc:0.562 val loss: 2.576, r2: -1.653, rate:0.312, acc:0.500 lr 9.534187405839861e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:1600 train loss: 0.706, r2: 0.708, rate:0.875, acc:1.000 val loss: 0.723, r2: 0.021, rate:0.688, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:29 step:1700 train loss: 1.166, r2: -0.825, rate:0.812, acc:0.875 val loss: 1.582, r2: 0.046, rate:0.750, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:29 step:1800 train loss: 0.867, r2: -0.187, rate:0.438, acc:0.500 val loss: 0.950, r2: 0.371, rate:0.688, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:29 step:1900 train loss: 2.035, r2: -1.229, rate:0.562, acc:0.750 val loss: 0.696, r2: 0.220, rate:0.750, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:29 step:2000 train loss: 1.316, r2: 0.268, rate:0.688, acc:0.688 val loss: 1.321, r2: -0.912, rate:0.750, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:29 step:2100 train loss: 0.741, r2: 0.200, rate:0.688, acc:0.688 val loss: 1.379, r2: -1.203, rate:0.375, acc:0.250 lr 9.534187405839861e-08\n",
      "epoch:29 step:2200 train loss: 0.731, r2: -0.268, rate:0.500, acc:0.625 val loss: 0.837, r2: 0.460, rate:0.812, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:29 step:2300 train loss: 2.294, r2: -1.805, rate:0.625, acc:0.375 val loss: 1.021, r2: -0.227, rate:0.500, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:29 step:2400 train loss: 0.690, r2: -0.230, rate:0.562, acc:0.625 val loss: 0.947, r2: -0.581, rate:0.438, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:29 step:2500 train loss: 0.716, r2: -0.548, rate:0.438, acc:0.750 val loss: 1.128, r2: -1.302, rate:0.438, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:29 step:2600 train loss: 1.081, r2: 0.172, rate:0.688, acc:0.625 val loss: 0.738, r2: 0.321, rate:0.688, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:29 step:2700 train loss: 1.113, r2: -2.962, rate:0.375, acc:0.625 val loss: 0.734, r2: 0.236, rate:0.688, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:29 step:2800 train loss: 1.268, r2: -2.569, rate:0.688, acc:0.812 val loss: 0.634, r2: 0.719, rate:0.750, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:29 step:2900 train loss: 0.642, r2: -1.143, rate:0.625, acc:0.750 val loss: 0.673, r2: 0.671, rate:0.875, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:29 step:3000 train loss: 0.832, r2: -1.460, rate:0.625, acc:0.562 val loss: 2.299, r2: -0.100, rate:0.438, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:29 step:3100 train loss: 0.930, r2: -4.016, rate:0.500, acc:0.938 val loss: 0.794, r2: -0.358, rate:0.688, acc:0.312 lr 9.534187405839861e-08\n",
      "epoch:29 step:3200 train loss: 1.107, r2: -3.120, rate:0.500, acc:0.312 val loss: 0.831, r2: -0.224, rate:0.562, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:29 step:3300 train loss: 1.026, r2: -0.231, rate:0.812, acc:0.562 val loss: 0.932, r2: 0.227, rate:0.688, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:29 step:3400 train loss: 0.622, r2: -0.970, rate:0.875, acc:0.500 val loss: 0.763, r2: -0.426, rate:0.500, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:29 step:3500 train loss: 0.995, r2: -0.312, rate:0.688, acc:0.438 val loss: 1.717, r2: -4.217, rate:0.812, acc:1.000 lr 9.534187405839861e-08\n",
      "epoch:29 step:3600 train loss: 0.493, r2: 0.511, rate:0.875, acc:0.875 val loss: 0.845, r2: 0.660, rate:0.688, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:29 step:3700 train loss: 0.667, r2: 0.479, rate:0.688, acc:0.500 val loss: 1.981, r2: 0.116, rate:0.625, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:29 step:3800 train loss: 1.369, r2: -1.308, rate:0.688, acc:0.500 val loss: 0.609, r2: 0.693, rate:0.938, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:29 step:3900 train loss: 0.752, r2: -0.032, rate:0.750, acc:0.625 val loss: 0.607, r2: 0.299, rate:0.875, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:29 step:4000 train loss: 1.671, r2: -2.165, rate:0.562, acc:0.312 val loss: 0.901, r2: 0.166, rate:0.688, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:29 step:4100 train loss: 1.086, r2: -1.515, rate:0.438, acc:0.312 val loss: 1.975, r2: -1.428, rate:0.562, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:29 step:4200 train loss: 0.870, r2: 0.163, rate:0.688, acc:1.000 val loss: 0.697, r2: 0.205, rate:0.625, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:29 step:4300 train loss: 0.641, r2: 0.568, rate:0.688, acc:0.688 val loss: 0.825, r2: -1.946, rate:0.562, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:29 step:4400 train loss: 1.137, r2: 0.519, rate:0.562, acc:0.625 val loss: 1.700, r2: -1.525, rate:0.812, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:29 step:4500 train loss: 0.580, r2: -0.828, rate:0.875, acc:0.938 val loss: 1.176, r2: -0.472, rate:0.625, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:29 step:4600 train loss: 0.934, r2: -0.558, rate:0.812, acc:0.938 val loss: 0.770, r2: -0.344, rate:0.562, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:29 step:4700 train loss: 0.655, r2: 0.701, rate:0.750, acc:0.875 val loss: 0.845, r2: 0.198, rate:0.688, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:29 step:4800 train loss: 0.700, r2: 0.446, rate:0.500, acc:0.812 val loss: 0.732, r2: -0.113, rate:0.750, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:29 step:4900 train loss: 0.910, r2: -1.288, rate:0.750, acc:0.375 val loss: 0.815, r2: -3.574, rate:0.625, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:29 step:5000 train loss: 2.485, r2: -0.557, rate:0.812, acc:0.562 val loss: 0.753, r2: -2.782, rate:0.438, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:29 step:5100 train loss: 1.275, r2: -1.756, rate:0.375, acc:0.688 val loss: 1.222, r2: 0.380, rate:0.938, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:29 step:5200 train loss: 0.633, r2: -0.142, rate:0.688, acc:0.688 val loss: 1.622, r2: 0.139, rate:0.562, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:29 step:5300 train loss: 0.731, r2: 0.341, rate:0.750, acc:0.812 val loss: 1.256, r2: 0.011, rate:0.750, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:29 step:5400 train loss: 0.874, r2: -1.002, rate:0.625, acc:0.438 val loss: 1.138, r2: -3.769, rate:0.562, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:29 step:5500 train loss: 1.082, r2: -1.685, rate:0.562, acc:0.500 val loss: 0.901, r2: -0.357, rate:0.688, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:29 step:5600 train loss: 1.170, r2: -6.888, rate:0.500, acc:0.250 val loss: 1.097, r2: 0.508, rate:0.750, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:29 step:5700 train loss: 1.029, r2: 0.073, rate:0.312, acc:0.688 val loss: 1.458, r2: -4.634, rate:0.625, acc:0.312 lr 9.534187405839861e-08\n",
      "epoch:29 step:5800 train loss: 1.667, r2: 0.307, rate:0.750, acc:0.875 val loss: 0.970, r2: 0.668, rate:0.750, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:29 step:5900 train loss: 1.439, r2: -0.118, rate:0.688, acc:0.438 val loss: 0.651, r2: -1.618, rate:0.562, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:29 step:6000 train loss: 0.831, r2: 0.334, rate:0.625, acc:0.438 val loss: 0.681, r2: 0.466, rate:0.688, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:29 step:6100 train loss: 0.679, r2: -0.367, rate:0.750, acc:0.750 val loss: 0.843, r2: -0.217, rate:0.938, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:29 step:6200 train loss: 1.334, r2: -0.456, rate:0.625, acc:0.625 val loss: 1.831, r2: -0.619, rate:0.438, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:29 step:6300 train loss: 0.847, r2: 0.209, rate:0.562, acc:0.688 val loss: 4.245, r2: 0.155, rate:0.938, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:29 step:6400 train loss: 0.809, r2: 0.217, rate:0.812, acc:0.812 val loss: 1.207, r2: -0.366, rate:0.688, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:29 step:6500 train loss: 2.444, r2: -0.834, rate:0.688, acc:0.625 val loss: 0.820, r2: -0.144, rate:0.688, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:29 step:6600 train loss: 0.623, r2: 0.112, rate:0.688, acc:0.875 val loss: 1.323, r2: 0.481, rate:0.938, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:29 step:6700 train loss: 2.260, r2: -0.601, rate:0.562, acc:0.438 val loss: 0.914, r2: 0.135, rate:0.750, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:29 step:6800 train loss: 0.806, r2: 0.548, rate:0.875, acc:0.812 val loss: 0.904, r2: 0.101, rate:0.562, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:29 step:6900 train loss: 0.527, r2: 0.666, rate:0.875, acc:0.812 val loss: 0.930, r2: 0.540, rate:0.750, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:29 step:7000 train loss: 1.151, r2: -0.605, rate:0.812, acc:0.625 val loss: 0.991, r2: -0.119, rate:0.688, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:29 step:7100 train loss: 1.004, r2: 0.696, rate:0.812, acc:0.750 val loss: 1.359, r2: -0.820, rate:0.438, acc:0.750 lr 9.534187405839861e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:7200 train loss: 0.705, r2: -0.349, rate:0.812, acc:0.938 val loss: 2.177, r2: 0.060, rate:0.625, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:29 step:7300 train loss: 1.500, r2: -1.291, rate:0.500, acc:0.688 val loss: 1.153, r2: 0.169, rate:0.688, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:29 step:7400 train loss: 4.759, r2: -10.214, rate:1.000, acc:1.000 val loss: 0.858, r2: -2.840, rate:0.562, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:29 step:7500 train loss: 0.674, r2: 0.354, rate:0.750, acc:0.625 val loss: 0.732, r2: 0.071, rate:0.562, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:29 step:7600 train loss: 1.917, r2: -2.153, rate:0.938, acc:1.000 val loss: 0.934, r2: 0.320, rate:0.688, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:29 step:7700 train loss: 1.215, r2: -5.031, rate:0.625, acc:0.500 val loss: 0.555, r2: 0.588, rate:0.875, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:29 step:7800 train loss: 0.757, r2: 0.329, rate:0.625, acc:0.750 val loss: 1.340, r2: 0.446, rate:0.750, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:29 step:0.328 lr 0.000000\n",
      "epoch:30 step:0 train loss: 1.178, r2: -3.671, rate:0.562, acc:0.312 val loss: 1.526, r2: -0.423, rate:0.562, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:30 step:100 train loss: 0.689, r2: 0.187, rate:0.812, acc:0.688 val loss: 0.695, r2: -0.138, rate:0.812, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:30 step:200 train loss: 3.428, r2: 0.222, rate:0.562, acc:0.688 val loss: 0.819, r2: 0.471, rate:0.750, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:30 step:300 train loss: 1.633, r2: -1.348, rate:0.938, acc:0.750 val loss: 2.493, r2: -0.225, rate:0.375, acc:0.375 lr 9.534187405839861e-08\n",
      "epoch:30 step:400 train loss: 1.032, r2: 0.475, rate:0.688, acc:0.438 val loss: 0.749, r2: 0.383, rate:0.812, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:30 step:500 train loss: 1.422, r2: -0.695, rate:0.812, acc:0.688 val loss: 0.756, r2: -0.093, rate:0.562, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:30 step:600 train loss: 0.856, r2: 0.302, rate:0.750, acc:0.500 val loss: 1.294, r2: -0.075, rate:0.688, acc:0.375 lr 9.534187405839861e-08\n",
      "epoch:30 step:700 train loss: 0.876, r2: -1.325, rate:0.688, acc:0.688 val loss: 0.646, r2: 0.112, rate:0.688, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:30 step:800 train loss: 0.764, r2: -0.273, rate:0.750, acc:0.500 val loss: 0.984, r2: 0.576, rate:0.875, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:30 step:900 train loss: 1.060, r2: 0.125, rate:0.625, acc:0.625 val loss: 0.738, r2: -1.847, rate:0.875, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:30 step:1000 train loss: 1.767, r2: -2.635, rate:0.250, acc:0.438 val loss: 0.768, r2: 0.120, rate:0.812, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:30 step:1100 train loss: 1.503, r2: 0.515, rate:0.562, acc:0.438 val loss: 1.679, r2: -2.681, rate:0.750, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:30 step:1200 train loss: 0.662, r2: -0.204, rate:0.812, acc:0.500 val loss: 0.921, r2: 0.240, rate:0.562, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:30 step:1300 train loss: 0.914, r2: -0.859, rate:0.812, acc:0.250 val loss: 0.692, r2: -0.046, rate:0.688, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:30 step:1400 train loss: 1.075, r2: -0.071, rate:0.625, acc:0.625 val loss: 0.768, r2: 0.508, rate:0.812, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:30 step:1500 train loss: 0.725, r2: -0.070, rate:0.812, acc:0.875 val loss: 0.967, r2: -1.864, rate:0.625, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:30 step:1600 train loss: 0.721, r2: -1.003, rate:0.750, acc:0.750 val loss: 1.529, r2: -1.130, rate:0.562, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:30 step:1700 train loss: 0.507, r2: -11.663, rate:1.000, acc:1.000 val loss: 2.397, r2: -0.083, rate:0.562, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:30 step:1800 train loss: 0.766, r2: -1.203, rate:0.750, acc:0.688 val loss: 0.668, r2: -1.168, rate:0.562, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:30 step:1900 train loss: 0.469, r2: 0.809, rate:1.000, acc:1.000 val loss: 0.965, r2: 0.288, rate:0.500, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:30 step:2000 train loss: 0.877, r2: 0.567, rate:0.562, acc:0.562 val loss: 1.065, r2: 0.027, rate:0.500, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:30 step:2100 train loss: 0.701, r2: 0.673, rate:0.812, acc:0.500 val loss: 1.361, r2: 0.015, rate:0.625, acc:0.375 lr 9.534187405839861e-08\n",
      "epoch:30 step:2200 train loss: 0.964, r2: -0.500, rate:0.625, acc:0.812 val loss: 0.870, r2: 0.133, rate:0.750, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:30 step:2300 train loss: 2.802, r2: -22.444, rate:0.812, acc:0.938 val loss: 0.737, r2: 0.251, rate:0.625, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:30 step:2400 train loss: 0.619, r2: -2.208, rate:0.875, acc:0.625 val loss: 1.661, r2: -2.345, rate:0.438, acc:0.250 lr 9.534187405839861e-08\n",
      "epoch:30 step:2500 train loss: 0.856, r2: -0.234, rate:0.625, acc:0.750 val loss: 0.874, r2: -0.582, rate:0.500, acc:0.250 lr 9.534187405839861e-08\n",
      "epoch:30 step:2600 train loss: 1.959, r2: -0.217, rate:0.625, acc:0.812 val loss: 0.668, r2: 0.454, rate:0.875, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:30 step:2700 train loss: 0.829, r2: -0.827, rate:0.625, acc:0.750 val loss: 0.569, r2: 0.674, rate:0.938, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:30 step:2800 train loss: 0.719, r2: -0.197, rate:0.562, acc:0.625 val loss: 0.639, r2: 0.349, rate:0.688, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:30 step:2900 train loss: 1.351, r2: -2.761, rate:0.438, acc:0.000 val loss: 0.940, r2: -0.316, rate:0.375, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:30 step:3000 train loss: 0.802, r2: 0.412, rate:0.750, acc:0.812 val loss: 0.742, r2: 0.333, rate:0.688, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:30 step:3100 train loss: 1.512, r2: 0.314, rate:0.688, acc:0.438 val loss: 0.799, r2: 0.536, rate:0.688, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:30 step:3200 train loss: 0.662, r2: -0.075, rate:0.875, acc:0.875 val loss: 0.617, r2: 0.078, rate:0.625, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:30 step:3300 train loss: 1.820, r2: 0.286, rate:0.875, acc:0.688 val loss: 1.504, r2: 0.474, rate:0.438, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:30 step:3400 train loss: 0.775, r2: -6.690, rate:0.438, acc:0.875 val loss: 0.809, r2: -0.193, rate:0.625, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:30 step:3500 train loss: 0.922, r2: -0.948, rate:0.375, acc:0.375 val loss: 0.912, r2: 0.380, rate:0.812, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:30 step:3600 train loss: 0.594, r2: -1.149, rate:0.688, acc:1.000 val loss: 0.835, r2: 0.286, rate:0.625, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:30 step:3700 train loss: 0.792, r2: 0.406, rate:0.750, acc:0.375 val loss: 0.944, r2: 0.556, rate:0.812, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:30 step:3800 train loss: 0.895, r2: 0.491, rate:0.812, acc:0.875 val loss: 0.610, r2: 0.561, rate:0.875, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:30 step:3900 train loss: 2.149, r2: -1.428, rate:0.688, acc:0.750 val loss: 0.718, r2: 0.140, rate:0.688, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:30 step:4000 train loss: 1.069, r2: -4.006, rate:0.688, acc:0.438 val loss: 1.076, r2: -0.453, rate:0.688, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:30 step:4100 train loss: 1.146, r2: 0.095, rate:0.688, acc:0.812 val loss: 0.968, r2: -1.277, rate:0.625, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:30 step:4200 train loss: 2.340, r2: -0.973, rate:0.562, acc:0.438 val loss: 0.796, r2: -0.398, rate:0.688, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:30 step:4300 train loss: 0.803, r2: -1.353, rate:0.438, acc:0.375 val loss: 1.130, r2: -0.995, rate:0.500, acc:0.375 lr 9.534187405839861e-08\n",
      "epoch:30 step:4400 train loss: 0.770, r2: 0.472, rate:0.812, acc:0.812 val loss: 1.349, r2: -1.482, rate:0.750, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:30 step:4500 train loss: 0.646, r2: -0.282, rate:0.688, acc:0.688 val loss: 1.180, r2: 0.177, rate:0.625, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:30 step:4600 train loss: 1.096, r2: 0.058, rate:0.438, acc:0.938 val loss: 0.590, r2: -0.646, rate:0.750, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:30 step:4700 train loss: 0.996, r2: -0.179, rate:0.562, acc:0.562 val loss: 1.265, r2: 0.303, rate:0.500, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:30 step:4800 train loss: 0.620, r2: 0.273, rate:0.875, acc:0.750 val loss: 0.962, r2: -5.113, rate:0.375, acc:0.625 lr 9.534187405839861e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:30 step:4900 train loss: 0.619, r2: 0.547, rate:0.750, acc:0.938 val loss: 2.247, r2: 0.441, rate:0.812, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:30 step:5000 train loss: 0.815, r2: 0.045, rate:0.750, acc:0.562 val loss: 0.649, r2: -1.711, rate:0.562, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:30 step:5100 train loss: 1.414, r2: -1.487, rate:0.438, acc:0.438 val loss: 2.231, r2: -0.425, rate:0.875, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:30 step:5200 train loss: 0.620, r2: 0.643, rate:0.812, acc:0.875 val loss: 2.193, r2: 0.023, rate:0.500, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:30 step:5300 train loss: 2.811, r2: 0.200, rate:0.812, acc:0.812 val loss: 1.118, r2: -0.205, rate:0.500, acc:0.312 lr 9.534187405839861e-08\n",
      "epoch:30 step:5400 train loss: 0.614, r2: -0.015, rate:0.812, acc:0.750 val loss: 1.019, r2: -0.381, rate:0.562, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:30 step:5500 train loss: 0.823, r2: -0.469, rate:0.625, acc:0.812 val loss: 1.047, r2: -0.209, rate:0.625, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:30 step:5600 train loss: 1.211, r2: -0.378, rate:0.688, acc:0.562 val loss: 0.666, r2: 0.503, rate:0.875, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:30 step:5700 train loss: 0.857, r2: -0.165, rate:0.688, acc:0.812 val loss: 0.900, r2: -0.108, rate:0.438, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:30 step:5800 train loss: 1.384, r2: -1.685, rate:0.688, acc:0.938 val loss: 0.968, r2: -0.109, rate:0.812, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:30 step:5900 train loss: 1.445, r2: 0.459, rate:0.688, acc:0.500 val loss: 1.230, r2: -0.009, rate:0.438, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:30 step:6000 train loss: 3.309, r2: 0.364, rate:0.625, acc:0.500 val loss: 0.869, r2: 0.373, rate:0.750, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:30 step:6100 train loss: 1.843, r2: -1.630, rate:0.750, acc:0.375 val loss: 0.894, r2: 0.515, rate:0.688, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:30 step:6200 train loss: 1.163, r2: -1.046, rate:0.562, acc:0.750 val loss: 0.863, r2: -7.737, rate:0.688, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:30 step:6300 train loss: 0.682, r2: -1.269, rate:0.625, acc:0.500 val loss: 0.768, r2: 0.303, rate:0.500, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:30 step:6400 train loss: 2.535, r2: -0.656, rate:0.750, acc:0.625 val loss: 1.090, r2: 0.520, rate:0.562, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:30 step:6500 train loss: 2.028, r2: -1.128, rate:0.438, acc:0.250 val loss: 0.890, r2: -9.639, rate:0.438, acc:0.312 lr 9.534187405839861e-08\n",
      "epoch:30 step:6600 train loss: 1.808, r2: -1.223, rate:0.750, acc:0.875 val loss: 0.717, r2: -0.031, rate:0.750, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:30 step:6700 train loss: 0.749, r2: 0.531, rate:0.938, acc:0.875 val loss: 1.029, r2: -1.534, rate:0.750, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:30 step:6800 train loss: 0.832, r2: -0.688, rate:0.750, acc:0.938 val loss: 0.686, r2: -0.126, rate:0.812, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:30 step:6900 train loss: 1.093, r2: 0.295, rate:0.688, acc:0.625 val loss: 0.677, r2: -0.587, rate:0.875, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:30 step:7000 train loss: 1.299, r2: -0.335, rate:0.375, acc:0.438 val loss: 2.205, r2: 0.219, rate:0.750, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:30 step:7100 train loss: 0.759, r2: 0.301, rate:0.688, acc:0.688 val loss: 0.809, r2: 0.260, rate:0.688, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:30 step:7200 train loss: 0.860, r2: 0.115, rate:0.625, acc:0.812 val loss: 0.816, r2: 0.544, rate:0.938, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:30 step:7300 train loss: 0.684, r2: -0.069, rate:0.562, acc:0.875 val loss: 0.682, r2: -0.067, rate:0.562, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:30 step:7400 train loss: 1.293, r2: -1.611, rate:0.438, acc:0.312 val loss: 1.039, r2: -0.615, rate:0.750, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:30 step:7500 train loss: 0.590, r2: 0.265, rate:0.875, acc:0.688 val loss: 1.063, r2: -0.325, rate:0.688, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:30 step:7600 train loss: 0.869, r2: 0.585, rate:0.875, acc:0.812 val loss: 0.891, r2: 0.161, rate:0.625, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:30 step:7700 train loss: 2.654, r2: -5.778, rate:0.938, acc:0.688 val loss: 2.696, r2: -0.004, rate:0.375, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:30 step:7800 train loss: 1.061, r2: -2.044, rate:0.625, acc:0.688 val loss: 1.094, r2: 0.044, rate:0.688, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:30 step:0.328 lr 0.000000\n",
      "epoch:31 step:0 train loss: 0.740, r2: 0.350, rate:0.688, acc:0.625 val loss: 1.247, r2: 0.061, rate:0.625, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:31 step:100 train loss: 1.126, r2: 0.142, rate:0.500, acc:0.438 val loss: 0.842, r2: 0.453, rate:0.562, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:31 step:200 train loss: 1.788, r2: -7.390, rate:0.375, acc:0.438 val loss: 0.781, r2: -0.560, rate:0.688, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:31 step:300 train loss: 0.915, r2: -0.422, rate:0.500, acc:0.562 val loss: 1.854, r2: -1.014, rate:0.812, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:31 step:400 train loss: 0.797, r2: 0.318, rate:0.625, acc:0.812 val loss: 0.891, r2: 0.482, rate:0.562, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:31 step:500 train loss: 0.809, r2: 0.426, rate:0.938, acc:1.000 val loss: 2.075, r2: -0.827, rate:0.312, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:31 step:600 train loss: 0.598, r2: -0.920, rate:0.812, acc:0.875 val loss: 0.763, r2: 0.570, rate:0.688, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:31 step:700 train loss: 6.283, r2: -13.887, rate:0.750, acc:0.875 val loss: 0.658, r2: 0.195, rate:0.688, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:31 step:800 train loss: 1.549, r2: -2.501, rate:0.625, acc:0.688 val loss: 1.065, r2: 0.155, rate:0.750, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:31 step:900 train loss: 0.709, r2: 0.115, rate:0.875, acc:0.875 val loss: 0.972, r2: -8.310, rate:0.625, acc:0.062 lr 9.534187405839861e-08\n",
      "epoch:31 step:1000 train loss: 1.228, r2: 0.190, rate:0.812, acc:0.562 val loss: 1.725, r2: -0.466, rate:0.500, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:31 step:1100 train loss: 1.228, r2: 0.186, rate:0.562, acc:0.562 val loss: 0.831, r2: 0.178, rate:0.875, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:31 step:1200 train loss: 0.689, r2: 0.384, rate:0.688, acc:0.812 val loss: 1.043, r2: 0.328, rate:0.688, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:31 step:1300 train loss: 0.786, r2: -0.349, rate:0.625, acc:0.688 val loss: 1.050, r2: -0.150, rate:0.750, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:31 step:1400 train loss: 1.150, r2: -0.470, rate:0.625, acc:0.750 val loss: 0.661, r2: -0.419, rate:0.812, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:31 step:1500 train loss: 0.781, r2: 0.441, rate:0.625, acc:0.688 val loss: 0.753, r2: 0.502, rate:0.625, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:31 step:1600 train loss: 0.737, r2: -4.838, rate:0.750, acc:0.562 val loss: 0.833, r2: 0.541, rate:0.688, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:31 step:1700 train loss: 1.073, r2: 0.159, rate:0.938, acc:0.875 val loss: 0.930, r2: -0.059, rate:0.688, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:31 step:1800 train loss: 0.541, r2: -0.304, rate:0.875, acc:0.875 val loss: 1.580, r2: -0.777, rate:0.562, acc:0.312 lr 9.534187405839861e-08\n",
      "epoch:31 step:1900 train loss: 0.739, r2: -0.489, rate:0.688, acc:0.812 val loss: 1.211, r2: 0.058, rate:0.562, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:31 step:2000 train loss: 0.618, r2: 0.178, rate:1.000, acc:0.812 val loss: 2.712, r2: -2.151, rate:0.625, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:31 step:2100 train loss: 1.134, r2: -0.488, rate:0.375, acc:0.500 val loss: 0.692, r2: -0.261, rate:0.625, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:31 step:2200 train loss: 3.299, r2: -1.534, rate:0.812, acc:0.875 val loss: 0.684, r2: 0.427, rate:0.562, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:31 step:2300 train loss: 2.107, r2: 0.066, rate:0.812, acc:0.875 val loss: 0.849, r2: 0.599, rate:0.688, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:31 step:2400 train loss: 0.951, r2: 0.360, rate:0.812, acc:0.812 val loss: 0.800, r2: -5.572, rate:0.750, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:31 step:2500 train loss: 0.630, r2: 0.497, rate:0.875, acc:1.000 val loss: 1.596, r2: -0.381, rate:0.688, acc:0.500 lr 9.534187405839861e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:31 step:2600 train loss: 0.477, r2: -0.036, rate:0.938, acc:0.938 val loss: 0.568, r2: 0.792, rate:0.938, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:31 step:2700 train loss: 0.767, r2: -0.468, rate:0.688, acc:0.750 val loss: 2.664, r2: -1.129, rate:0.438, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:31 step:2800 train loss: 2.411, r2: -5.434, rate:0.625, acc:0.938 val loss: 1.341, r2: -0.129, rate:0.812, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:31 step:2900 train loss: 1.550, r2: -1.932, rate:0.250, acc:0.688 val loss: 1.395, r2: -2.238, rate:0.500, acc:0.250 lr 9.534187405839861e-08\n",
      "epoch:31 step:3000 train loss: 0.765, r2: -1.524, rate:0.562, acc:0.875 val loss: 0.933, r2: -2.050, rate:0.562, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:31 step:3100 train loss: 5.502, r2: -1.072, rate:0.688, acc:0.875 val loss: 0.819, r2: 0.292, rate:0.562, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:31 step:3200 train loss: 0.735, r2: -0.446, rate:0.812, acc:0.188 val loss: 0.706, r2: -0.083, rate:0.688, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:31 step:3300 train loss: 0.556, r2: -0.996, rate:0.812, acc:0.750 val loss: 0.732, r2: 0.607, rate:0.688, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:31 step:3400 train loss: 0.637, r2: 0.710, rate:0.812, acc:0.688 val loss: 1.231, r2: 0.046, rate:0.688, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:31 step:3500 train loss: 0.898, r2: -0.097, rate:0.625, acc:0.688 val loss: 0.541, r2: -1.297, rate:0.750, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:31 step:3600 train loss: 1.266, r2: 0.309, rate:0.562, acc:0.375 val loss: 0.939, r2: 0.541, rate:0.812, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:31 step:3700 train loss: 0.706, r2: -0.295, rate:0.812, acc:0.938 val loss: 1.149, r2: 0.389, rate:0.812, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:31 step:3800 train loss: 0.924, r2: -0.357, rate:0.438, acc:0.500 val loss: 1.205, r2: -0.314, rate:0.562, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:31 step:3900 train loss: 0.794, r2: -0.437, rate:0.625, acc:0.562 val loss: 0.555, r2: -1.738, rate:0.750, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:31 step:4000 train loss: 2.629, r2: -3.090, rate:0.438, acc:0.688 val loss: 0.793, r2: -0.206, rate:0.562, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:31 step:4100 train loss: 1.690, r2: -2.316, rate:0.250, acc:0.688 val loss: 1.217, r2: -1.488, rate:0.562, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:31 step:4200 train loss: 1.201, r2: 0.535, rate:0.438, acc:0.750 val loss: 0.746, r2: 0.344, rate:0.562, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:31 step:4300 train loss: 1.338, r2: -0.827, rate:0.438, acc:0.375 val loss: 0.727, r2: 0.127, rate:0.688, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:31 step:4400 train loss: 2.560, r2: -0.354, rate:0.750, acc:0.750 val loss: 1.600, r2: -0.627, rate:0.625, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:31 step:4500 train loss: 0.812, r2: -0.745, rate:0.500, acc:0.938 val loss: 1.109, r2: 0.630, rate:0.750, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:31 step:4600 train loss: 1.031, r2: -0.038, rate:0.625, acc:0.812 val loss: 1.431, r2: 0.315, rate:0.812, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:31 step:4700 train loss: 3.495, r2: -5.105, rate:0.812, acc:0.500 val loss: 0.772, r2: -0.586, rate:0.812, acc:1.000 lr 9.534187405839861e-08\n",
      "epoch:31 step:4800 train loss: 0.700, r2: 0.478, rate:0.750, acc:0.875 val loss: 0.684, r2: -0.281, rate:0.750, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:31 step:4900 train loss: 1.114, r2: 0.610, rate:0.750, acc:0.812 val loss: 1.088, r2: -0.376, rate:0.500, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:31 step:5000 train loss: 1.280, r2: 0.127, rate:0.625, acc:0.625 val loss: 0.578, r2: 0.619, rate:0.938, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:31 step:5100 train loss: 1.119, r2: -2.365, rate:0.750, acc:0.938 val loss: 1.048, r2: 0.387, rate:0.688, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:31 step:5200 train loss: 1.525, r2: -3.585, rate:0.562, acc:0.688 val loss: 0.674, r2: 0.631, rate:0.688, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:31 step:5300 train loss: 1.147, r2: -0.816, rate:0.438, acc:0.625 val loss: 0.922, r2: 0.534, rate:0.938, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:31 step:5400 train loss: 0.730, r2: -0.822, rate:0.375, acc:0.750 val loss: 0.650, r2: -0.375, rate:0.812, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:31 step:5500 train loss: 0.683, r2: 0.584, rate:0.875, acc:0.688 val loss: 0.622, r2: 0.570, rate:0.875, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:31 step:5600 train loss: 0.975, r2: -0.653, rate:0.750, acc:0.312 val loss: 1.373, r2: 0.333, rate:0.750, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:31 step:5700 train loss: 0.830, r2: -0.376, rate:0.750, acc:0.625 val loss: 0.760, r2: 0.154, rate:0.750, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:31 step:5800 train loss: 0.772, r2: -0.503, rate:0.750, acc:0.812 val loss: 0.823, r2: 0.120, rate:0.562, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:31 step:5900 train loss: 4.701, r2: -3.180, rate:0.500, acc:0.938 val loss: 0.683, r2: -0.211, rate:0.688, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:31 step:6000 train loss: 0.931, r2: -0.392, rate:0.688, acc:0.500 val loss: 0.654, r2: 0.471, rate:0.812, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:31 step:6100 train loss: 0.673, r2: 0.565, rate:0.750, acc:0.500 val loss: 1.784, r2: 0.044, rate:0.562, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:31 step:6200 train loss: 2.700, r2: -0.169, rate:0.750, acc:0.875 val loss: 0.948, r2: 0.259, rate:0.562, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:31 step:6300 train loss: 0.688, r2: 0.438, rate:0.438, acc:0.688 val loss: 0.848, r2: 0.342, rate:0.688, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:31 step:6400 train loss: 0.897, r2: -0.131, rate:0.625, acc:0.438 val loss: 0.527, r2: -1.020, rate:0.938, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:31 step:6500 train loss: 0.961, r2: 0.207, rate:0.625, acc:0.312 val loss: 0.613, r2: -0.796, rate:0.812, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:31 step:6600 train loss: 1.149, r2: -3.533, rate:0.375, acc:0.188 val loss: 0.769, r2: -0.379, rate:0.750, acc:0.188 lr 9.534187405839861e-08\n",
      "epoch:31 step:6700 train loss: 1.493, r2: -0.722, rate:0.375, acc:0.312 val loss: 1.716, r2: -0.067, rate:0.688, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:31 step:6800 train loss: 0.693, r2: 0.807, rate:0.938, acc:0.938 val loss: 1.437, r2: -2.188, rate:0.812, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:31 step:6900 train loss: 1.265, r2: -0.293, rate:0.438, acc:0.375 val loss: 1.093, r2: 0.463, rate:0.688, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:31 step:7000 train loss: 1.114, r2: -2.123, rate:0.438, acc:0.562 val loss: 0.683, r2: 0.724, rate:0.875, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:31 step:7100 train loss: 5.549, r2: -6.554, rate:0.625, acc:0.312 val loss: 0.758, r2: -0.006, rate:0.750, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:31 step:7200 train loss: 0.737, r2: -2.004, rate:0.562, acc:0.812 val loss: 0.569, r2: 0.720, rate:0.875, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:31 step:7300 train loss: 1.251, r2: -1.974, rate:0.625, acc:0.500 val loss: 0.728, r2: -0.084, rate:0.812, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:31 step:7400 train loss: 1.421, r2: -1.316, rate:0.500, acc:0.375 val loss: 1.223, r2: -0.204, rate:0.562, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:31 step:7500 train loss: 3.269, r2: -4.513, rate:0.438, acc:0.562 val loss: 0.637, r2: -0.695, rate:0.688, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:31 step:7600 train loss: 2.766, r2: -0.278, rate:0.750, acc:0.812 val loss: 0.934, r2: -0.479, rate:0.688, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:31 step:7700 train loss: 0.913, r2: -4.928, rate:0.438, acc:0.938 val loss: 0.733, r2: -0.091, rate:0.750, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:31 step:7800 train loss: 0.837, r2: -0.289, rate:0.438, acc:0.938 val loss: 0.785, r2: 0.280, rate:0.688, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:31 step:0.328 lr 0.000000\n",
      "epoch:32 step:0 train loss: 0.995, r2: -0.358, rate:0.688, acc:0.250 val loss: 2.728, r2: -0.205, rate:0.625, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:32 step:100 train loss: 0.634, r2: 0.293, rate:0.812, acc:1.000 val loss: 0.847, r2: 0.067, rate:0.500, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:32 step:200 train loss: 0.652, r2: 0.257, rate:0.750, acc:0.688 val loss: 1.056, r2: 0.205, rate:0.562, acc:0.625 lr 9.534187405839861e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:32 step:300 train loss: 0.971, r2: -0.810, rate:0.562, acc:0.625 val loss: 1.046, r2: -0.647, rate:0.562, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:32 step:400 train loss: 0.552, r2: 0.243, rate:0.875, acc:0.875 val loss: 1.817, r2: -1.394, rate:0.500, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:32 step:500 train loss: 1.399, r2: -2.920, rate:0.500, acc:0.875 val loss: 0.945, r2: 0.147, rate:0.688, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:32 step:600 train loss: 0.687, r2: 0.244, rate:0.875, acc:0.875 val loss: 2.803, r2: -1.401, rate:0.750, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:32 step:700 train loss: 0.992, r2: -0.176, rate:0.688, acc:0.812 val loss: 1.783, r2: -6.572, rate:0.375, acc:0.125 lr 9.534187405839861e-08\n",
      "epoch:32 step:800 train loss: 2.690, r2: 0.442, rate:0.875, acc:0.750 val loss: 1.429, r2: -4.490, rate:0.750, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:32 step:900 train loss: 0.630, r2: 0.257, rate:0.750, acc:0.688 val loss: 2.294, r2: -0.031, rate:0.375, acc:0.250 lr 9.534187405839861e-08\n",
      "epoch:32 step:1000 train loss: 0.662, r2: 0.747, rate:0.688, acc:0.562 val loss: 2.314, r2: -2.197, rate:0.688, acc:0.250 lr 9.534187405839861e-08\n",
      "epoch:32 step:1100 train loss: 0.646, r2: 0.557, rate:0.750, acc:0.812 val loss: 1.568, r2: 0.070, rate:0.750, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:32 step:1200 train loss: 0.638, r2: 0.400, rate:0.812, acc:0.562 val loss: 1.157, r2: -0.149, rate:0.500, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:32 step:1300 train loss: 1.714, r2: -1.381, rate:0.625, acc:0.812 val loss: 0.658, r2: 0.442, rate:0.750, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:32 step:1400 train loss: 0.551, r2: 0.514, rate:0.938, acc:0.875 val loss: 1.295, r2: -2.065, rate:0.375, acc:0.125 lr 9.534187405839861e-08\n",
      "epoch:32 step:1500 train loss: 1.434, r2: -1.559, rate:0.562, acc:0.938 val loss: 2.569, r2: 0.404, rate:0.875, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:32 step:1600 train loss: 0.573, r2: 0.526, rate:0.750, acc:0.750 val loss: 0.647, r2: 0.226, rate:0.875, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:32 step:1700 train loss: 0.801, r2: 0.581, rate:0.750, acc:0.688 val loss: 0.925, r2: -0.336, rate:0.625, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:32 step:1800 train loss: 0.633, r2: 0.627, rate:0.750, acc:0.688 val loss: 1.673, r2: -0.213, rate:0.438, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:32 step:1900 train loss: 0.949, r2: -0.863, rate:0.438, acc:0.562 val loss: 1.113, r2: 0.230, rate:0.812, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:32 step:2000 train loss: 0.786, r2: -1.294, rate:0.562, acc:0.500 val loss: 0.971, r2: -1.534, rate:0.812, acc:1.000 lr 9.534187405839861e-08\n",
      "epoch:32 step:2100 train loss: 0.651, r2: -0.362, rate:0.938, acc:0.938 val loss: 0.821, r2: -1.571, rate:0.562, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:32 step:2200 train loss: 1.138, r2: 0.124, rate:0.625, acc:0.500 val loss: 0.850, r2: -0.025, rate:0.562, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:32 step:2300 train loss: 1.184, r2: -0.468, rate:0.562, acc:0.750 val loss: 0.658, r2: 0.657, rate:0.812, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:32 step:2400 train loss: 3.806, r2: -0.593, rate:0.688, acc:0.750 val loss: 1.111, r2: 0.027, rate:0.875, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:32 step:2500 train loss: 0.664, r2: 0.259, rate:0.688, acc:0.500 val loss: 0.606, r2: 0.276, rate:0.938, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:32 step:2600 train loss: 1.752, r2: -2.579, rate:0.562, acc:0.625 val loss: 1.148, r2: -1.959, rate:0.438, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:32 step:2700 train loss: 0.643, r2: 0.468, rate:0.875, acc:0.938 val loss: 1.477, r2: -1.467, rate:0.688, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:32 step:2800 train loss: 0.758, r2: 0.054, rate:0.562, acc:0.688 val loss: 2.035, r2: 0.161, rate:0.625, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:32 step:2900 train loss: 0.516, r2: 0.792, rate:0.938, acc:0.875 val loss: 0.839, r2: -1.986, rate:0.562, acc:0.312 lr 9.534187405839861e-08\n",
      "epoch:32 step:3000 train loss: 0.812, r2: -0.927, rate:0.562, acc:0.375 val loss: 1.359, r2: 0.459, rate:0.750, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:32 step:3100 train loss: 3.209, r2: 0.018, rate:0.812, acc:0.625 val loss: 0.847, r2: -3.772, rate:0.812, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:32 step:3200 train loss: 0.848, r2: 0.202, rate:0.875, acc:1.000 val loss: 1.503, r2: 0.046, rate:0.500, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:32 step:3300 train loss: 2.573, r2: -4.344, rate:0.500, acc:0.500 val loss: 1.108, r2: -0.186, rate:0.688, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:32 step:3400 train loss: 1.202, r2: -0.431, rate:0.562, acc:0.625 val loss: 1.224, r2: -0.109, rate:0.438, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:32 step:3500 train loss: 1.158, r2: -0.377, rate:0.625, acc:0.625 val loss: 1.003, r2: 0.451, rate:0.750, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:32 step:3600 train loss: 1.350, r2: -1.467, rate:0.375, acc:0.438 val loss: 0.964, r2: 0.115, rate:0.625, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:32 step:3700 train loss: 0.862, r2: 0.344, rate:0.500, acc:0.750 val loss: 0.813, r2: 0.024, rate:0.812, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:32 step:3800 train loss: 0.915, r2: 0.256, rate:0.688, acc:0.438 val loss: 0.873, r2: -0.393, rate:0.812, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:32 step:3900 train loss: 0.700, r2: -0.193, rate:0.875, acc:0.875 val loss: 0.713, r2: -0.622, rate:0.688, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:32 step:4000 train loss: 0.909, r2: 0.001, rate:0.688, acc:0.750 val loss: 0.843, r2: -0.411, rate:0.500, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:32 step:4100 train loss: 0.697, r2: 0.092, rate:0.562, acc:0.812 val loss: 1.253, r2: -1.193, rate:0.438, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:32 step:4200 train loss: 1.574, r2: -25.753, rate:0.438, acc:0.812 val loss: 0.845, r2: 0.450, rate:0.750, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:32 step:4300 train loss: 0.718, r2: -0.507, rate:0.750, acc:0.562 val loss: 0.914, r2: 0.323, rate:0.625, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:32 step:4400 train loss: 2.103, r2: 0.284, rate:0.625, acc:0.625 val loss: 1.363, r2: -0.710, rate:0.500, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:32 step:4500 train loss: 0.992, r2: -14.567, rate:0.312, acc:0.500 val loss: 0.527, r2: 0.722, rate:0.875, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:32 step:4600 train loss: 0.616, r2: 0.580, rate:0.750, acc:0.938 val loss: 1.361, r2: -0.139, rate:0.625, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:32 step:4700 train loss: 0.785, r2: 0.056, rate:0.562, acc:0.750 val loss: 1.191, r2: -0.507, rate:0.438, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:32 step:4800 train loss: 1.541, r2: -0.456, rate:0.688, acc:0.938 val loss: 0.593, r2: 0.551, rate:0.750, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:32 step:4900 train loss: 5.877, r2: -2.973, rate:0.562, acc:0.938 val loss: 1.293, r2: -0.156, rate:0.750, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:32 step:5000 train loss: 0.854, r2: 0.650, rate:0.688, acc:0.688 val loss: 0.574, r2: 0.659, rate:0.750, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:32 step:5100 train loss: 0.732, r2: 0.580, rate:0.750, acc:0.688 val loss: 0.640, r2: 0.419, rate:0.938, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:32 step:5200 train loss: 0.683, r2: 0.406, rate:0.688, acc:0.438 val loss: 0.614, r2: 0.539, rate:0.875, acc:1.000 lr 9.534187405839861e-08\n",
      "epoch:32 step:5300 train loss: 1.666, r2: -2.051, rate:0.750, acc:0.938 val loss: 1.334, r2: -0.148, rate:0.562, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:32 step:5400 train loss: 3.202, r2: -6.938, rate:0.500, acc:0.375 val loss: 1.074, r2: -2.652, rate:0.562, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:32 step:5500 train loss: 1.223, r2: 0.213, rate:0.812, acc:0.750 val loss: 0.769, r2: -0.054, rate:0.812, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:32 step:5600 train loss: 0.934, r2: -1.870, rate:0.438, acc:0.188 val loss: 1.169, r2: -1.754, rate:0.500, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:32 step:5700 train loss: 3.054, r2: 0.304, rate:0.562, acc:0.562 val loss: 1.285, r2: 0.079, rate:0.625, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:32 step:5800 train loss: 0.900, r2: 0.671, rate:0.750, acc:0.812 val loss: 0.688, r2: -0.349, rate:0.625, acc:0.625 lr 9.534187405839861e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:32 step:5900 train loss: 0.840, r2: 0.322, rate:0.875, acc:0.812 val loss: 1.001, r2: -0.526, rate:0.688, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:32 step:6000 train loss: 0.578, r2: 0.265, rate:0.875, acc:0.812 val loss: 1.054, r2: -0.437, rate:0.500, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:32 step:6100 train loss: 0.806, r2: -0.445, rate:0.625, acc:0.438 val loss: 1.175, r2: 0.554, rate:0.688, acc:0.375 lr 9.534187405839861e-08\n",
      "epoch:32 step:6200 train loss: 3.799, r2: -11.998, rate:0.875, acc:0.750 val loss: 1.249, r2: -1.190, rate:0.812, acc:1.000 lr 9.534187405839861e-08\n",
      "epoch:32 step:6300 train loss: 0.849, r2: 0.645, rate:0.938, acc:0.688 val loss: 1.179, r2: -1.689, rate:0.750, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:32 step:6400 train loss: 0.592, r2: 0.496, rate:0.750, acc:0.875 val loss: 0.986, r2: 0.341, rate:0.750, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:32 step:6500 train loss: 0.972, r2: -0.325, rate:0.688, acc:0.312 val loss: 0.995, r2: 0.600, rate:0.688, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:32 step:6600 train loss: 0.868, r2: -1.287, rate:0.500, acc:0.375 val loss: 0.854, r2: 0.118, rate:0.688, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:32 step:6700 train loss: 1.554, r2: 0.339, rate:0.625, acc:0.438 val loss: 0.910, r2: -0.230, rate:0.750, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:32 step:6800 train loss: 0.734, r2: -0.180, rate:0.562, acc:0.812 val loss: 0.811, r2: 0.585, rate:0.812, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:32 step:6900 train loss: 0.658, r2: 0.214, rate:0.500, acc:0.500 val loss: 0.846, r2: 0.187, rate:0.750, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:32 step:7000 train loss: 0.892, r2: 0.224, rate:0.688, acc:0.688 val loss: 0.782, r2: -0.518, rate:0.750, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:32 step:7100 train loss: 4.626, r2: -6.132, rate:0.750, acc:0.750 val loss: 1.333, r2: -1.085, rate:0.438, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:32 step:7200 train loss: 0.803, r2: -0.993, rate:0.625, acc:0.750 val loss: 0.728, r2: 0.386, rate:0.750, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:32 step:7300 train loss: 0.873, r2: 0.387, rate:0.812, acc:0.875 val loss: 0.932, r2: 0.224, rate:0.688, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:32 step:7400 train loss: 4.276, r2: 0.305, rate:0.750, acc:0.812 val loss: 2.250, r2: 0.300, rate:0.625, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:32 step:7500 train loss: 0.860, r2: 0.423, rate:0.750, acc:0.625 val loss: 0.618, r2: 0.707, rate:0.812, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:32 step:7600 train loss: 1.126, r2: -0.811, rate:0.438, acc:0.812 val loss: 0.888, r2: 0.548, rate:0.875, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:32 step:7700 train loss: 0.968, r2: 0.380, rate:0.812, acc:0.812 val loss: 2.141, r2: -9.492, rate:0.688, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:32 step:7800 train loss: 0.785, r2: 0.209, rate:0.500, acc:0.625 val loss: 0.634, r2: 0.608, rate:0.938, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:32 step:0.328 lr 0.000000\n",
      "epoch:33 step:0 train loss: 0.619, r2: 0.224, rate:0.750, acc:0.625 val loss: 0.706, r2: -0.036, rate:0.750, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:33 step:100 train loss: 3.528, r2: 0.404, rate:0.875, acc:0.688 val loss: 0.793, r2: 0.470, rate:0.625, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:33 step:200 train loss: 0.996, r2: -0.103, rate:0.750, acc:0.500 val loss: 0.943, r2: -1.503, rate:0.625, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:33 step:300 train loss: 0.570, r2: 0.712, rate:0.938, acc:0.500 val loss: 0.723, r2: -0.934, rate:0.500, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:33 step:400 train loss: 5.742, r2: -7.703, rate:0.688, acc:1.000 val loss: 0.616, r2: 0.729, rate:1.000, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:33 step:500 train loss: 0.628, r2: -6.394, rate:0.812, acc:0.875 val loss: 1.203, r2: -2.178, rate:0.500, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:33 step:600 train loss: 1.047, r2: -6.187, rate:0.688, acc:0.875 val loss: 1.215, r2: -0.132, rate:0.375, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:33 step:700 train loss: 0.952, r2: -2.001, rate:0.688, acc:0.812 val loss: 0.645, r2: 0.562, rate:0.938, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:33 step:800 train loss: 1.073, r2: 0.362, rate:0.750, acc:0.750 val loss: 0.945, r2: 0.181, rate:0.688, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:33 step:900 train loss: 0.852, r2: 0.204, rate:0.875, acc:0.562 val loss: 1.288, r2: -3.361, rate:1.000, acc:1.000 lr 9.534187405839861e-08\n",
      "epoch:33 step:1000 train loss: 0.981, r2: -11.864, rate:0.500, acc:0.688 val loss: 0.792, r2: -0.060, rate:0.938, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:33 step:1100 train loss: 0.679, r2: -1.007, rate:0.938, acc:0.938 val loss: 0.873, r2: -0.212, rate:0.750, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:33 step:1200 train loss: 0.941, r2: 0.139, rate:0.562, acc:0.375 val loss: 0.973, r2: -0.527, rate:0.688, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:33 step:1300 train loss: 5.068, r2: -2.877, rate:0.500, acc:0.375 val loss: 0.935, r2: 0.255, rate:0.750, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:33 step:1400 train loss: 0.558, r2: -2.304, rate:0.938, acc:1.000 val loss: 1.100, r2: 0.647, rate:0.875, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:33 step:1500 train loss: 1.002, r2: -0.555, rate:0.625, acc:0.688 val loss: 0.864, r2: -0.113, rate:0.625, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:33 step:1600 train loss: 0.611, r2: 0.548, rate:0.812, acc:0.625 val loss: 0.716, r2: 0.473, rate:0.562, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:33 step:1700 train loss: 1.371, r2: -0.172, rate:0.688, acc:0.688 val loss: 0.614, r2: 0.337, rate:0.812, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:33 step:1800 train loss: 0.723, r2: -1.434, rate:0.625, acc:0.875 val loss: 0.919, r2: 0.257, rate:0.688, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:33 step:1900 train loss: 0.794, r2: 0.291, rate:0.688, acc:1.000 val loss: 0.798, r2: 0.328, rate:0.688, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:33 step:2000 train loss: 0.659, r2: 0.687, rate:0.812, acc:0.938 val loss: 0.976, r2: 0.367, rate:0.750, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:33 step:2100 train loss: 1.754, r2: -0.781, rate:0.750, acc:0.750 val loss: 1.064, r2: -0.710, rate:0.688, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:33 step:2200 train loss: 0.690, r2: -0.601, rate:0.500, acc:0.812 val loss: 0.811, r2: -1.605, rate:0.500, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:33 step:2300 train loss: 1.118, r2: 0.200, rate:0.688, acc:0.688 val loss: 0.906, r2: -0.487, rate:0.562, acc:0.375 lr 9.534187405839861e-08\n",
      "epoch:33 step:2400 train loss: 0.649, r2: 0.311, rate:0.625, acc:0.500 val loss: 0.681, r2: 0.485, rate:0.750, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:33 step:2500 train loss: 0.796, r2: -0.237, rate:0.625, acc:0.312 val loss: 0.741, r2: 0.576, rate:0.688, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:33 step:2600 train loss: 1.124, r2: -0.359, rate:0.500, acc:0.500 val loss: 0.867, r2: 0.612, rate:0.688, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:33 step:2700 train loss: 0.582, r2: -0.205, rate:0.875, acc:0.875 val loss: 0.955, r2: 0.425, rate:0.688, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:33 step:2800 train loss: 2.449, r2: 0.512, rate:0.812, acc:0.688 val loss: 1.339, r2: -1.062, rate:0.688, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:33 step:2900 train loss: 1.441, r2: -4.909, rate:0.375, acc:0.438 val loss: 1.518, r2: 0.330, rate:0.688, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:33 step:3000 train loss: 0.964, r2: -0.204, rate:0.500, acc:0.938 val loss: 0.988, r2: 0.288, rate:0.750, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:33 step:3100 train loss: 0.610, r2: 0.241, rate:0.812, acc:0.750 val loss: 1.516, r2: -0.161, rate:0.750, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:33 step:3200 train loss: 0.760, r2: -2.143, rate:0.562, acc:0.625 val loss: 1.417, r2: -0.659, rate:0.500, acc:0.312 lr 9.534187405839861e-08\n",
      "epoch:33 step:3300 train loss: 0.928, r2: 0.372, rate:0.625, acc:0.500 val loss: 0.689, r2: 0.648, rate:0.812, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:33 step:3400 train loss: 0.726, r2: -5.927, rate:0.562, acc:0.688 val loss: 1.815, r2: -1.410, rate:0.625, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:33 step:3500 train loss: 3.918, r2: -17.870, rate:0.438, acc:0.875 val loss: 0.590, r2: -0.177, rate:0.625, acc:0.875 lr 9.534187405839861e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:33 step:3600 train loss: 0.573, r2: -0.017, rate:0.750, acc:0.938 val loss: 1.624, r2: 0.475, rate:0.688, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:33 step:3700 train loss: 0.744, r2: -15.957, rate:0.562, acc:0.562 val loss: 0.787, r2: 0.404, rate:0.812, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:33 step:3800 train loss: 0.696, r2: 0.021, rate:0.750, acc:0.750 val loss: 1.201, r2: 0.338, rate:0.562, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:33 step:3900 train loss: 1.224, r2: -0.789, rate:0.688, acc:0.250 val loss: 0.763, r2: 0.365, rate:0.625, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:33 step:4000 train loss: 1.044, r2: -0.427, rate:0.625, acc:0.500 val loss: 0.575, r2: 0.800, rate:0.812, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:33 step:4100 train loss: 0.637, r2: 0.603, rate:0.750, acc:0.875 val loss: 0.941, r2: 0.436, rate:0.625, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:33 step:4200 train loss: 0.865, r2: 0.677, rate:0.812, acc:0.562 val loss: 0.741, r2: -0.288, rate:0.625, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:33 step:4300 train loss: 0.849, r2: -6.222, rate:0.500, acc:0.062 val loss: 2.482, r2: -0.108, rate:0.812, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:33 step:4400 train loss: 2.494, r2: -1.686, rate:0.750, acc:0.938 val loss: 1.108, r2: -0.293, rate:0.688, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:33 step:4500 train loss: 0.638, r2: 0.540, rate:0.875, acc:0.375 val loss: 2.507, r2: -0.966, rate:0.812, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:33 step:4600 train loss: 1.036, r2: 0.203, rate:0.562, acc:0.438 val loss: 0.698, r2: -0.243, rate:0.688, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:33 step:4700 train loss: 1.329, r2: -1.666, rate:0.625, acc:0.375 val loss: 1.740, r2: -2.395, rate:0.625, acc:0.375 lr 9.534187405839861e-08\n",
      "epoch:33 step:4800 train loss: 0.853, r2: 0.219, rate:0.750, acc:0.562 val loss: 0.898, r2: -0.231, rate:0.812, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:33 step:4900 train loss: 2.352, r2: -0.572, rate:0.875, acc:1.000 val loss: 0.580, r2: 0.623, rate:0.812, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:33 step:5000 train loss: 1.237, r2: -0.689, rate:0.625, acc:0.375 val loss: 1.082, r2: 0.160, rate:0.562, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:33 step:5100 train loss: 1.286, r2: 0.135, rate:0.500, acc:0.438 val loss: 0.703, r2: 0.077, rate:0.625, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:33 step:5200 train loss: 0.513, r2: -0.278, rate:0.875, acc:0.875 val loss: 1.082, r2: -0.101, rate:0.875, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:33 step:5300 train loss: 5.238, r2: -4.513, rate:0.812, acc:0.750 val loss: 0.795, r2: -0.873, rate:0.562, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:33 step:5400 train loss: 0.634, r2: 0.666, rate:0.812, acc:0.562 val loss: 0.759, r2: -0.235, rate:0.625, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:33 step:5500 train loss: 3.005, r2: -0.494, rate:0.688, acc:0.812 val loss: 0.577, r2: -0.253, rate:0.750, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:33 step:5600 train loss: 1.373, r2: -0.343, rate:0.375, acc:0.375 val loss: 0.716, r2: -3.302, rate:0.625, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:33 step:5700 train loss: 0.702, r2: 0.185, rate:0.562, acc:0.750 val loss: 2.736, r2: -0.054, rate:0.562, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:33 step:5800 train loss: 1.391, r2: 0.114, rate:0.688, acc:0.438 val loss: 0.868, r2: -3.523, rate:0.562, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:33 step:5900 train loss: 0.686, r2: 0.246, rate:0.875, acc:0.438 val loss: 0.861, r2: 0.230, rate:0.625, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:33 step:6000 train loss: 0.825, r2: -0.153, rate:0.688, acc:0.625 val loss: 0.700, r2: 0.437, rate:0.750, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:33 step:6100 train loss: 0.812, r2: -0.832, rate:0.625, acc:0.500 val loss: 0.654, r2: 0.029, rate:0.625, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:33 step:6200 train loss: 0.509, r2: 0.179, rate:1.000, acc:0.938 val loss: 1.139, r2: 0.410, rate:0.625, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:33 step:6300 train loss: 0.606, r2: -0.854, rate:0.562, acc:0.812 val loss: 0.678, r2: -1.082, rate:0.500, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:33 step:6400 train loss: 0.573, r2: 0.745, rate:0.875, acc:0.812 val loss: 0.846, r2: 0.179, rate:0.625, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:33 step:6500 train loss: 5.493, r2: -2.846, rate:0.750, acc:0.500 val loss: 0.730, r2: 0.735, rate:0.750, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:33 step:6600 train loss: 1.006, r2: -2.090, rate:0.250, acc:0.625 val loss: 1.884, r2: -0.348, rate:0.500, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:33 step:6700 train loss: 1.842, r2: -0.049, rate:0.938, acc:0.938 val loss: 0.911, r2: 0.316, rate:0.688, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:33 step:6800 train loss: 0.633, r2: 0.701, rate:0.812, acc:0.438 val loss: 1.304, r2: 0.249, rate:0.625, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:33 step:6900 train loss: 1.021, r2: 0.667, rate:0.750, acc:0.875 val loss: 0.994, r2: -0.166, rate:0.438, acc:0.375 lr 9.534187405839861e-08\n",
      "epoch:33 step:7000 train loss: 1.255, r2: -1.154, rate:0.562, acc:0.500 val loss: 1.428, r2: -0.014, rate:0.812, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:33 step:7100 train loss: 2.007, r2: -2.456, rate:0.812, acc:0.875 val loss: 0.825, r2: 0.135, rate:0.750, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:33 step:7200 train loss: 0.623, r2: 0.535, rate:0.562, acc:0.500 val loss: 0.679, r2: 0.532, rate:0.625, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:33 step:7300 train loss: 1.112, r2: 0.373, rate:0.438, acc:0.625 val loss: 1.401, r2: 0.067, rate:0.750, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:33 step:7400 train loss: 1.056, r2: -1.345, rate:0.438, acc:0.375 val loss: 0.920, r2: -0.382, rate:0.375, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:33 step:7500 train loss: 0.717, r2: -0.128, rate:0.812, acc:0.500 val loss: 0.580, r2: 0.130, rate:0.625, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:33 step:7600 train loss: 0.612, r2: 0.790, rate:1.000, acc:0.562 val loss: 0.707, r2: -0.004, rate:0.812, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:33 step:7700 train loss: 1.571, r2: -0.129, rate:0.750, acc:0.625 val loss: 1.237, r2: -0.195, rate:0.688, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:33 step:7800 train loss: 0.791, r2: -0.305, rate:0.562, acc:0.375 val loss: 1.434, r2: 0.396, rate:0.688, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:33 step:0.328 lr 0.000000\n",
      "epoch:34 step:0 train loss: 0.595, r2: 0.589, rate:0.875, acc:0.625 val loss: 0.657, r2: -1.951, rate:0.750, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:34 step:100 train loss: 0.865, r2: 0.058, rate:0.875, acc:0.812 val loss: 0.774, r2: 0.481, rate:0.625, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:34 step:200 train loss: 1.089, r2: -1.353, rate:0.312, acc:0.250 val loss: 1.142, r2: 0.308, rate:0.625, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:34 step:300 train loss: 2.943, r2: -4.585, rate:0.562, acc:0.812 val loss: 1.076, r2: -0.247, rate:0.875, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:34 step:400 train loss: 1.740, r2: 0.011, rate:0.562, acc:0.438 val loss: 0.956, r2: -0.049, rate:0.812, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:34 step:500 train loss: 0.596, r2: 0.551, rate:0.938, acc:0.688 val loss: 2.888, r2: -0.181, rate:0.625, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:34 step:600 train loss: 1.946, r2: 0.623, rate:0.625, acc:0.562 val loss: 0.865, r2: 0.117, rate:0.562, acc:0.312 lr 9.534187405839861e-08\n",
      "epoch:34 step:700 train loss: 0.609, r2: 0.587, rate:0.875, acc:0.688 val loss: 1.462, r2: 0.319, rate:0.750, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:34 step:800 train loss: 0.905, r2: 0.504, rate:0.625, acc:0.688 val loss: 1.011, r2: -0.108, rate:0.562, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:34 step:900 train loss: 1.018, r2: -3.734, rate:0.625, acc:0.375 val loss: 0.777, r2: -0.465, rate:0.688, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:34 step:1000 train loss: 0.946, r2: 0.532, rate:0.500, acc:1.000 val loss: 0.789, r2: -0.764, rate:0.688, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:34 step:1100 train loss: 0.810, r2: -0.378, rate:0.750, acc:0.812 val loss: 0.841, r2: -1.468, rate:0.625, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:34 step:1200 train loss: 0.882, r2: -1.083, rate:0.625, acc:0.438 val loss: 0.624, r2: 0.344, rate:0.750, acc:0.875 lr 9.534187405839861e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:34 step:1300 train loss: 1.017, r2: -0.480, rate:0.812, acc:1.000 val loss: 1.038, r2: 0.109, rate:0.625, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:34 step:1400 train loss: 1.221, r2: -1.497, rate:0.312, acc:0.125 val loss: 0.554, r2: 0.761, rate:0.875, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:34 step:1500 train loss: 1.933, r2: -0.436, rate:0.500, acc:0.500 val loss: 1.162, r2: -0.529, rate:0.625, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:34 step:1600 train loss: 0.784, r2: 0.280, rate:0.875, acc:0.688 val loss: 1.266, r2: -0.351, rate:0.875, acc:1.000 lr 9.534187405839861e-08\n",
      "epoch:34 step:1700 train loss: 1.152, r2: -2.892, rate:0.500, acc:0.500 val loss: 0.973, r2: 0.086, rate:0.500, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:34 step:1800 train loss: 0.631, r2: -2.737, rate:0.562, acc:0.938 val loss: 1.400, r2: -0.155, rate:0.750, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:34 step:1900 train loss: 0.835, r2: -0.681, rate:0.562, acc:0.312 val loss: 1.727, r2: -2.827, rate:0.500, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:34 step:2000 train loss: 0.649, r2: -0.012, rate:0.875, acc:0.438 val loss: 0.934, r2: -0.625, rate:0.438, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:34 step:2100 train loss: 1.223, r2: 0.268, rate:0.750, acc:0.812 val loss: 0.553, r2: 0.507, rate:0.938, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:34 step:2200 train loss: 0.819, r2: -0.052, rate:0.812, acc:0.625 val loss: 1.300, r2: 0.252, rate:0.625, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:34 step:2300 train loss: 1.205, r2: 0.024, rate:0.500, acc:0.500 val loss: 1.109, r2: -8.667, rate:0.688, acc:0.312 lr 9.534187405839861e-08\n",
      "epoch:34 step:2400 train loss: 1.066, r2: 0.103, rate:0.438, acc:0.438 val loss: 1.741, r2: 0.139, rate:0.688, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:34 step:2500 train loss: 0.727, r2: -0.166, rate:0.812, acc:0.500 val loss: 0.867, r2: -0.065, rate:0.875, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:34 step:2600 train loss: 2.206, r2: -9.982, rate:1.000, acc:0.812 val loss: 0.702, r2: 0.690, rate:0.875, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:34 step:2700 train loss: 0.686, r2: 0.704, rate:0.688, acc:0.562 val loss: 1.077, r2: 0.078, rate:0.625, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:34 step:2800 train loss: 0.695, r2: 0.515, rate:0.750, acc:0.750 val loss: 1.817, r2: 0.256, rate:0.688, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:34 step:2900 train loss: 0.931, r2: 0.401, rate:0.875, acc:0.875 val loss: 0.754, r2: 0.431, rate:0.625, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:34 step:3000 train loss: 0.997, r2: -0.793, rate:0.688, acc:0.875 val loss: 0.760, r2: 0.227, rate:0.875, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:34 step:3100 train loss: 1.546, r2: -6.472, rate:0.875, acc:1.000 val loss: 0.935, r2: 0.468, rate:0.688, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:34 step:3200 train loss: 0.880, r2: 0.395, rate:0.688, acc:0.625 val loss: 0.740, r2: 0.290, rate:0.688, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:34 step:3300 train loss: 0.736, r2: -1.976, rate:0.688, acc:0.438 val loss: 0.705, r2: -0.172, rate:0.750, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:34 step:3400 train loss: 1.682, r2: -0.652, rate:0.750, acc:0.500 val loss: 0.837, r2: 0.290, rate:0.750, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:34 step:3500 train loss: 1.329, r2: -0.537, rate:0.500, acc:0.375 val loss: 1.270, r2: -0.733, rate:0.562, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:34 step:3600 train loss: 0.768, r2: -3.163, rate:0.688, acc:0.062 val loss: 0.623, r2: 0.530, rate:0.750, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:34 step:3700 train loss: 0.839, r2: -0.997, rate:0.625, acc:0.188 val loss: 0.691, r2: 0.671, rate:0.875, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:34 step:3800 train loss: 5.652, r2: -3.729, rate:0.812, acc:0.375 val loss: 1.088, r2: -0.149, rate:0.688, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:34 step:3900 train loss: 0.831, r2: 0.360, rate:0.688, acc:0.312 val loss: 0.831, r2: -0.167, rate:0.562, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:34 step:4000 train loss: 1.078, r2: -0.199, rate:0.562, acc:0.875 val loss: 0.638, r2: -0.304, rate:0.688, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:34 step:4100 train loss: 1.137, r2: -1.079, rate:0.750, acc:0.375 val loss: 0.616, r2: -0.114, rate:0.625, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:34 step:4200 train loss: 0.710, r2: 0.198, rate:0.562, acc:0.875 val loss: 0.791, r2: 0.295, rate:0.562, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:34 step:4300 train loss: 0.782, r2: 0.014, rate:0.562, acc:0.750 val loss: 0.901, r2: 0.016, rate:0.688, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:34 step:4400 train loss: 1.011, r2: -0.232, rate:0.562, acc:0.375 val loss: 0.914, r2: 0.522, rate:0.625, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:34 step:4500 train loss: 0.669, r2: 0.052, rate:0.625, acc:0.875 val loss: 0.780, r2: -0.362, rate:0.625, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:34 step:4600 train loss: 1.718, r2: -0.896, rate:0.500, acc:0.250 val loss: 0.779, r2: -1.238, rate:0.750, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:34 step:4700 train loss: 0.859, r2: -0.305, rate:0.688, acc:0.562 val loss: 0.966, r2: 0.256, rate:0.562, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:34 step:4800 train loss: 0.639, r2: 0.453, rate:0.875, acc:0.438 val loss: 0.875, r2: -0.206, rate:0.562, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:34 step:4900 train loss: 0.922, r2: -2.759, rate:0.875, acc:1.000 val loss: 1.289, r2: -1.099, rate:0.625, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:34 step:5000 train loss: 1.912, r2: -1.617, rate:0.875, acc:0.938 val loss: 1.040, r2: -0.493, rate:0.562, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:34 step:5100 train loss: 1.816, r2: -0.650, rate:0.812, acc:0.500 val loss: 0.975, r2: -0.635, rate:0.812, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:34 step:5200 train loss: 0.711, r2: 0.461, rate:0.562, acc:0.375 val loss: 1.863, r2: -3.595, rate:0.500, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:34 step:5300 train loss: 1.712, r2: -6.755, rate:0.812, acc:1.000 val loss: 2.763, r2: -0.081, rate:0.688, acc:0.375 lr 9.534187405839861e-08\n",
      "epoch:34 step:5400 train loss: 0.639, r2: 0.098, rate:0.688, acc:0.812 val loss: 0.533, r2: 0.618, rate:0.875, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:34 step:5500 train loss: 1.692, r2: -1.949, rate:0.750, acc:0.875 val loss: 0.712, r2: -0.437, rate:0.688, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:34 step:5600 train loss: 1.096, r2: -2.693, rate:0.438, acc:0.938 val loss: 1.181, r2: -0.164, rate:0.938, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:34 step:5700 train loss: 1.232, r2: -2.272, rate:0.438, acc:0.500 val loss: 0.750, r2: -0.293, rate:0.875, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:34 step:5800 train loss: 3.154, r2: -3.101, rate:0.625, acc:0.500 val loss: 0.779, r2: 0.541, rate:0.500, acc:0.312 lr 9.534187405839861e-08\n",
      "epoch:34 step:5900 train loss: 1.350, r2: -0.560, rate:0.312, acc:0.250 val loss: 0.587, r2: 0.600, rate:0.750, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:34 step:6000 train loss: 0.805, r2: -0.088, rate:0.625, acc:0.438 val loss: 1.343, r2: -0.421, rate:0.688, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:34 step:6100 train loss: 0.901, r2: 0.088, rate:0.750, acc:0.938 val loss: 0.768, r2: 0.693, rate:0.938, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:34 step:6200 train loss: 1.423, r2: -0.666, rate:0.625, acc:0.312 val loss: 0.849, r2: -1.135, rate:0.938, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:34 step:6300 train loss: 0.599, r2: 0.214, rate:0.812, acc:0.625 val loss: 0.998, r2: -0.582, rate:0.688, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:34 step:6400 train loss: 0.780, r2: -0.559, rate:0.875, acc:0.875 val loss: 1.461, r2: -1.412, rate:0.500, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:34 step:6500 train loss: 1.509, r2: -1.885, rate:0.625, acc:0.938 val loss: 0.824, r2: 0.568, rate:0.625, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:34 step:6600 train loss: 0.665, r2: 0.361, rate:0.750, acc:0.500 val loss: 1.054, r2: -1.109, rate:0.500, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:34 step:6700 train loss: 3.267, r2: -3.661, rate:0.312, acc:0.250 val loss: 0.812, r2: 0.351, rate:0.750, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:34 step:6800 train loss: 1.187, r2: -2.368, rate:0.938, acc:1.000 val loss: 0.576, r2: 0.814, rate:0.812, acc:0.750 lr 9.534187405839861e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:34 step:6900 train loss: 0.670, r2: -0.778, rate:0.688, acc:0.625 val loss: 1.181, r2: -0.816, rate:0.688, acc:0.375 lr 9.534187405839861e-08\n",
      "epoch:34 step:7000 train loss: 0.759, r2: -0.499, rate:0.500, acc:0.250 val loss: 1.203, r2: -0.720, rate:0.438, acc:0.375 lr 9.534187405839861e-08\n",
      "epoch:34 step:7100 train loss: 1.875, r2: 0.482, rate:0.812, acc:0.812 val loss: 1.557, r2: -0.281, rate:0.938, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:34 step:7200 train loss: 0.612, r2: 0.405, rate:0.875, acc:0.938 val loss: 1.535, r2: -5.004, rate:0.562, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:34 step:7300 train loss: 2.181, r2: -1.751, rate:0.562, acc:0.562 val loss: 2.811, r2: -1.109, rate:0.562, acc:0.375 lr 9.534187405839861e-08\n",
      "epoch:34 step:7400 train loss: 0.567, r2: 0.651, rate:0.875, acc:0.750 val loss: 0.815, r2: 0.376, rate:0.812, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:34 step:7500 train loss: 0.709, r2: -0.155, rate:0.562, acc:0.750 val loss: 1.085, r2: -0.291, rate:0.438, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:34 step:7600 train loss: 2.754, r2: -2.702, rate:0.625, acc:0.375 val loss: 0.717, r2: 0.480, rate:0.812, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:34 step:7700 train loss: 0.816, r2: -0.665, rate:0.562, acc:0.375 val loss: 0.692, r2: 0.099, rate:0.750, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:34 step:7800 train loss: 0.999, r2: -0.206, rate:0.625, acc:0.375 val loss: 1.637, r2: -2.765, rate:0.562, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:34 step:0.328 lr 0.000000\n",
      "epoch:35 step:0 train loss: 0.818, r2: -0.707, rate:0.688, acc:0.812 val loss: 1.108, r2: 0.279, rate:0.562, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:35 step:100 train loss: 1.580, r2: -1.132, rate:0.750, acc:0.750 val loss: 1.141, r2: 0.142, rate:0.938, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:35 step:200 train loss: 0.916, r2: -0.082, rate:0.562, acc:0.312 val loss: 0.899, r2: -0.565, rate:0.938, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:35 step:300 train loss: 0.527, r2: 0.509, rate:0.812, acc:0.750 val loss: 0.541, r2: 0.654, rate:0.875, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:35 step:400 train loss: 4.414, r2: -4.669, rate:0.812, acc:1.000 val loss: 0.787, r2: -0.249, rate:0.750, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:35 step:500 train loss: 0.615, r2: 0.802, rate:0.875, acc:0.875 val loss: 1.120, r2: 0.043, rate:0.562, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:35 step:600 train loss: 0.760, r2: 0.435, rate:0.812, acc:0.500 val loss: 0.687, r2: 0.447, rate:0.688, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:35 step:700 train loss: 0.942, r2: 0.405, rate:0.938, acc:0.875 val loss: 0.767, r2: -3.661, rate:0.688, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:35 step:800 train loss: 1.158, r2: 0.030, rate:0.812, acc:0.812 val loss: 1.213, r2: -1.315, rate:0.562, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:35 step:900 train loss: 0.528, r2: 0.868, rate:0.812, acc:0.875 val loss: 0.736, r2: 0.604, rate:0.750, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:35 step:1000 train loss: 0.713, r2: -0.031, rate:0.562, acc:0.375 val loss: 1.002, r2: 0.441, rate:0.625, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:35 step:1100 train loss: 0.697, r2: 0.092, rate:0.750, acc:0.688 val loss: 0.601, r2: 0.248, rate:0.812, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:35 step:1200 train loss: 1.226, r2: -0.904, rate:0.438, acc:0.750 val loss: 0.987, r2: -3.781, rate:0.562, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:35 step:1300 train loss: 12.118, r2: -0.415, rate:0.938, acc:0.938 val loss: 1.526, r2: -0.395, rate:0.688, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:35 step:1400 train loss: 0.631, r2: 0.104, rate:0.625, acc:0.812 val loss: 0.984, r2: 0.591, rate:0.625, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:35 step:1500 train loss: 0.820, r2: 0.141, rate:0.562, acc:0.375 val loss: 0.639, r2: -0.239, rate:0.688, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:35 step:1600 train loss: 0.600, r2: 0.547, rate:0.875, acc:0.812 val loss: 0.841, r2: -1.818, rate:0.562, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:35 step:1700 train loss: 0.543, r2: 0.177, rate:0.938, acc:0.875 val loss: 1.425, r2: -1.530, rate:0.562, acc:0.375 lr 9.534187405839861e-08\n",
      "epoch:35 step:1800 train loss: 0.777, r2: 0.380, rate:0.688, acc:0.688 val loss: 0.717, r2: 0.206, rate:0.688, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:35 step:1900 train loss: 0.556, r2: 0.804, rate:0.875, acc:0.688 val loss: 0.785, r2: -0.124, rate:0.625, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:35 step:2000 train loss: 0.644, r2: -1.403, rate:0.750, acc:0.875 val loss: 0.527, r2: 0.655, rate:0.875, acc:1.000 lr 9.534187405839861e-08\n",
      "epoch:35 step:2100 train loss: 0.747, r2: -0.994, rate:0.438, acc:0.438 val loss: 0.594, r2: 0.645, rate:0.875, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:35 step:2200 train loss: 0.925, r2: 0.129, rate:0.375, acc:0.438 val loss: 1.269, r2: -1.804, rate:0.688, acc:0.375 lr 9.534187405839861e-08\n",
      "epoch:35 step:2300 train loss: 2.444, r2: -0.337, rate:0.625, acc:0.562 val loss: 0.849, r2: -1.045, rate:0.750, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:35 step:2400 train loss: 0.734, r2: 0.099, rate:0.625, acc:0.625 val loss: 1.045, r2: 0.492, rate:0.500, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:35 step:2500 train loss: 0.493, r2: 0.697, rate:1.000, acc:0.812 val loss: 1.503, r2: -0.967, rate:0.562, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:35 step:2600 train loss: 2.591, r2: -10.164, rate:0.938, acc:0.750 val loss: 0.583, r2: 0.486, rate:0.750, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:35 step:2700 train loss: 0.957, r2: -0.506, rate:0.625, acc:0.312 val loss: 1.173, r2: -0.093, rate:0.875, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:35 step:2800 train loss: 0.819, r2: 0.380, rate:0.562, acc:0.875 val loss: 1.957, r2: -0.239, rate:0.312, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:35 step:2900 train loss: 0.867, r2: 0.193, rate:0.688, acc:0.562 val loss: 0.791, r2: 0.372, rate:0.625, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:35 step:3000 train loss: 1.303, r2: -1.214, rate:0.312, acc:0.438 val loss: 0.731, r2: -0.281, rate:0.750, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:35 step:3100 train loss: 1.030, r2: 0.732, rate:0.938, acc:1.000 val loss: 0.942, r2: -0.026, rate:0.625, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:35 step:3200 train loss: 0.774, r2: 0.376, rate:0.688, acc:0.438 val loss: 0.941, r2: -0.295, rate:0.625, acc:0.375 lr 9.534187405839861e-08\n",
      "epoch:35 step:3300 train loss: 0.595, r2: 0.564, rate:0.688, acc:0.750 val loss: 0.585, r2: 0.512, rate:0.812, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:35 step:3400 train loss: 0.676, r2: 0.059, rate:0.625, acc:0.500 val loss: 1.622, r2: 0.119, rate:0.750, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:35 step:3500 train loss: 0.979, r2: -0.281, rate:0.750, acc:0.938 val loss: 2.499, r2: -5.510, rate:0.875, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:35 step:3600 train loss: 0.988, r2: 0.223, rate:0.688, acc:0.625 val loss: 2.137, r2: -1.842, rate:0.500, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:35 step:3700 train loss: 0.900, r2: 0.487, rate:0.750, acc:0.625 val loss: 1.617, r2: 0.165, rate:0.750, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:35 step:3800 train loss: 1.251, r2: -0.130, rate:0.500, acc:0.438 val loss: 0.858, r2: -0.354, rate:0.625, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:35 step:3900 train loss: 3.134, r2: -12.056, rate:0.500, acc:0.625 val loss: 1.061, r2: 0.112, rate:0.625, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:35 step:4000 train loss: 1.196, r2: 0.473, rate:0.812, acc:0.750 val loss: 0.866, r2: -0.604, rate:0.562, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:35 step:4100 train loss: 1.423, r2: 0.524, rate:0.688, acc:0.625 val loss: 1.145, r2: 0.090, rate:0.875, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:35 step:4200 train loss: 1.126, r2: -0.019, rate:0.688, acc:0.750 val loss: 0.861, r2: 0.600, rate:0.812, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:35 step:4300 train loss: 0.894, r2: 0.270, rate:0.938, acc:1.000 val loss: 0.534, r2: 0.593, rate:0.812, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:35 step:4400 train loss: 1.276, r2: -0.890, rate:0.438, acc:0.562 val loss: 3.718, r2: -2.045, rate:0.688, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:35 step:4500 train loss: 0.669, r2: 0.143, rate:0.812, acc:0.688 val loss: 1.146, r2: 0.344, rate:0.688, acc:0.750 lr 9.534187405839861e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:35 step:4600 train loss: 0.673, r2: 0.199, rate:0.812, acc:0.625 val loss: 1.242, r2: 0.063, rate:0.500, acc:0.375 lr 9.534187405839861e-08\n",
      "epoch:35 step:4700 train loss: 1.190, r2: -1.598, rate:0.812, acc:0.938 val loss: 0.877, r2: 0.395, rate:0.812, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:35 step:4800 train loss: 0.778, r2: -0.644, rate:0.750, acc:0.875 val loss: 2.238, r2: -1.141, rate:0.812, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:35 step:4900 train loss: 2.250, r2: -1.199, rate:0.562, acc:0.562 val loss: 0.816, r2: -0.688, rate:0.625, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:35 step:5000 train loss: 5.365, r2: -3.179, rate:0.875, acc:1.000 val loss: 2.462, r2: 0.329, rate:0.938, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:35 step:5100 train loss: 1.191, r2: 0.377, rate:0.562, acc:0.688 val loss: 1.254, r2: -1.159, rate:0.875, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:35 step:5200 train loss: 0.808, r2: 0.040, rate:0.625, acc:0.562 val loss: 1.240, r2: -1.475, rate:0.312, acc:0.250 lr 9.534187405839861e-08\n",
      "epoch:35 step:5300 train loss: 3.456, r2: 0.033, rate:0.562, acc:0.562 val loss: 1.212, r2: -1.445, rate:0.500, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:35 step:5400 train loss: 0.984, r2: -0.300, rate:0.750, acc:0.750 val loss: 0.564, r2: -0.298, rate:0.938, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:35 step:5500 train loss: 0.715, r2: 0.609, rate:0.688, acc:0.938 val loss: 1.203, r2: 0.237, rate:0.562, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:35 step:5600 train loss: 1.740, r2: -2.218, rate:0.688, acc:0.812 val loss: 0.753, r2: 0.654, rate:0.812, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:35 step:5700 train loss: 0.654, r2: -0.908, rate:0.750, acc:0.688 val loss: 0.718, r2: 0.546, rate:0.875, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:35 step:5800 train loss: 3.207, r2: -0.990, rate:0.875, acc:0.750 val loss: 0.807, r2: 0.419, rate:0.625, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:35 step:5900 train loss: 2.708, r2: -1.012, rate:0.938, acc:1.000 val loss: 1.246, r2: 0.222, rate:0.625, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:35 step:6000 train loss: 1.070, r2: 0.471, rate:0.875, acc:0.500 val loss: 1.019, r2: 0.429, rate:0.875, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:35 step:6100 train loss: 0.757, r2: -1.601, rate:0.562, acc:0.562 val loss: 1.575, r2: 0.015, rate:0.500, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:35 step:6200 train loss: 2.692, r2: -4.719, rate:0.562, acc:0.312 val loss: 1.129, r2: -2.378, rate:0.688, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:35 step:6300 train loss: 0.504, r2: -0.567, rate:0.750, acc:0.875 val loss: 1.120, r2: 0.317, rate:0.875, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:35 step:6400 train loss: 1.140, r2: 0.124, rate:0.688, acc:0.688 val loss: 0.724, r2: -0.085, rate:0.688, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:35 step:6500 train loss: 0.995, r2: 0.629, rate:0.750, acc:0.625 val loss: 1.123, r2: 0.566, rate:0.812, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:35 step:6600 train loss: 0.879, r2: -0.408, rate:0.500, acc:0.500 val loss: 0.859, r2: 0.126, rate:0.562, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:35 step:6700 train loss: 1.123, r2: 0.582, rate:0.500, acc:0.688 val loss: 0.665, r2: -0.363, rate:0.562, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:35 step:6800 train loss: 2.272, r2: -0.456, rate:0.562, acc:0.500 val loss: 0.789, r2: 0.270, rate:0.750, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:35 step:6900 train loss: 0.943, r2: -0.002, rate:0.812, acc:0.625 val loss: 0.773, r2: -0.104, rate:0.750, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:35 step:7000 train loss: 0.737, r2: 0.227, rate:0.438, acc:0.375 val loss: 0.931, r2: 0.023, rate:0.625, acc:0.375 lr 9.534187405839861e-08\n",
      "epoch:35 step:7100 train loss: 1.255, r2: -0.995, rate:0.562, acc:0.312 val loss: 1.393, r2: -0.318, rate:0.375, acc:0.375 lr 9.534187405839861e-08\n",
      "epoch:35 step:7200 train loss: 0.632, r2: -0.419, rate:0.750, acc:0.562 val loss: 0.763, r2: -0.445, rate:0.812, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:35 step:7300 train loss: 0.639, r2: -0.204, rate:0.812, acc:0.500 val loss: 1.333, r2: 0.149, rate:0.625, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:35 step:7400 train loss: 0.988, r2: -0.335, rate:0.562, acc:0.438 val loss: 0.730, r2: 0.187, rate:0.625, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:35 step:7500 train loss: 1.441, r2: 0.297, rate:0.938, acc:1.000 val loss: 1.085, r2: -0.868, rate:0.688, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:35 step:7600 train loss: 0.943, r2: 0.322, rate:0.688, acc:0.625 val loss: 0.965, r2: -0.191, rate:0.562, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:35 step:7700 train loss: 1.752, r2: 0.466, rate:0.875, acc:0.812 val loss: 2.057, r2: -0.247, rate:0.625, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:35 step:7800 train loss: 0.704, r2: 0.295, rate:0.688, acc:0.750 val loss: 1.096, r2: -0.048, rate:0.750, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:35 step:0.328 lr 0.000000\n",
      "epoch:36 step:0 train loss: 0.642, r2: 0.355, rate:0.812, acc:0.750 val loss: 1.083, r2: -0.603, rate:0.562, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:36 step:100 train loss: 0.697, r2: -0.339, rate:0.812, acc:0.938 val loss: 1.576, r2: 0.002, rate:0.500, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:36 step:200 train loss: 1.443, r2: -0.569, rate:0.562, acc:0.688 val loss: 1.020, r2: -0.354, rate:0.688, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:36 step:300 train loss: 0.789, r2: -0.006, rate:0.750, acc:0.562 val loss: 0.704, r2: 0.586, rate:1.000, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:36 step:400 train loss: 1.131, r2: 0.129, rate:0.438, acc:0.500 val loss: 0.944, r2: 0.361, rate:0.875, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:36 step:500 train loss: 1.232, r2: 0.028, rate:0.938, acc:0.625 val loss: 1.025, r2: 0.475, rate:0.750, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:36 step:600 train loss: 0.789, r2: 0.187, rate:0.438, acc:0.750 val loss: 1.490, r2: -0.683, rate:0.688, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:36 step:700 train loss: 0.721, r2: -0.833, rate:0.625, acc:0.625 val loss: 0.527, r2: 0.629, rate:0.812, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:36 step:800 train loss: 1.151, r2: -0.855, rate:0.500, acc:0.438 val loss: 0.689, r2: -1.284, rate:0.562, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:36 step:900 train loss: 0.706, r2: -1.064, rate:0.562, acc:0.812 val loss: 0.971, r2: -0.490, rate:0.562, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:36 step:1000 train loss: 0.675, r2: 0.205, rate:0.688, acc:0.562 val loss: 1.624, r2: -0.007, rate:0.562, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:36 step:1100 train loss: 4.293, r2: -8.443, rate:0.312, acc:0.812 val loss: 0.641, r2: 0.429, rate:0.688, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:36 step:1200 train loss: 0.710, r2: 0.030, rate:0.625, acc:0.875 val loss: 0.804, r2: 0.108, rate:0.750, acc:0.312 lr 9.534187405839861e-08\n",
      "epoch:36 step:1300 train loss: 1.561, r2: 0.238, rate:0.750, acc:0.938 val loss: 0.750, r2: 0.310, rate:0.875, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:36 step:1400 train loss: 1.200, r2: 0.373, rate:0.500, acc:0.688 val loss: 0.601, r2: 0.611, rate:1.000, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:36 step:1500 train loss: 0.653, r2: -0.346, rate:0.500, acc:0.812 val loss: 2.201, r2: -0.348, rate:0.750, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:36 step:1600 train loss: 0.772, r2: -0.666, rate:0.562, acc:0.625 val loss: 0.838, r2: -2.521, rate:0.625, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:36 step:1700 train loss: 0.976, r2: -3.861, rate:0.312, acc:0.438 val loss: 0.818, r2: -1.825, rate:0.562, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:36 step:1800 train loss: 0.689, r2: -0.078, rate:0.688, acc:0.562 val loss: 0.707, r2: 0.003, rate:0.688, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:36 step:1900 train loss: 0.974, r2: -1.628, rate:0.812, acc:0.688 val loss: 0.622, r2: 0.552, rate:0.875, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:36 step:2000 train loss: 1.181, r2: -0.743, rate:0.375, acc:0.500 val loss: 0.835, r2: 0.397, rate:0.750, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:36 step:2100 train loss: 1.184, r2: 0.474, rate:0.688, acc:0.500 val loss: 0.749, r2: 0.229, rate:0.688, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:36 step:2200 train loss: 0.805, r2: 0.656, rate:0.812, acc:0.750 val loss: 0.770, r2: -1.638, rate:0.562, acc:0.500 lr 9.534187405839861e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:36 step:2300 train loss: 0.682, r2: -0.124, rate:0.750, acc:1.000 val loss: 1.808, r2: -0.589, rate:0.625, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:36 step:2400 train loss: 0.668, r2: -0.824, rate:0.625, acc:0.812 val loss: 1.201, r2: -0.441, rate:0.500, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:36 step:2500 train loss: 1.053, r2: -2.924, rate:0.312, acc:0.125 val loss: 0.628, r2: 0.487, rate:0.750, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:36 step:2600 train loss: 0.683, r2: 0.038, rate:0.562, acc:0.812 val loss: 0.997, r2: -0.569, rate:0.562, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:36 step:2700 train loss: 1.660, r2: -0.337, rate:0.562, acc:0.500 val loss: 1.650, r2: 0.025, rate:0.875, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:36 step:2800 train loss: 1.395, r2: -1.274, rate:0.375, acc:0.438 val loss: 1.239, r2: 0.406, rate:0.562, acc:0.375 lr 9.534187405839861e-08\n",
      "epoch:36 step:2900 train loss: 1.692, r2: 0.183, rate:0.688, acc:0.438 val loss: 0.919, r2: 0.647, rate:0.812, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:36 step:3000 train loss: 0.828, r2: 0.237, rate:0.688, acc:0.812 val loss: 1.127, r2: 0.354, rate:0.750, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:36 step:3100 train loss: 2.579, r2: -5.833, rate:0.312, acc:0.875 val loss: 0.828, r2: -0.069, rate:0.750, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:36 step:3200 train loss: 0.639, r2: -0.164, rate:0.812, acc:0.875 val loss: 0.720, r2: -0.445, rate:0.625, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:36 step:3300 train loss: 1.011, r2: 0.484, rate:0.688, acc:0.625 val loss: 1.093, r2: -0.439, rate:0.562, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:36 step:3400 train loss: 0.937, r2: -0.807, rate:0.812, acc:0.875 val loss: 1.279, r2: 0.470, rate:0.625, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:36 step:3500 train loss: 0.496, r2: 0.795, rate:0.875, acc:0.875 val loss: 0.814, r2: 0.506, rate:0.812, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:36 step:3600 train loss: 0.779, r2: 0.474, rate:0.688, acc:0.688 val loss: 1.131, r2: 0.093, rate:0.438, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:36 step:3700 train loss: 0.706, r2: -0.588, rate:0.875, acc:0.875 val loss: 0.800, r2: 0.476, rate:0.625, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:36 step:3800 train loss: 0.971, r2: 0.599, rate:0.688, acc:0.562 val loss: 0.949, r2: 0.164, rate:0.812, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:36 step:3900 train loss: 0.770, r2: -0.171, rate:0.750, acc:0.750 val loss: 1.548, r2: 0.234, rate:0.688, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:36 step:4000 train loss: 2.364, r2: -0.067, rate:0.500, acc:0.500 val loss: 2.042, r2: -0.092, rate:0.625, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:36 step:4100 train loss: 1.807, r2: -3.902, rate:0.812, acc:0.938 val loss: 0.468, r2: 0.192, rate:0.750, acc:1.000 lr 9.534187405839861e-08\n",
      "epoch:36 step:4200 train loss: 0.836, r2: -0.845, rate:0.438, acc:0.625 val loss: 1.485, r2: -0.826, rate:0.438, acc:0.188 lr 9.534187405839861e-08\n",
      "epoch:36 step:4300 train loss: 0.815, r2: -0.429, rate:0.688, acc:1.000 val loss: 1.398, r2: -0.258, rate:0.438, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:36 step:4400 train loss: 0.834, r2: -0.728, rate:0.625, acc:0.812 val loss: 1.214, r2: -0.215, rate:0.750, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:36 step:4500 train loss: 0.847, r2: -1.131, rate:0.625, acc:0.875 val loss: 0.903, r2: -0.334, rate:0.500, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:36 step:4600 train loss: 0.727, r2: -1.199, rate:0.750, acc:0.438 val loss: 0.863, r2: -1.753, rate:0.438, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:36 step:4700 train loss: 1.131, r2: 0.314, rate:0.750, acc:0.562 val loss: 2.456, r2: -0.982, rate:0.375, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:36 step:4800 train loss: 1.304, r2: -2.165, rate:0.688, acc:0.625 val loss: 0.616, r2: 0.593, rate:0.750, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:36 step:4900 train loss: 1.312, r2: -0.423, rate:0.438, acc:0.750 val loss: 1.252, r2: -2.277, rate:0.562, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:36 step:5000 train loss: 1.393, r2: 0.076, rate:0.750, acc:0.688 val loss: 0.576, r2: 0.411, rate:0.875, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:36 step:5100 train loss: 1.039, r2: -1.455, rate:0.438, acc:0.188 val loss: 0.851, r2: 0.498, rate:0.688, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:36 step:5200 train loss: 1.056, r2: -2.040, rate:0.500, acc:0.688 val loss: 0.816, r2: 0.333, rate:0.562, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:36 step:5300 train loss: 1.737, r2: -0.405, rate:0.375, acc:0.312 val loss: 0.685, r2: -0.691, rate:0.562, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:36 step:5400 train loss: 0.611, r2: 0.606, rate:0.875, acc:0.625 val loss: 0.949, r2: -0.656, rate:0.562, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:36 step:5500 train loss: 0.691, r2: 0.659, rate:0.750, acc:0.562 val loss: 0.737, r2: -0.727, rate:0.750, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:36 step:5600 train loss: 4.120, r2: -0.110, rate:0.500, acc:0.375 val loss: 0.875, r2: -0.193, rate:0.500, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:36 step:5700 train loss: 4.942, r2: -0.214, rate:0.938, acc:1.000 val loss: 1.056, r2: -0.249, rate:0.688, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:36 step:5800 train loss: 0.863, r2: -0.350, rate:0.562, acc:0.562 val loss: 0.583, r2: -0.583, rate:0.875, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:36 step:5900 train loss: 0.902, r2: -0.557, rate:0.375, acc:0.500 val loss: 0.992, r2: 0.410, rate:0.812, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:36 step:6000 train loss: 0.747, r2: -0.172, rate:0.562, acc:0.375 val loss: 0.753, r2: 0.648, rate:0.750, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:36 step:6100 train loss: 0.672, r2: -0.485, rate:0.812, acc:0.562 val loss: 0.683, r2: -0.167, rate:0.750, acc:1.000 lr 9.534187405839861e-08\n",
      "epoch:36 step:6200 train loss: 5.739, r2: -4.398, rate:0.188, acc:0.125 val loss: 1.630, r2: 0.294, rate:0.812, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:36 step:6300 train loss: 1.208, r2: -1.714, rate:0.375, acc:0.812 val loss: 0.732, r2: 0.488, rate:0.688, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:36 step:6400 train loss: 0.741, r2: -29.884, rate:0.500, acc:0.625 val loss: 0.705, r2: 0.657, rate:0.812, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:36 step:6500 train loss: 1.003, r2: -0.479, rate:0.688, acc:0.688 val loss: 1.305, r2: 0.230, rate:0.750, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:36 step:6600 train loss: 0.642, r2: -1.255, rate:0.625, acc:0.750 val loss: 1.129, r2: -0.445, rate:0.812, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:36 step:6700 train loss: 0.961, r2: -0.446, rate:0.500, acc:0.688 val loss: 0.900, r2: -3.269, rate:0.688, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:36 step:6800 train loss: 1.036, r2: -1.677, rate:0.438, acc:0.375 val loss: 0.700, r2: 0.197, rate:0.875, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:36 step:6900 train loss: 0.822, r2: -3.018, rate:0.500, acc:0.688 val loss: 1.726, r2: 0.441, rate:0.812, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:36 step:7000 train loss: 0.627, r2: 0.658, rate:0.812, acc:0.375 val loss: 0.879, r2: 0.431, rate:0.750, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:36 step:7100 train loss: 1.243, r2: 0.318, rate:0.875, acc:0.625 val loss: 0.917, r2: -0.544, rate:0.562, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:36 step:7200 train loss: 1.447, r2: -0.378, rate:0.875, acc:0.938 val loss: 0.786, r2: 0.510, rate:0.812, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:36 step:7300 train loss: 0.877, r2: -17.683, rate:0.312, acc:0.625 val loss: 1.391, r2: -0.898, rate:0.688, acc:0.312 lr 9.534187405839861e-08\n",
      "epoch:36 step:7400 train loss: 1.644, r2: 0.410, rate:0.562, acc:0.438 val loss: 0.634, r2: -0.831, rate:0.625, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:36 step:7500 train loss: 1.053, r2: -0.324, rate:0.688, acc:0.625 val loss: 0.597, r2: 0.530, rate:0.812, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:36 step:7600 train loss: 1.481, r2: 0.479, rate:0.938, acc:0.938 val loss: 0.837, r2: -0.753, rate:0.750, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:36 step:7700 train loss: 1.039, r2: -1.323, rate:0.688, acc:0.750 val loss: 0.780, r2: -0.808, rate:0.812, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:36 step:7800 train loss: 0.655, r2: 0.524, rate:0.688, acc:0.875 val loss: 0.875, r2: 0.535, rate:0.875, acc:0.875 lr 9.534187405839861e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:36 step:0.328 lr 0.000000\n",
      "epoch:37 step:0 train loss: 1.221, r2: -1.610, rate:0.312, acc:0.250 val loss: 1.175, r2: -0.367, rate:0.625, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:37 step:100 train loss: 0.686, r2: 0.473, rate:0.875, acc:0.875 val loss: 1.731, r2: -1.936, rate:0.312, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:37 step:200 train loss: 0.966, r2: 0.088, rate:0.625, acc:0.562 val loss: 0.991, r2: 0.566, rate:0.812, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:37 step:300 train loss: 0.949, r2: -1.708, rate:0.500, acc:0.625 val loss: 0.712, r2: 0.443, rate:0.812, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:37 step:400 train loss: 1.748, r2: 0.502, rate:0.625, acc:0.750 val loss: 0.837, r2: -0.378, rate:0.438, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:37 step:500 train loss: 0.791, r2: -5.445, rate:0.438, acc:0.938 val loss: 0.610, r2: 0.101, rate:0.750, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:37 step:600 train loss: 0.723, r2: 0.219, rate:0.688, acc:0.812 val loss: 2.342, r2: 0.054, rate:0.750, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:37 step:700 train loss: 0.674, r2: 0.034, rate:0.625, acc:0.438 val loss: 0.878, r2: 0.554, rate:0.750, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:37 step:800 train loss: 1.750, r2: -4.877, rate:0.812, acc:0.562 val loss: 1.334, r2: 0.527, rate:0.875, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:37 step:900 train loss: 0.607, r2: -2.087, rate:0.750, acc:0.812 val loss: 1.208, r2: 0.445, rate:0.812, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:37 step:1000 train loss: 1.117, r2: -1.760, rate:0.875, acc:1.000 val loss: 1.049, r2: 0.321, rate:0.688, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:37 step:1100 train loss: 0.478, r2: 0.784, rate:1.000, acc:0.812 val loss: 0.884, r2: 0.354, rate:0.688, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:37 step:1200 train loss: 0.871, r2: 0.196, rate:0.438, acc:0.625 val loss: 0.738, r2: -0.258, rate:0.688, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:37 step:1300 train loss: 1.668, r2: -3.290, rate:0.438, acc:0.625 val loss: 0.875, r2: 0.065, rate:0.750, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:37 step:1400 train loss: 0.727, r2: 0.550, rate:0.812, acc:0.875 val loss: 0.674, r2: 0.420, rate:0.812, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:37 step:1500 train loss: 0.896, r2: -3.206, rate:0.562, acc:0.375 val loss: 1.294, r2: 0.096, rate:0.750, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:37 step:1600 train loss: 0.873, r2: -0.309, rate:0.438, acc:0.438 val loss: 0.657, r2: 0.490, rate:0.750, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:37 step:1700 train loss: 0.716, r2: -0.348, rate:0.812, acc:0.500 val loss: 0.692, r2: 0.650, rate:0.688, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:37 step:1800 train loss: 0.693, r2: -0.407, rate:0.625, acc:0.812 val loss: 1.840, r2: 0.028, rate:0.688, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:37 step:1900 train loss: 0.858, r2: -2.064, rate:0.812, acc:0.750 val loss: 0.711, r2: -3.018, rate:0.625, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:37 step:2000 train loss: 0.957, r2: -0.029, rate:0.688, acc:0.562 val loss: 0.802, r2: 0.466, rate:1.000, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:37 step:2100 train loss: 0.719, r2: 0.229, rate:0.750, acc:0.812 val loss: 1.103, r2: 0.637, rate:0.875, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:37 step:2200 train loss: 2.392, r2: -3.934, rate:0.688, acc:0.312 val loss: 0.798, r2: 0.338, rate:0.562, acc:0.250 lr 9.534187405839861e-08\n",
      "epoch:37 step:2300 train loss: 0.776, r2: 0.481, rate:0.875, acc:0.562 val loss: 0.886, r2: -0.853, rate:0.438, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:37 step:2400 train loss: 0.449, r2: 0.599, rate:0.938, acc:0.875 val loss: 1.002, r2: 0.569, rate:1.000, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:37 step:2500 train loss: 1.683, r2: -12.985, rate:1.000, acc:0.875 val loss: 0.729, r2: 0.603, rate:0.812, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:37 step:2600 train loss: 0.796, r2: -0.372, rate:0.750, acc:0.750 val loss: 1.359, r2: -0.207, rate:0.750, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:37 step:2700 train loss: 0.765, r2: -1.957, rate:0.688, acc:0.875 val loss: 0.704, r2: -0.170, rate:0.688, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:37 step:2800 train loss: 0.899, r2: -2.146, rate:0.625, acc:0.500 val loss: 0.836, r2: 0.030, rate:0.688, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:37 step:2900 train loss: 1.155, r2: -0.222, rate:0.812, acc:0.625 val loss: 1.241, r2: 0.517, rate:0.750, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:37 step:3000 train loss: 0.624, r2: -0.426, rate:0.750, acc:0.688 val loss: 1.403, r2: -0.015, rate:0.625, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:37 step:3100 train loss: 2.523, r2: -4.309, rate:0.875, acc:0.750 val loss: 0.728, r2: 0.218, rate:0.500, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:37 step:3200 train loss: 2.195, r2: -0.815, rate:0.750, acc:0.938 val loss: 0.806, r2: -1.087, rate:0.562, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:37 step:3300 train loss: 0.905, r2: -0.519, rate:0.812, acc:1.000 val loss: 1.022, r2: 0.199, rate:0.812, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:37 step:3400 train loss: 0.706, r2: -0.431, rate:0.875, acc:0.938 val loss: 0.618, r2: -1.557, rate:0.688, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:37 step:3500 train loss: 1.176, r2: -0.249, rate:0.625, acc:0.625 val loss: 1.000, r2: 0.028, rate:0.625, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:37 step:3600 train loss: 0.688, r2: -0.872, rate:0.812, acc:0.750 val loss: 0.929, r2: -0.174, rate:0.375, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:37 step:3700 train loss: 0.795, r2: -0.466, rate:0.500, acc:0.438 val loss: 0.581, r2: 0.733, rate:0.938, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:37 step:3800 train loss: 2.565, r2: -1.055, rate:0.688, acc:0.438 val loss: 0.856, r2: -1.377, rate:0.625, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:37 step:3900 train loss: 0.653, r2: -0.307, rate:0.750, acc:0.562 val loss: 0.998, r2: 0.146, rate:0.750, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:37 step:4000 train loss: 1.181, r2: 0.171, rate:0.500, acc:0.625 val loss: 1.029, r2: -0.162, rate:0.688, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:37 step:4100 train loss: 1.346, r2: -0.782, rate:0.438, acc:0.562 val loss: 1.272, r2: -0.738, rate:0.562, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:37 step:4200 train loss: 0.825, r2: -1.155, rate:0.375, acc:0.562 val loss: 0.606, r2: -0.504, rate:0.812, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:37 step:4300 train loss: 1.707, r2: -0.354, rate:0.562, acc:0.562 val loss: 3.213, r2: -0.138, rate:0.562, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:37 step:4400 train loss: 0.578, r2: 0.746, rate:0.812, acc:0.875 val loss: 1.463, r2: 0.380, rate:0.875, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:37 step:4500 train loss: 0.683, r2: 0.164, rate:0.812, acc:0.375 val loss: 0.805, r2: 0.547, rate:0.688, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:37 step:4600 train loss: 1.511, r2: -4.637, rate:0.250, acc:0.375 val loss: 0.611, r2: -0.592, rate:0.750, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:37 step:4700 train loss: 0.607, r2: 0.814, rate:0.812, acc:0.500 val loss: 0.450, r2: -0.224, rate:0.875, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:37 step:4800 train loss: 0.576, r2: -0.102, rate:0.875, acc:0.688 val loss: 0.737, r2: -0.626, rate:0.500, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:37 step:4900 train loss: 0.629, r2: 0.322, rate:0.875, acc:0.562 val loss: 0.631, r2: -1.835, rate:0.750, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:37 step:5000 train loss: 0.888, r2: 0.481, rate:0.750, acc:0.625 val loss: 0.638, r2: 0.533, rate:0.688, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:37 step:5100 train loss: 0.884, r2: -2.176, rate:0.500, acc:0.562 val loss: 2.168, r2: -0.744, rate:0.750, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:37 step:5200 train loss: 1.288, r2: -1.215, rate:0.312, acc:0.750 val loss: 1.137, r2: -0.537, rate:0.562, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:37 step:5300 train loss: 1.970, r2: -0.964, rate:0.625, acc:0.688 val loss: 1.463, r2: 0.424, rate:0.812, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:37 step:5400 train loss: 0.826, r2: -4.427, rate:0.562, acc:0.125 val loss: 0.561, r2: 0.723, rate:0.875, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:37 step:5500 train loss: 0.606, r2: 0.220, rate:0.688, acc:0.875 val loss: 0.694, r2: 0.478, rate:0.812, acc:0.812 lr 9.534187405839861e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:37 step:5600 train loss: 0.723, r2: -0.646, rate:0.750, acc:1.000 val loss: 0.832, r2: -0.004, rate:0.750, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:37 step:5700 train loss: 0.922, r2: 0.383, rate:1.000, acc:0.688 val loss: 0.862, r2: -0.009, rate:0.438, acc:0.312 lr 9.534187405839861e-08\n",
      "epoch:37 step:5800 train loss: 1.607, r2: -0.205, rate:0.875, acc:0.875 val loss: 1.042, r2: -0.819, rate:0.625, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:37 step:5900 train loss: 0.505, r2: 0.562, rate:1.000, acc:0.812 val loss: 0.927, r2: 0.182, rate:0.625, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:37 step:6000 train loss: 0.792, r2: 0.345, rate:0.812, acc:0.438 val loss: 0.856, r2: -2.875, rate:0.688, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:37 step:6100 train loss: 0.835, r2: 0.567, rate:0.688, acc:0.562 val loss: 1.225, r2: 0.022, rate:0.688, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:37 step:6200 train loss: 1.686, r2: -0.634, rate:1.000, acc:0.812 val loss: 1.174, r2: -1.631, rate:0.312, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:37 step:6300 train loss: 0.757, r2: 0.181, rate:0.438, acc:0.625 val loss: 0.700, r2: 0.336, rate:0.812, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:37 step:6400 train loss: 0.937, r2: 0.264, rate:0.500, acc:0.688 val loss: 1.135, r2: -0.460, rate:0.625, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:37 step:6500 train loss: 1.417, r2: 0.202, rate:0.625, acc:0.625 val loss: 0.724, r2: -0.014, rate:0.625, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:37 step:6600 train loss: 0.802, r2: -1.782, rate:0.625, acc:0.250 val loss: 1.290, r2: 0.062, rate:0.625, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:37 step:6700 train loss: 0.856, r2: 0.488, rate:0.688, acc:0.688 val loss: 0.827, r2: -0.448, rate:0.625, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:37 step:6800 train loss: 0.799, r2: 0.409, rate:0.625, acc:0.688 val loss: 1.325, r2: 0.010, rate:0.875, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:37 step:6900 train loss: 0.903, r2: 0.227, rate:0.375, acc:0.562 val loss: 1.463, r2: -0.062, rate:0.625, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:37 step:7000 train loss: 0.826, r2: -1.265, rate:0.562, acc:0.625 val loss: 0.848, r2: 0.433, rate:0.500, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:37 step:7100 train loss: 1.447, r2: 0.419, rate:0.812, acc:0.812 val loss: 1.303, r2: -0.596, rate:0.688, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:37 step:7200 train loss: 0.639, r2: 0.303, rate:0.812, acc:0.938 val loss: 1.136, r2: -4.403, rate:0.375, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:37 step:7300 train loss: 0.916, r2: -0.989, rate:0.625, acc:0.938 val loss: 2.475, r2: -1.814, rate:0.375, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:37 step:7400 train loss: 3.895, r2: -0.750, rate:0.312, acc:0.188 val loss: 0.558, r2: 0.649, rate:0.875, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:37 step:7500 train loss: 0.634, r2: 0.170, rate:0.875, acc:0.625 val loss: 1.044, r2: 0.128, rate:0.875, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:37 step:7600 train loss: 1.136, r2: -0.401, rate:0.312, acc:0.750 val loss: 0.861, r2: -3.145, rate:0.562, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:37 step:7700 train loss: 0.794, r2: 0.441, rate:0.750, acc:0.438 val loss: 2.024, r2: -0.911, rate:0.625, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:37 step:7800 train loss: 0.814, r2: -1.444, rate:0.438, acc:0.812 val loss: 1.240, r2: -0.554, rate:0.438, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:37 step:0.328 lr 0.000000\n",
      "epoch:38 step:0 train loss: 1.095, r2: 0.426, rate:0.625, acc:0.688 val loss: 1.186, r2: -0.444, rate:0.625, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:38 step:100 train loss: 0.973, r2: 0.555, rate:0.688, acc:0.688 val loss: 1.187, r2: 0.289, rate:0.625, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:38 step:200 train loss: 3.148, r2: 0.006, rate:0.812, acc:0.625 val loss: 0.995, r2: 0.275, rate:0.875, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:38 step:300 train loss: 1.148, r2: 0.280, rate:0.625, acc:0.688 val loss: 0.797, r2: -0.312, rate:0.688, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:38 step:400 train loss: 0.818, r2: -0.691, rate:0.625, acc:0.625 val loss: 0.922, r2: 0.088, rate:0.812, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:38 step:500 train loss: 1.330, r2: 0.277, rate:0.750, acc:0.625 val loss: 1.112, r2: -1.602, rate:0.500, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:38 step:600 train loss: 0.865, r2: -2.689, rate:0.500, acc:0.688 val loss: 1.127, r2: 0.532, rate:0.750, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:38 step:700 train loss: 0.942, r2: -1.294, rate:0.562, acc:0.312 val loss: 1.213, r2: 0.035, rate:0.688, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:38 step:800 train loss: 0.740, r2: 0.809, rate:0.562, acc:0.500 val loss: 0.801, r2: 0.191, rate:0.750, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:38 step:900 train loss: 0.758, r2: 0.542, rate:0.500, acc:0.750 val loss: 1.244, r2: -1.740, rate:0.812, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:38 step:1000 train loss: 0.952, r2: 0.070, rate:0.562, acc:0.438 val loss: 1.554, r2: -1.753, rate:0.750, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:38 step:1100 train loss: 0.699, r2: -3.989, rate:0.562, acc:0.875 val loss: 0.914, r2: 0.140, rate:0.500, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:38 step:1200 train loss: 1.007, r2: -0.111, rate:0.438, acc:0.562 val loss: 0.860, r2: -4.529, rate:0.812, acc:1.000 lr 9.534187405839861e-08\n",
      "epoch:38 step:1300 train loss: 0.901, r2: 0.275, rate:0.625, acc:0.688 val loss: 0.704, r2: 0.662, rate:0.938, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:38 step:1400 train loss: 0.794, r2: -0.966, rate:0.812, acc:0.875 val loss: 1.041, r2: -0.347, rate:0.625, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:38 step:1500 train loss: 0.660, r2: 0.436, rate:0.625, acc:0.750 val loss: 2.372, r2: -0.559, rate:0.562, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:38 step:1600 train loss: 0.662, r2: 0.386, rate:0.688, acc:0.750 val loss: 0.635, r2: 0.456, rate:0.688, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:38 step:1700 train loss: 0.795, r2: -6.161, rate:0.500, acc:0.500 val loss: 1.084, r2: 0.490, rate:0.875, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:38 step:1800 train loss: 1.104, r2: 0.415, rate:0.562, acc:0.562 val loss: 2.635, r2: -0.859, rate:0.750, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:38 step:1900 train loss: 0.591, r2: 0.209, rate:0.688, acc:0.938 val loss: 1.197, r2: -2.287, rate:0.750, acc:1.000 lr 9.534187405839861e-08\n",
      "epoch:38 step:2000 train loss: 0.959, r2: -1.764, rate:0.812, acc:0.938 val loss: 1.237, r2: 0.144, rate:0.500, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:38 step:2100 train loss: 0.703, r2: -2.974, rate:0.750, acc:0.750 val loss: 0.878, r2: 0.306, rate:0.562, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:38 step:2200 train loss: 10.666, r2: -9.287, rate:0.688, acc:0.875 val loss: 1.006, r2: -1.065, rate:0.625, acc:0.250 lr 9.534187405839861e-08\n",
      "epoch:38 step:2300 train loss: 1.674, r2: -3.252, rate:0.812, acc:0.688 val loss: 0.500, r2: 0.842, rate:0.875, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:38 step:2400 train loss: 0.771, r2: -0.137, rate:0.625, acc:0.938 val loss: 0.847, r2: 0.097, rate:0.750, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:38 step:2500 train loss: 0.798, r2: 0.149, rate:0.438, acc:0.688 val loss: 0.905, r2: -1.459, rate:0.625, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:38 step:2600 train loss: 1.170, r2: -1.774, rate:0.750, acc:0.250 val loss: 1.375, r2: 0.028, rate:0.812, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:38 step:2700 train loss: 0.458, r2: -0.345, rate:1.000, acc:0.938 val loss: 0.928, r2: -0.229, rate:0.500, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:38 step:2800 train loss: 1.340, r2: -2.722, rate:0.875, acc:1.000 val loss: 0.775, r2: 0.589, rate:0.750, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:38 step:2900 train loss: 6.440, r2: -0.419, rate:0.750, acc:0.875 val loss: 1.415, r2: 0.377, rate:0.812, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:38 step:3000 train loss: 0.814, r2: 0.194, rate:0.688, acc:1.000 val loss: 0.733, r2: -0.124, rate:0.688, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:38 step:3100 train loss: 0.564, r2: 0.730, rate:0.750, acc:0.688 val loss: 0.586, r2: 0.751, rate:0.938, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:38 step:3200 train loss: 0.634, r2: 0.669, rate:0.875, acc:0.750 val loss: 0.807, r2: 0.029, rate:0.625, acc:0.812 lr 9.534187405839861e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:38 step:3300 train loss: 0.666, r2: -0.841, rate:0.500, acc:0.938 val loss: 0.877, r2: -1.666, rate:0.375, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:38 step:3400 train loss: 0.833, r2: -0.544, rate:0.688, acc:0.188 val loss: 0.648, r2: 0.163, rate:0.875, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:38 step:3500 train loss: 0.882, r2: 0.194, rate:1.000, acc:0.625 val loss: 0.740, r2: 0.070, rate:0.688, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:38 step:3600 train loss: 0.707, r2: -2.363, rate:0.562, acc:0.625 val loss: 0.837, r2: -0.811, rate:0.625, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:38 step:3700 train loss: 3.049, r2: -1.087, rate:0.750, acc:0.750 val loss: 1.337, r2: -0.920, rate:0.500, acc:0.312 lr 9.534187405839861e-08\n",
      "epoch:38 step:3800 train loss: 1.412, r2: -1.416, rate:0.875, acc:0.562 val loss: 1.017, r2: -0.552, rate:0.375, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:38 step:3900 train loss: 0.740, r2: 0.481, rate:0.812, acc:0.812 val loss: 0.657, r2: -1.146, rate:0.750, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:38 step:4000 train loss: 2.152, r2: 0.167, rate:0.812, acc:0.688 val loss: 0.661, r2: 0.681, rate:0.750, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:38 step:4100 train loss: 2.450, r2: -1.736, rate:0.438, acc:0.625 val loss: 0.627, r2: 0.351, rate:0.812, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:38 step:4200 train loss: 0.955, r2: -0.388, rate:0.688, acc:0.312 val loss: 0.955, r2: -1.943, rate:0.625, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:38 step:4300 train loss: 0.622, r2: 0.788, rate:0.812, acc:0.875 val loss: 2.400, r2: -5.580, rate:0.312, acc:0.062 lr 9.534187405839861e-08\n",
      "epoch:38 step:4400 train loss: 1.139, r2: 0.588, rate:0.688, acc:0.750 val loss: 0.681, r2: -0.634, rate:0.875, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:38 step:4500 train loss: 1.265, r2: -1.360, rate:0.500, acc:0.125 val loss: 0.711, r2: -0.373, rate:0.812, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:38 step:4600 train loss: 0.826, r2: -0.242, rate:0.625, acc:0.625 val loss: 0.652, r2: 0.588, rate:0.875, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:38 step:4700 train loss: 1.404, r2: 0.164, rate:0.750, acc:0.625 val loss: 0.823, r2: 0.482, rate:0.750, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:38 step:4800 train loss: 1.158, r2: -0.636, rate:0.375, acc:0.312 val loss: 1.347, r2: 0.272, rate:0.750, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:38 step:4900 train loss: 0.753, r2: -3.143, rate:0.625, acc:0.438 val loss: 0.794, r2: -0.379, rate:0.562, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:38 step:5000 train loss: 1.066, r2: 0.261, rate:0.750, acc:0.562 val loss: 0.850, r2: 0.280, rate:0.875, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:38 step:5100 train loss: 1.070, r2: -0.040, rate:0.688, acc:0.438 val loss: 0.483, r2: 0.605, rate:0.938, acc:1.000 lr 9.534187405839861e-08\n",
      "epoch:38 step:5200 train loss: 1.095, r2: -0.308, rate:0.312, acc:0.438 val loss: 0.906, r2: 0.496, rate:0.812, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:38 step:5300 train loss: 1.310, r2: -0.421, rate:0.750, acc:0.562 val loss: 0.901, r2: 0.292, rate:0.750, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:38 step:5400 train loss: 0.715, r2: 0.292, rate:0.750, acc:0.938 val loss: 2.806, r2: 0.122, rate:0.688, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:38 step:5500 train loss: 1.481, r2: -1.414, rate:0.375, acc:0.500 val loss: 0.836, r2: 0.498, rate:0.688, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:38 step:5600 train loss: 2.328, r2: -5.430, rate:0.688, acc:0.438 val loss: 3.054, r2: -1.502, rate:0.562, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:38 step:5700 train loss: 0.664, r2: -0.715, rate:0.750, acc:0.562 val loss: 0.909, r2: -2.019, rate:0.688, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:38 step:5800 train loss: 1.201, r2: -6.796, rate:0.812, acc:1.000 val loss: 1.994, r2: 0.221, rate:0.625, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:38 step:5900 train loss: 2.861, r2: -0.037, rate:0.688, acc:0.688 val loss: 1.027, r2: 0.456, rate:0.812, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:38 step:6000 train loss: 0.937, r2: -0.206, rate:0.625, acc:0.438 val loss: 1.041, r2: -0.507, rate:0.562, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:38 step:6100 train loss: 0.773, r2: 0.264, rate:0.688, acc:0.500 val loss: 0.655, r2: 0.373, rate:0.812, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:38 step:6200 train loss: 0.991, r2: -0.680, rate:0.312, acc:0.500 val loss: 0.630, r2: -0.157, rate:0.750, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:38 step:6300 train loss: 1.566, r2: -2.102, rate:1.000, acc:0.812 val loss: 1.607, r2: 0.108, rate:0.688, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:38 step:6400 train loss: 0.895, r2: 0.472, rate:0.625, acc:0.688 val loss: 0.692, r2: 0.424, rate:0.750, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:38 step:6500 train loss: 0.565, r2: 0.619, rate:0.688, acc:0.750 val loss: 1.309, r2: 0.170, rate:0.688, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:38 step:6600 train loss: 0.524, r2: 0.679, rate:0.938, acc:0.812 val loss: 2.697, r2: -0.108, rate:0.438, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:38 step:6700 train loss: 0.938, r2: 0.761, rate:0.938, acc:0.938 val loss: 1.642, r2: 0.156, rate:0.688, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:38 step:6800 train loss: 0.852, r2: -0.420, rate:0.688, acc:0.562 val loss: 0.631, r2: 0.802, rate:0.750, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:38 step:6900 train loss: 0.942, r2: -0.159, rate:0.875, acc:0.938 val loss: 0.747, r2: -0.028, rate:0.688, acc:0.250 lr 9.534187405839861e-08\n",
      "epoch:38 step:7000 train loss: 0.748, r2: -0.410, rate:0.688, acc:0.938 val loss: 1.227, r2: -6.415, rate:0.375, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:38 step:7100 train loss: 0.486, r2: 0.171, rate:0.938, acc:1.000 val loss: 0.698, r2: 0.433, rate:0.625, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:38 step:7200 train loss: 0.654, r2: -0.133, rate:0.688, acc:0.312 val loss: 1.430, r2: -3.935, rate:0.500, acc:0.125 lr 9.534187405839861e-08\n",
      "epoch:38 step:7300 train loss: 0.760, r2: 0.151, rate:0.750, acc:0.625 val loss: 1.004, r2: 0.121, rate:0.688, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:38 step:7400 train loss: 0.772, r2: -0.570, rate:0.938, acc:0.938 val loss: 1.649, r2: -0.132, rate:0.875, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:38 step:7500 train loss: 0.748, r2: 0.088, rate:0.688, acc:0.750 val loss: 1.441, r2: -0.718, rate:0.625, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:38 step:7600 train loss: 0.981, r2: -0.579, rate:0.562, acc:0.438 val loss: 0.811, r2: 0.097, rate:0.625, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:38 step:7700 train loss: 0.817, r2: 0.086, rate:0.688, acc:0.938 val loss: 1.143, r2: 0.299, rate:0.688, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:38 step:7800 train loss: 0.637, r2: 0.436, rate:0.812, acc:0.500 val loss: 1.061, r2: -1.863, rate:0.875, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:38 step:0.328 lr 0.000000\n",
      "epoch:39 step:0 train loss: 0.706, r2: -0.017, rate:0.688, acc:0.438 val loss: 1.070, r2: 0.352, rate:0.500, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:39 step:100 train loss: 0.978, r2: -1.370, rate:0.438, acc:0.688 val loss: 0.841, r2: -0.065, rate:0.500, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:39 step:200 train loss: 0.410, r2: 0.923, rate:1.000, acc:0.625 val loss: 0.865, r2: 0.267, rate:0.750, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:39 step:300 train loss: 0.645, r2: 0.563, rate:0.750, acc:0.562 val loss: 0.885, r2: 0.164, rate:0.812, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:39 step:400 train loss: 3.071, r2: -4.086, rate:0.812, acc:0.438 val loss: 0.941, r2: -0.711, rate:0.562, acc:0.375 lr 9.534187405839861e-08\n",
      "epoch:39 step:500 train loss: 0.997, r2: -4.549, rate:0.562, acc:0.625 val loss: 1.274, r2: -1.173, rate:0.375, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:39 step:600 train loss: 0.695, r2: 0.244, rate:0.938, acc:0.688 val loss: 1.653, r2: -0.374, rate:0.562, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:39 step:700 train loss: 0.804, r2: 0.036, rate:0.375, acc:0.875 val loss: 0.917, r2: -0.193, rate:0.562, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:39 step:800 train loss: 1.239, r2: 0.528, rate:0.562, acc:0.562 val loss: 0.615, r2: -0.324, rate:0.750, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:39 step:900 train loss: 0.710, r2: -1.374, rate:0.750, acc:0.562 val loss: 2.705, r2: 0.130, rate:0.500, acc:0.500 lr 9.534187405839861e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:39 step:1000 train loss: 1.249, r2: -1.173, rate:0.625, acc:0.312 val loss: 1.050, r2: 0.414, rate:0.812, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:39 step:1100 train loss: 0.938, r2: -0.729, rate:0.500, acc:0.188 val loss: 1.019, r2: 0.344, rate:0.562, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:39 step:1200 train loss: 1.067, r2: -0.066, rate:0.562, acc:0.438 val loss: 0.652, r2: 0.324, rate:0.688, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:39 step:1300 train loss: 0.532, r2: 0.697, rate:0.875, acc:0.625 val loss: 0.960, r2: 0.447, rate:0.688, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:39 step:1400 train loss: 0.837, r2: 0.486, rate:0.938, acc:0.812 val loss: 0.949, r2: -3.296, rate:0.688, acc:1.000 lr 9.534187405839861e-08\n",
      "epoch:39 step:1500 train loss: 0.834, r2: -0.062, rate:0.750, acc:0.688 val loss: 0.691, r2: 0.313, rate:0.812, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:39 step:1600 train loss: 2.492, r2: -4.063, rate:0.812, acc:0.938 val loss: 0.909, r2: -0.068, rate:0.625, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:39 step:1700 train loss: 0.743, r2: -0.351, rate:0.688, acc:0.875 val loss: 0.685, r2: -4.337, rate:0.688, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:39 step:1800 train loss: 2.310, r2: -4.583, rate:0.812, acc:0.625 val loss: 2.292, r2: -0.694, rate:0.375, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:39 step:1900 train loss: 2.041, r2: -5.393, rate:0.500, acc:0.938 val loss: 1.196, r2: -0.713, rate:0.688, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:39 step:2000 train loss: 1.316, r2: 0.174, rate:0.500, acc:0.375 val loss: 1.532, r2: -1.218, rate:0.500, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:39 step:2100 train loss: 0.755, r2: 0.473, rate:0.500, acc:0.562 val loss: 0.690, r2: 0.504, rate:0.875, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:39 step:2200 train loss: 1.839, r2: -5.300, rate:0.500, acc:0.750 val loss: 2.247, r2: -2.490, rate:0.750, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:39 step:2300 train loss: 1.271, r2: -0.313, rate:0.562, acc:0.375 val loss: 0.698, r2: 0.467, rate:0.750, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:39 step:2400 train loss: 1.065, r2: -0.266, rate:0.688, acc:1.000 val loss: 0.784, r2: 0.035, rate:0.750, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:39 step:2500 train loss: 2.315, r2: -0.463, rate:0.938, acc:0.938 val loss: 1.430, r2: -1.350, rate:0.312, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:39 step:2600 train loss: 0.712, r2: -1.296, rate:0.688, acc:0.625 val loss: 0.712, r2: 0.477, rate:0.938, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:39 step:2700 train loss: 0.696, r2: 0.373, rate:0.812, acc:0.812 val loss: 0.984, r2: -0.909, rate:0.750, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:39 step:2800 train loss: 4.180, r2: -6.305, rate:0.812, acc:0.750 val loss: 0.755, r2: 0.271, rate:0.750, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:39 step:2900 train loss: 0.638, r2: 0.310, rate:0.875, acc:0.625 val loss: 1.193, r2: -0.167, rate:0.750, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:39 step:3000 train loss: 0.651, r2: 0.524, rate:0.812, acc:1.000 val loss: 1.256, r2: -1.371, rate:0.688, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:39 step:3100 train loss: 0.800, r2: 0.632, rate:0.625, acc:0.562 val loss: 1.667, r2: -0.272, rate:0.812, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:39 step:3200 train loss: 0.847, r2: -0.471, rate:0.500, acc:0.750 val loss: 0.541, r2: 0.615, rate:0.875, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:39 step:3300 train loss: 0.833, r2: -3.048, rate:0.500, acc:0.750 val loss: 0.798, r2: 0.196, rate:0.625, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:39 step:3400 train loss: 0.859, r2: -0.871, rate:0.750, acc:0.938 val loss: 0.571, r2: 0.625, rate:0.938, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:39 step:3500 train loss: 0.613, r2: 0.388, rate:0.688, acc:0.625 val loss: 0.912, r2: 0.057, rate:0.562, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:39 step:3600 train loss: 0.609, r2: -0.269, rate:0.812, acc:0.688 val loss: 0.678, r2: 0.456, rate:0.812, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:39 step:3700 train loss: 0.762, r2: -1.011, rate:0.812, acc:0.375 val loss: 1.136, r2: -0.134, rate:0.750, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:39 step:3800 train loss: 1.656, r2: -0.460, rate:0.625, acc:0.375 val loss: 0.710, r2: -0.135, rate:0.750, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:39 step:3900 train loss: 0.637, r2: 0.296, rate:0.812, acc:0.875 val loss: 0.855, r2: 0.172, rate:0.875, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:39 step:4000 train loss: 0.758, r2: 0.527, rate:0.562, acc:0.812 val loss: 1.864, r2: -0.036, rate:0.812, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:39 step:4100 train loss: 0.774, r2: 0.522, rate:0.688, acc:0.625 val loss: 0.824, r2: 0.461, rate:0.750, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:39 step:4200 train loss: 0.645, r2: -1.820, rate:0.750, acc:0.562 val loss: 0.868, r2: 0.377, rate:0.500, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:39 step:4300 train loss: 2.423, r2: -3.861, rate:0.500, acc:0.250 val loss: 1.014, r2: 0.224, rate:0.625, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:39 step:4400 train loss: 4.249, r2: -3.291, rate:0.500, acc:0.750 val loss: 0.653, r2: -0.035, rate:0.625, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:39 step:4500 train loss: 1.271, r2: -0.892, rate:0.562, acc:0.750 val loss: 1.038, r2: 0.612, rate:0.750, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:39 step:4600 train loss: 2.167, r2: -1.023, rate:0.375, acc:0.188 val loss: 1.142, r2: -1.482, rate:0.562, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:39 step:4700 train loss: 0.769, r2: 0.409, rate:0.750, acc:0.750 val loss: 2.543, r2: -0.620, rate:0.688, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:39 step:4800 train loss: 0.475, r2: 0.394, rate:1.000, acc:0.875 val loss: 0.694, r2: 0.051, rate:0.688, acc:0.812 lr 9.534187405839861e-08\n",
      "epoch:39 step:4900 train loss: 0.613, r2: 0.209, rate:0.812, acc:0.688 val loss: 0.655, r2: 0.160, rate:0.688, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:39 step:5000 train loss: 1.957, r2: 0.419, rate:0.688, acc:0.688 val loss: 0.658, r2: -0.038, rate:0.625, acc:0.438 lr 9.534187405839861e-08\n",
      "epoch:39 step:5100 train loss: 0.508, r2: 0.860, rate:0.750, acc:0.750 val loss: 0.889, r2: -1.511, rate:0.562, acc:0.188 lr 9.534187405839861e-08\n",
      "epoch:39 step:5200 train loss: 0.749, r2: 0.344, rate:0.688, acc:0.562 val loss: 1.731, r2: -0.498, rate:0.500, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:39 step:5300 train loss: 0.750, r2: -1.085, rate:0.875, acc:0.625 val loss: 0.589, r2: 0.649, rate:0.875, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:39 step:5400 train loss: 0.698, r2: -0.610, rate:0.750, acc:0.562 val loss: 0.743, r2: -0.249, rate:0.500, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:39 step:5500 train loss: 0.762, r2: -0.987, rate:0.625, acc:0.625 val loss: 0.637, r2: 0.361, rate:0.688, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:39 step:5600 train loss: 2.322, r2: 0.445, rate:0.562, acc:0.938 val loss: 1.515, r2: -0.312, rate:0.625, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:39 step:5700 train loss: 0.791, r2: -0.074, rate:0.688, acc:0.312 val loss: 1.679, r2: 0.072, rate:0.688, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:39 step:5800 train loss: 1.322, r2: 0.339, rate:0.562, acc:0.875 val loss: 0.735, r2: 0.089, rate:0.750, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:39 step:5900 train loss: 0.786, r2: 0.698, rate:0.688, acc:0.875 val loss: 1.909, r2: 0.413, rate:0.812, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:39 step:6000 train loss: 0.726, r2: 0.568, rate:0.688, acc:0.812 val loss: 0.555, r2: 0.735, rate:0.812, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:39 step:6100 train loss: 1.014, r2: -0.799, rate:0.562, acc:0.375 val loss: 1.256, r2: -0.476, rate:0.750, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:39 step:6200 train loss: 0.772, r2: -0.369, rate:0.688, acc:0.688 val loss: 0.983, r2: -0.951, rate:0.688, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:39 step:6300 train loss: 0.723, r2: 0.098, rate:0.812, acc:0.812 val loss: 0.849, r2: 0.132, rate:0.750, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:39 step:6400 train loss: 2.463, r2: -0.551, rate:1.000, acc:0.938 val loss: 0.580, r2: 0.460, rate:0.938, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:39 step:6500 train loss: 17.512, r2: -4.691, rate:0.875, acc:0.750 val loss: 0.631, r2: 0.278, rate:0.625, acc:0.750 lr 9.534187405839861e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:39 step:6600 train loss: 1.203, r2: -0.486, rate:0.562, acc:0.625 val loss: 0.923, r2: -0.721, rate:0.688, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:39 step:6700 train loss: 1.899, r2: -1.933, rate:0.625, acc:0.938 val loss: 1.942, r2: 0.400, rate:0.688, acc:0.875 lr 9.534187405839861e-08\n",
      "epoch:39 step:6800 train loss: 1.005, r2: -0.175, rate:0.625, acc:0.875 val loss: 1.060, r2: -1.609, rate:0.688, acc:0.750 lr 9.534187405839861e-08\n",
      "epoch:39 step:6900 train loss: 0.703, r2: 0.208, rate:0.688, acc:0.500 val loss: 2.377, r2: -2.961, rate:0.438, acc:0.250 lr 9.534187405839861e-08\n",
      "epoch:39 step:7000 train loss: 0.606, r2: -0.475, rate:0.812, acc:1.000 val loss: 0.981, r2: 0.032, rate:0.688, acc:0.500 lr 9.534187405839861e-08\n",
      "epoch:39 step:7100 train loss: 1.073, r2: -2.242, rate:0.875, acc:1.000 val loss: 0.633, r2: -0.006, rate:0.688, acc:0.938 lr 9.534187405839861e-08\n",
      "epoch:39 step:7200 train loss: 0.777, r2: -1.308, rate:0.688, acc:0.812 val loss: 0.758, r2: 0.238, rate:0.812, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:39 step:7300 train loss: 0.777, r2: 0.055, rate:0.688, acc:0.438 val loss: 0.902, r2: 0.567, rate:0.750, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:39 step:7400 train loss: 1.077, r2: -0.105, rate:0.625, acc:0.750 val loss: 0.616, r2: 0.657, rate:0.812, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:39 step:7500 train loss: 1.283, r2: -7.002, rate:0.500, acc:0.562 val loss: 0.805, r2: 0.636, rate:0.500, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:39 step:7600 train loss: 1.684, r2: 0.348, rate:0.625, acc:0.500 val loss: 2.660, r2: 0.048, rate:0.562, acc:0.625 lr 9.534187405839861e-08\n",
      "epoch:39 step:7700 train loss: 1.391, r2: 0.388, rate:0.812, acc:0.812 val loss: 1.224, r2: 0.091, rate:0.562, acc:0.562 lr 9.534187405839861e-08\n",
      "epoch:39 step:7800 train loss: 0.665, r2: -0.133, rate:0.750, acc:0.688 val loss: 1.124, r2: 0.013, rate:0.500, acc:0.688 lr 9.534187405839861e-08\n",
      "epoch:39 step:0.328 lr 0.000000\n"
     ]
    }
   ],
   "source": [
    "epochs = 40\n",
    "for e in range(0, epochs):\n",
    "    out = []\n",
    "    for b in range(sum_n//batch_size - 1):\n",
    "        t1.train()\n",
    "        # t2.train()\n",
    "        optimizer1.zero_grad()\n",
    "        # optimizer2.zero_grad()\n",
    "\n",
    "        X, Y = train_fs[train_file_index % len(train_fs)].next_item()\n",
    "        train_file_index += 1\n",
    "\n",
    "        #Forward pass and calculate loss\n",
    "        net_out = t1(X)\n",
    "        #print(net_out.shape,Y.shape)\n",
    "        loss1 = torch.mean((net_out[0] - Y) ** 2) + F.binary_cross_entropy_with_logits(net_out[1], torch.gt(Y, 0).float())\n",
    "        net_out = net_out[0]\n",
    "        if b % 100 == 0:\n",
    "            scheduler1.step(loss1)\n",
    "        #backwards pass\n",
    "        loss1.backward()\n",
    "        nn.utils.clip_grad_norm_(t1.parameters(), max_norm=20, norm_type=2)\n",
    "        optimizer1.step()\n",
    "        \n",
    "        # ============================= #\n",
    "        \n",
    "#         #Forward pass and calculate loss\n",
    "#         net_out = t2(X)\n",
    "#         #print(net_out.shape,Y.shape)\n",
    "#         loss2 = torch.mean((net_out - Y) ** 2)\n",
    "#         #backwards pass\n",
    "#         loss2.backward()\n",
    "#         nn.utils.clip_grad_norm_(t2.parameters(), max_norm=20, norm_type=2)\n",
    "#         optimizer2.step()\n",
    "        \n",
    "        \n",
    "        if b % 100 == 0:\n",
    "            t1.eval()\n",
    "            # t2.eval()\n",
    "            # net_out = (t1(X) + t2(X)) / 2\n",
    "            # train\n",
    "            X, Y = train_fs[train_file_index % len(train_fs)].next_item()\n",
    "            net_out = t1(X)\n",
    "            loss = torch.mean((net_out[0] - Y) ** 2)+ F.binary_cross_entropy_with_logits(net_out[1], torch.gt(Y, 0).float())\n",
    "\n",
    "            r2 = f_r2(net_out[0], Y)\n",
    "            rate = f_rate(net_out[0], Y)\n",
    "            acc = f_acc(net_out[1], torch.gt(Y, 0))\n",
    "            \n",
    "            train_loss = f_average(loss, train_loss)\n",
    "            train_rate = f_average(rate, train_rate)\n",
    "            train_r2 = f_average(r2, train_r2)\n",
    "            train_acc = f_average(acc, train_acc)\n",
    "\n",
    "            # test\n",
    "            tst_X, tst_Y = test_fs[test_file_index%len(test_fs)].next_item()\n",
    "            test_file_index += 1\n",
    "            \n",
    "            # net_out = (t1(tst_X) + t2(tst_X)) / 2\n",
    "            with torch.no_grad():\n",
    "                net_out  = t1(tst_X)\n",
    "            loss = torch.mean((net_out[0] - tst_Y) ** 2)+ F.binary_cross_entropy_with_logits(net_out[1], torch.gt(tst_Y, 0).float())\n",
    "\n",
    "            r2 = f_r2(net_out[0], tst_Y)\n",
    "            rate = f_rate(net_out[0], tst_Y)\n",
    "            acc = f_acc(net_out[1], torch.gt(tst_Y, 0))\n",
    "\n",
    "            metrics_loss = f_average(loss, metrics_loss)\n",
    "            metrics_rate = f_average(rate, metrics_rate)\n",
    "            metrics_r2 = f_average(r2, metrics_r2)\n",
    "            metrics_acc = f_average(acc, metrics_acc)\n",
    "\n",
    "            print('epoch:{} step:{}'.format(e, b), 'train loss: {:.3f}, r2: {:.3f}, rate:{:.3f}, acc:{:.3f}'.format(train_loss, train_r2, train_rate, train_acc), 'val loss: {:.3f}, r2: {:.3f}, rate:{:.3f}, acc:{:.3f}'.format(metrics_loss, metrics_r2, metrics_rate, metrics_acc), 'lr', optimizer1.state_dict()['param_groups'][0]['lr'])\n",
    "    # eval\n",
    "    t1.eval()\n",
    "    avg_r2 = []\n",
    "    for eval_test_file_index in range(len(test_fs)):\n",
    "        mask = test_fs[eval_test_file_index].mask\n",
    "        eval_X = test_fs[eval_test_file_index].f['x'][:][:,:,mask]\n",
    "        eval_Y = test_fs[eval_test_file_index].f['y'][:]\n",
    "\n",
    "        eval_X = torch.Tensor(eval_X).cuda()\n",
    "        net_out = t1(eval_X)[0].cpu().detach().numpy()\n",
    "        r2 = 1 - np.sum((net_out - eval_Y)**2) /(np.sum((eval_Y - np.mean(eval_Y))**2) + 1e-4)\n",
    "        avg_r2.append(r2)\n",
    "    with open('model/epoch_{}_r2_{:.3f}.txt'.format(e, float(np.mean(avg_r2))), 'w') as f:\n",
    "        for eval_test_file_index, r2 in enumerate(avg_r2):\n",
    "            f.write(test_fs[eval_test_file_index].name + ' ' + '{:.3f}'.format(r2) + '\\n')\n",
    "    torch.save({'state_dict': t1.state_dict()}, 'model/rnn_sgd_scheduler_0.01_model_epoch_{}_r2_{:.3f}.pth.tar'.format(e, float(np.mean(avg_r2))))\n",
    "    \n",
    "    print('epoch:{} step:{:.3f}'.format(e, np.mean(avg_r2)), 'lr {:.6f}'.format(optimizer1.state_dict()['param_groups'][0]['lr']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "different-given",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('model/adam_scheduler_0.01_model_epoch_1_r2_0.344.pth.tar')\n",
    "t1.load_state_dict(checkpoint['state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dietary-spoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "SGD 0.002\n",
    "epoch:0 step:0.340\n",
    "epoch:1 step:0.332\n",
    "epoch:2 step:0.338\n",
    "epoch:3 step:0.330\n",
    "epoch:4 step:0.348\n",
    "epoch:5 step:0.350\n",
    "epoch:6 step:0.352\n",
    "epoch:7 step:0.281\n",
    "epoch:8 step:0.352\n",
    "epoch:9 step:0.350\n",
    "epoch:10 step:0.346\n",
    "epoch:11 step:0.338\n",
    "epoch:12 step:0.351\n",
    "epoch:13 step:0.351\n",
    "epoch:14 step:0.322\n",
    "epoch:15 step:0.352\n",
    "epoch:16 step:0.351\n",
    "epoch:17 step:0.347\n",
    "epoch:18 step:0.348\n",
    "epoch:19 step:0.350\n",
    "\n",
    "SGD 0.01\n",
    "epoch:0 step:0.251\n",
    "epoch:1 step:0.341\n",
    "epoch:2 step:0.315\n",
    "epoch:3 step:0.318\n",
    "epoch:4 step:0.339\n",
    "epoch:5 step:0.329\n",
    "epoch:6 step:0.347\n",
    "epoch:7 step:0.346\n",
    "epoch:8 step:0.328\n",
    "epoch:9 step:0.341\n",
    "epoch:10 step:0.348\n",
    "epoch:11 step:0.343\n",
    "epoch:12 step:0.344\n",
    "epoch:13 step:0.181\n",
    "epoch:14 step:0.302\n",
    "epoch:15 step:0.329\n",
    "epoch:16 step:0.345\n",
    "epoch:17 step:0.321\n",
    "epoch:18 step:0.340\n",
    "epoch:19 step:0.347\n",
    "\n",
    "SGD 0.0005\n",
    "epoch:19 step:0.341\n",
    "epoch:20 step:0.306\n",
    "epoch:21 step:0.321\n",
    "epoch:22 step:0.336\n",
    "epoch:23 step:0.339\n",
    "epoch:24 step:0.339\n",
    "epoch:25 step:0.345\n",
    "epoch:26 step:0.345\n",
    "epoch:27 step:0.337\n",
    "epoch:28 step:0.334\n",
    "epoch:29 step:0.334\n",
    "epoch:30 step:0.334\n",
    "epoch:31 step:0.338\n",
    "epoch:32 step:0.344\n",
    "epoch:33 step:0.350\n",
    "epoch:34 step:0.352\n",
    "epoch:35 step:0.352\n",
    "epoch:36 step:0.349\n",
    "epoch:37 step:0.352\n",
    "epoch:38 step:0.354\n",
    "epoch:39 step:0.353\n",
    "epoch:40 step:0.349\n",
    "epoch:41 step:0.354\n",
    "epoch:42 step:0.354\n",
    "epoch:43 step:0.347\n",
    "epoch:44 step:0.347\n",
    "epoch:45 step:0.350\n",
    "epoch:46 step:0.349\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approximate-explorer",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dental-crowd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval\n",
    "t1.eval()\n",
    "avg_r2 = []\n",
    "for eval_test_file_index in range(len(test_fs)):\n",
    "    mask = test_fs[eval_test_file_index].mask\n",
    "    eval_X = test_fs[eval_test_file_index].f['x'][:][:,:,mask]\n",
    "    eval_Y = test_fs[eval_test_file_index].f['y'][:]\n",
    "\n",
    "    eval_X = torch.Tensor(eval_X).cuda()\n",
    "    net_out = t1(eval_X).cpu().detach().numpy()\n",
    "    net_out = (y_max - y_min) * net_out + y_min\n",
    "    r2 = 1 - np.sum((net_out - eval_Y)**2) /(np.sum((eval_Y - np.mean(eval_Y))**2) + 1e-4)\n",
    "    avg_r2.append(r2)\n",
    "with open('model/epoch_{}_r2_{:.3f}.txt'.format(e, float(np.mean(avg_r2))), 'w') as f:\n",
    "    for eval_test_file_index, r2 in enumerate(avg_r2):\n",
    "        f.write(test_fs[eval_test_file_index].name + ' ' + '{:.3f}'.format(r2) + '\\n')\n",
    "torch.save({'state_dict': t1.state_dict()}, 'model/rnn_sgd_scheduler_0.01_model_epoch_{}_r2_{:.3f}.pth.tar'.format(e, float(np.mean(avg_r2))))\n",
    "\n",
    "print('epoch:{} step:{:.3f}'.format(e, np.mean(avg_r2)), 'lr {:.6f}'.format(optimizer1.state_dict()['param_groups'][0]['lr']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convinced-attempt",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(avg_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drawn-jesus",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = RNN(dim_val, dim_attn, 7, n_layers=2, n_heads=n_heads)\n",
    "checkpoint = torch.load('model/select_rnn_model_epoch_2_r2_0.313.pth.tar')\n",
    "t1.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "t2 = RNN(dim_val, dim_attn, 8, n_layers=2, n_heads=n_heads)\n",
    "checkpoint = torch.load('model/select_rnn2_model_epoch_2_r2_0.109.pth.tar')\n",
    "t2.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "\n",
    "t3 = RNN(dim_val, dim_attn, 15, n_layers=2, n_heads=n_heads)\n",
    "checkpoint = torch.load('model/rnn2_model_epoch_11_r2_0.341.pth.tar')\n",
    "t3.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "t1.cuda()\n",
    "t2.cuda()\n",
    "t3.cuda()\n",
    "\n",
    "t1.eval()\n",
    "t2.eval()\n",
    "t3.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competitive-shelter",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_r2 = []\n",
    "for eval_test_file_index in range(len(test_fs)):\n",
    "\n",
    "    eval_X = test_fs[eval_test_file_index].f['x'][:]\n",
    "    eval_X1 = eval_X[:,:,[0, 2, 10, 11, 12, 13, 14]]\n",
    "    # eval_X2 = eval_X[:,:,[1, 3, 4, 5, 6, 7, 8, 9]]\n",
    "    eval_Y = test_fs[eval_test_file_index].f['y'][:]\n",
    "\n",
    "    eval_X = torch.Tensor(eval_X).cuda()\n",
    "    eval_X1 = torch.Tensor(eval_X1).cuda()\n",
    "    # eval_X2 = torch.Tensor(eval_X2).cuda()\n",
    "\n",
    "    net_out1 = t1(eval_X1).cpu().detach().numpy()\n",
    "    r2_1 = 1 - np.sum((net_out1 - eval_Y)**2) /(np.sum((eval_Y - np.mean(eval_Y))**2) + 1e-4)\n",
    "\n",
    "    # net_out2 = t2(eval_X2).cpu().detach().numpy()\n",
    "\n",
    "    # plt.plot(net_out1[0:200])\n",
    "    # plt.plot(eval_Y[0:200])\n",
    "    # plt.show()\n",
    "\n",
    "    # plt.plot(net_out2[0:200])\n",
    "    # plt.plot(eval_Y[0:200])\n",
    "    # plt.show()\n",
    "\n",
    "    net_out3 = t3(eval_X).cpu().detach().numpy()\n",
    "    r2_3 = 1 - np.sum((net_out3 - eval_Y)**2) /(np.sum((eval_Y - np.mean(eval_Y))**2) + 1e-4)\n",
    "\n",
    "    # r2 = 1 - np.sum((net_out3 - eval_Y)**2) /(np.sum((eval_Y - np.mean(eval_Y))**2) + 1e-4)\n",
    "    # print(r2)\n",
    "    net_out = 0.3 *net_out1 + 0.7*net_out3\n",
    "    r2 = 1 - np.sum((net_out - eval_Y)**2) /(np.sum((eval_Y - np.mean(eval_Y))**2) + 1e-4)\n",
    "\n",
    "    avg_r2.append([r2_1, r2_3, r2])\n",
    "\n",
    "\n",
    "# plt.plot(net_out[0:200])\n",
    "# plt.plot(eval_Y[0:200])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pretty-resolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(avg_r2).mean(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
