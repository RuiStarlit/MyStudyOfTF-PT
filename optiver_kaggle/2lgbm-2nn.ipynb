{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "578c050e",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-09-22T13:00:56.635047Z",
     "iopub.status.busy": "2021-09-22T13:00:56.634428Z",
     "iopub.status.idle": "2021-09-22T13:00:57.501696Z",
     "shell.execute_reply": "2021-09-22T13:00:57.501131Z",
     "shell.execute_reply.started": "2021-09-12T04:45:17.208857Z"
    },
    "papermill": {
     "duration": 0.903111,
     "end_time": "2021-09-22T13:00:57.501848",
     "exception": false,
     "start_time": "2021-09-22T13:00:56.598737",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import glob\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from sklearn import preprocessing, model_selection\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import numpy.matlib\n",
    "\n",
    "\n",
    "path_submissions = '/'\n",
    "\n",
    "target_name = 'target'\n",
    "scores_folds = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f8882a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-22T13:00:57.592838Z",
     "iopub.status.busy": "2021-09-22T13:00:57.570508Z",
     "iopub.status.idle": "2021-09-22T13:00:57.602351Z",
     "shell.execute_reply": "2021-09-22T13:00:57.601870Z",
     "shell.execute_reply.started": "2021-09-12T04:45:18.297331Z"
    },
    "papermill": {
     "duration": 0.078964,
     "end_time": "2021-09-22T13:00:57.602454",
     "exception": false,
     "start_time": "2021-09-22T13:00:57.523490",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data directory\n",
    "data_dir = 'input/optiver-realized-volatility-prediction/'\n",
    "\n",
    "# Function to calculate first WAP\n",
    "def calc_wap1(df):\n",
    "    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "    return wap\n",
    "\n",
    "# Function to calculate second WAP\n",
    "def calc_wap2(df):\n",
    "    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "    return wap\n",
    "\n",
    "def calc_wap3(df):\n",
    "    wap = (df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "    return wap\n",
    "\n",
    "def calc_wap4(df):\n",
    "    wap = (df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "    return wap\n",
    "\n",
    "# Function to calculate the log of the return\n",
    "# Remember that logb(x / y) = logb(x) - logb(y)\n",
    "def log_return(series):\n",
    "    return np.log(series).diff()\n",
    "\n",
    "# Calculate the realized volatility\n",
    "def realized_volatility(series):\n",
    "    return np.sqrt(np.sum(series**2))\n",
    "\n",
    "# Function to count unique elements of a series\n",
    "def count_unique(series):\n",
    "    return len(np.unique(series))\n",
    "\n",
    "def read_train():\n",
    "    train = pd.read_csv('input/optiver-realized-volatility-prediction/train.csv')\n",
    "#     test = pd.read_csv('input/optiver-realized-volatility-prediction/test.csv')\n",
    "    # Create a key to merge with book and trade data\n",
    "    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n",
    "#     test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n",
    "    print(f'Our training set has {train.shape[0]} rows')\n",
    "    return train\n",
    "\n",
    "# Function to read our base train and test set\n",
    "def read_train_test():\n",
    "#     train = pd.read_csv('input/optiver-realized-volatility-prediction/train.csv')\n",
    "    test = pd.read_csv('input/optiver-realized-volatility-prediction/test.csv')\n",
    "    # Create a key to merge with book and trade data\n",
    "#     train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n",
    "    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n",
    "    print(f'Our training set has {train.shape[0]} rows')\n",
    "    return test\n",
    "\n",
    "# Function to preprocess book data (for each stock id)\n",
    "def book_preprocessor(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    # Calculate Wap\n",
    "    df['wap1'] = calc_wap1(df)\n",
    "    df['wap2'] = calc_wap2(df)\n",
    "    df['wap3'] = calc_wap3(df)\n",
    "    df['wap4'] = calc_wap4(df)\n",
    "    # Calculate log returns\n",
    "    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n",
    "    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n",
    "    df['log_return3'] = df.groupby(['time_id'])['wap3'].apply(log_return)\n",
    "    df['log_return4'] = df.groupby(['time_id'])['wap4'].apply(log_return)\n",
    "    # Calculate wap balance\n",
    "    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n",
    "    # Calculate spread\n",
    "    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n",
    "    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) / ((df['ask_price2'] + df['bid_price2']) / 2)\n",
    "    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n",
    "    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n",
    "    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n",
    "    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n",
    "    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n",
    "    \n",
    "    # Dict for aggregations\n",
    "    create_feature_dict = {\n",
    "        'wap1': [np.sum, np.std],\n",
    "        'wap2': [np.sum, np.std],\n",
    "        'wap3': [np.sum, np.std],\n",
    "        'wap4': [np.sum, np.std],\n",
    "        'log_return1': [realized_volatility],\n",
    "        'log_return2': [realized_volatility],\n",
    "        'log_return3': [realized_volatility],\n",
    "        'log_return4': [realized_volatility],\n",
    "        'wap_balance': [np.sum, np.max],\n",
    "        'price_spread':[np.sum, np.max],\n",
    "        'price_spread2':[np.sum, np.max],\n",
    "        'bid_spread':[np.sum, np.max],\n",
    "        'ask_spread':[np.sum, np.max],\n",
    "        'total_volume':[np.sum, np.max],\n",
    "        'volume_imbalance':[np.sum, np.max],\n",
    "        \"bid_ask_spread\":[np.sum,  np.max],\n",
    "    }\n",
    "    create_feature_dict_time = {\n",
    "        'log_return1': [realized_volatility],\n",
    "        'log_return2': [realized_volatility],\n",
    "        'log_return3': [realized_volatility],\n",
    "        'log_return4': [realized_volatility],\n",
    "    }\n",
    "    \n",
    "    # Function to get group stats for different windows (seconds in bucket)\n",
    "    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n",
    "        # Group by the window\n",
    "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n",
    "        # Rename columns joining suffix\n",
    "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "        # Add a suffix to differentiate windows\n",
    "        if add_suffix:\n",
    "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
    "        return df_feature\n",
    "    \n",
    "    # Get the stats for different windows\n",
    "    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n",
    "    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n",
    "    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n",
    "    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n",
    "    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n",
    "    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n",
    "\n",
    "    # Merge all\n",
    "    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n",
    "    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n",
    "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
    "    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n",
    "    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n",
    "    # Drop unnecesary time_ids\n",
    "    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id__100'], axis = 1, inplace = True)\n",
    "    \n",
    "    \n",
    "    # Create row_id so we can merge\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n",
    "    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n",
    "    return df_feature\n",
    "\n",
    "# Function to preprocess trade data (for each stock id)\n",
    "def trade_preprocessor(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n",
    "    df['amount']=df['price']*df['size']\n",
    "    # Dict for aggregations\n",
    "    create_feature_dict = {\n",
    "        'log_return':[realized_volatility],\n",
    "        'seconds_in_bucket':[count_unique],\n",
    "        'size':[np.sum, np.max, np.min],\n",
    "        'order_count':[np.sum,np.max],\n",
    "        'amount':[np.sum,np.max,np.min],\n",
    "    }\n",
    "    create_feature_dict_time = {\n",
    "        'log_return':[realized_volatility],\n",
    "        'seconds_in_bucket':[count_unique],\n",
    "        'size':[np.sum],\n",
    "        'order_count':[np.sum],\n",
    "    }\n",
    "    # Function to get group stats for different windows (seconds in bucket)\n",
    "    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n",
    "        # Group by the window\n",
    "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n",
    "        # Rename columns joining suffix\n",
    "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "        # Add a suffix to differentiate windows\n",
    "        if add_suffix:\n",
    "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
    "        return df_feature\n",
    "    \n",
    "\n",
    "    # Get the stats for different windows\n",
    "    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n",
    "    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n",
    "    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n",
    "    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n",
    "    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n",
    "    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n",
    "    \n",
    "    def tendency(price, vol):    \n",
    "        df_diff = np.diff(price)\n",
    "        val = (df_diff/price[1:])*100\n",
    "        power = np.sum(val*vol[1:])\n",
    "        return(power)\n",
    "    \n",
    "    lis = []\n",
    "    for n_time_id in df['time_id'].unique():\n",
    "        df_id = df[df['time_id'] == n_time_id]        \n",
    "        tendencyV = tendency(df_id['price'].values, df_id['size'].values)      \n",
    "        f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n",
    "        f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n",
    "        df_max =  np.sum(np.diff(df_id['price'].values) > 0)\n",
    "        df_min =  np.sum(np.diff(df_id['price'].values) < 0)\n",
    "        # new\n",
    "        abs_diff = np.median(np.abs( df_id['price'].values - np.mean(df_id['price'].values)))        \n",
    "        energy = np.mean(df_id['price'].values**2)\n",
    "        iqr_p = np.percentile(df_id['price'].values,75) - np.percentile(df_id['price'].values,25)\n",
    "        \n",
    "        # vol vars\n",
    "        \n",
    "        abs_diff_v = np.median(np.abs( df_id['size'].values - np.mean(df_id['size'].values)))        \n",
    "        energy_v = np.sum(df_id['size'].values**2)\n",
    "        iqr_p_v = np.percentile(df_id['size'].values,75) - np.percentile(df_id['size'].values,25)\n",
    "        \n",
    "        lis.append({'time_id':n_time_id,'tendency':tendencyV,'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,\n",
    "                   'abs_diff':abs_diff,'energy':energy,'iqr_p':iqr_p,'abs_diff_v':abs_diff_v,'energy_v':energy_v,'iqr_p_v':iqr_p_v})\n",
    "    \n",
    "    df_lr = pd.DataFrame(lis)\n",
    "        \n",
    "   \n",
    "    df_feature = df_feature.merge(df_lr, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n",
    "    \n",
    "    # Merge all\n",
    "    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n",
    "    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n",
    "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
    "    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n",
    "    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n",
    "    # Drop unnecesary time_ids\n",
    "    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id','time_id__100'], axis = 1, inplace = True)\n",
    "    \n",
    "    \n",
    "    df_feature = df_feature.add_prefix('trade_')\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n",
    "    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n",
    "    return df_feature\n",
    "\n",
    "# Function to get group stats for the stock_id and time_id\n",
    "def get_time_stock(df):\n",
    "    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_400', 'log_return2_realized_volatility_400', \n",
    "                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_200', 'log_return2_realized_volatility_200', \n",
    "                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_400', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_200']\n",
    "\n",
    "\n",
    "    # Group by the stock id\n",
    "    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
    "    # Rename columns joining suffix\n",
    "    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n",
    "    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n",
    "\n",
    "    # Group by the stock id\n",
    "    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
    "    # Rename columns joining suffix\n",
    "    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n",
    "    df_time_id = df_time_id.add_suffix('_' + 'time')\n",
    "    \n",
    "    # Merge with original dataframe\n",
    "    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n",
    "    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n",
    "    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n",
    "    return df\n",
    "    \n",
    "# Funtion to make preprocessing function in parallel (for each stock id)\n",
    "def preprocessor(list_stock_ids, is_train = True):\n",
    "    \n",
    "    # Parrallel for loop\n",
    "    def for_joblib(stock_id):\n",
    "        # Train\n",
    "        if is_train:\n",
    "            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n",
    "        # Test\n",
    "        else:\n",
    "            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n",
    "    \n",
    "        # Preprocess book and trade data and merge them\n",
    "        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n",
    "        \n",
    "        # Return the merge dataframe\n",
    "        return df_tmp\n",
    "    \n",
    "    # Use parallel api to call paralle for loop\n",
    "    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n",
    "    # Concatenate all the dataframes that return from Parallel\n",
    "    df = pd.concat(df, ignore_index = True)\n",
    "    return df\n",
    "\n",
    "# Function to calculate the root mean squared percentage error\n",
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "# Function to early stop with root mean squared percentage error\n",
    "def feval_rmspe(y_pred, lgb_train):\n",
    "    y_true = lgb_train.get_label()\n",
    "    return 'RMSPE', rmspe(y_true, y_pred), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "987a32ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-22T13:00:57.654017Z",
     "iopub.status.busy": "2021-09-22T13:00:57.652516Z",
     "iopub.status.idle": "2021-09-22T13:01:02.451723Z",
     "shell.execute_reply": "2021-09-22T13:01:02.452505Z",
     "shell.execute_reply.started": "2021-09-12T04:45:18.36654Z"
    },
    "papermill": {
     "duration": 4.829193,
     "end_time": "2021-09-22T13:01:02.452746",
     "exception": false,
     "start_time": "2021-09-22T13:00:57.623553",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our training set has 428932 rows\n",
      "Our training set has 428932 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:  1.7min\n",
      "d:\\Users\\RuiSavior\\anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:688: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=-1)]: Done 112 out of 112 | elapsed:  6.2min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "# Read train and test\n",
    "#train =pd.read_pickle(\"../input/optiver006/train.pkl\")\n",
    "train = read_train()\n",
    "test = read_train_test()\n",
    "\n",
    "# Get unique stock ids \n",
    "train_stock_ids = train['stock_id'].unique()\n",
    "# Preprocess them using Parallel and our single stock id functions\n",
    "train_ = preprocessor(train_stock_ids, is_train = True)\n",
    "train = train.merge(train_, on = ['row_id'], how = 'left')\n",
    "\n",
    "# Get unique stock ids \n",
    "test_stock_ids = test['stock_id'].unique()\n",
    "# Preprocess them using Parallel and our single stock id functions\n",
    "test_ = preprocessor(test_stock_ids, is_train = False)\n",
    "test = test.merge(test_, on = ['row_id'], how = 'left')\n",
    "\n",
    "# Get group stats of time_id and stock_id\n",
    "#train = get_time_stock(train)\n",
    "test = get_time_stock(test)\n",
    "\n",
    "train1=train\n",
    "test1=test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "547b8bf8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-22T13:01:02.520312Z",
     "iopub.status.busy": "2021-09-22T13:01:02.519063Z",
     "iopub.status.idle": "2021-09-22T13:01:02.544553Z",
     "shell.execute_reply": "2021-09-22T13:01:02.544066Z",
     "shell.execute_reply.started": "2021-09-12T04:45:24.54331Z"
    },
    "papermill": {
     "duration": 0.062174,
     "end_time": "2021-09-22T13:01:02.544720",
     "exception": false,
     "start_time": "2021-09-22T13:01:02.482546",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# replace by order sum (tau)\n",
    "train['size_tau'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique'] )\n",
    "test['size_tau'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique'] )\n",
    "#train['size_tau_450'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_450'] )\n",
    "#test['size_tau_450'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_450'] )\n",
    "train['size_tau_400'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_400'] )\n",
    "test['size_tau_400'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_400'] )\n",
    "train['size_tau_300'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_300'] )\n",
    "test['size_tau_300'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_300'] )\n",
    "#train['size_tau_150'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_150'] )\n",
    "#test['size_tau_150'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_150'] )\n",
    "train['size_tau_200'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_200'] )\n",
    "test['size_tau_200'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_200'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51bf3b83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-22T13:01:02.601467Z",
     "iopub.status.busy": "2021-09-22T13:01:02.600429Z",
     "iopub.status.idle": "2021-09-22T13:01:02.632133Z",
     "shell.execute_reply": "2021-09-22T13:01:02.631491Z",
     "shell.execute_reply.started": "2021-09-12T04:45:24.576462Z"
    },
    "papermill": {
     "duration": 0.062786,
     "end_time": "2021-09-22T13:01:02.632299",
     "exception": false,
     "start_time": "2021-09-22T13:01:02.569513",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train['size_tau2'] = np.sqrt( 1/ train['trade_order_count_sum'] )\n",
    "test['size_tau2'] = np.sqrt( 1/ test['trade_order_count_sum'] )\n",
    "#train['size_tau2_450'] = np.sqrt( 0.25/ train['trade_order_count_sum'] )\n",
    "#test['size_tau2_450'] = np.sqrt( 0.25/ test['trade_order_count_sum'] )\n",
    "train['size_tau2_400'] = np.sqrt( 0.33/ train['trade_order_count_sum'] )\n",
    "test['size_tau2_400'] = np.sqrt( 0.33/ test['trade_order_count_sum'] )\n",
    "train['size_tau2_300'] = np.sqrt( 0.5/ train['trade_order_count_sum'] )\n",
    "test['size_tau2_300'] = np.sqrt( 0.5/ test['trade_order_count_sum'] )\n",
    "#train['size_tau2_150'] = np.sqrt( 0.75/ train['trade_order_count_sum'] )\n",
    "#test['size_tau2_150'] = np.sqrt( 0.75/ test['trade_order_count_sum'] )\n",
    "train['size_tau2_200'] = np.sqrt( 0.66/ train['trade_order_count_sum'] )\n",
    "test['size_tau2_200'] = np.sqrt( 0.66/ test['trade_order_count_sum'] )\n",
    "\n",
    "# delta tau\n",
    "train['size_tau2_d'] = train['size_tau2_400'] - train['size_tau2']\n",
    "test['size_tau2_d'] = test['size_tau2_400'] - test['size_tau2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f84018ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-22T13:01:02.694365Z",
     "iopub.status.busy": "2021-09-22T13:01:02.693379Z",
     "iopub.status.idle": "2021-09-22T13:01:02.697345Z",
     "shell.execute_reply": "2021-09-22T13:01:02.697811Z",
     "shell.execute_reply.started": "2021-09-12T04:45:24.613884Z"
    },
    "papermill": {
     "duration": 0.037869,
     "end_time": "2021-09-22T13:01:02.697994",
     "exception": false,
     "start_time": "2021-09-22T13:01:02.660125",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colNames = [col for col in list(train.columns)\n",
    "            if col not in {\"stock_id\", \"time_id\", \"target\", \"row_id\"}]\n",
    "len(colNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d779309",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-22T13:01:02.759407Z",
     "iopub.status.busy": "2021-09-22T13:01:02.758610Z",
     "iopub.status.idle": "2021-09-22T13:01:04.772247Z",
     "shell.execute_reply": "2021-09-22T13:01:04.772658Z",
     "shell.execute_reply.started": "2021-09-12T04:45:24.625763Z"
    },
    "papermill": {
     "duration": 2.047724,
     "end_time": "2021-09-22T13:01:04.772803",
     "exception": false,
     "start_time": "2021-09-22T13:01:02.725079",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 4 3 1 3 0 1 3 5 1 0 4 3 3 3 3 3 1 3 3 6 0 0 3 6 3 0 3 6 3 6 3 3 0 4 6 3\n",
      " 6 3 3 3 0 3 3 0 4 3 3 3 4 0 6 6 6 1 4 1 3 0 3 3 0 3 0 0 6 4 0 6 4 5 2 6 4\n",
      " 4 3 4 0 6 4 4 3 0 0 4 4 6 6 3 4 0 3 3 3 3 6 0 6 6 0 0 3 0 0 3 3 0 0 3 4 3\n",
      " 4]\n",
      "[5, 10, 22, 23, 29, 36, 44, 48, 56, 66, 69, 72, 73, 76, 87, 94, 95, 102, 109, 112, 113, 115, 116, 120, 122]\n",
      "[3, 6, 9, 18, 61, 63]\n",
      "[81]\n",
      "[0, 2, 4, 7, 13, 14, 15, 16, 17, 19, 20, 26, 28, 30, 32, 34, 35, 39, 41, 42, 43, 46, 47, 51, 52, 53, 64, 67, 68, 70, 85, 93, 100, 103, 104, 105, 107, 114, 118, 119, 123, 125]\n",
      "[1, 11, 37, 50, 55, 62, 75, 78, 83, 84, 86, 89, 90, 96, 97, 101, 124, 126]\n",
      "[8, 80]\n",
      "[21, 27, 31, 33, 38, 40, 58, 59, 60, 74, 77, 82, 88, 98, 99, 108, 110, 111]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# making agg features\n",
    "\n",
    "train_p = pd.read_csv('input/optiver-realized-volatility-prediction/train.csv')\n",
    "train_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n",
    "\n",
    "corr = train_p.corr()\n",
    "\n",
    "ids = corr.index\n",
    "\n",
    "kmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\n",
    "print(kmeans.labels_)\n",
    "\n",
    "l = []\n",
    "for n in range(7):\n",
    "    l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n",
    "    \n",
    "\n",
    "mat = []\n",
    "matTest = []\n",
    "\n",
    "n = 0\n",
    "for ind in l:\n",
    "    print(ind)\n",
    "    newDf = train.loc[train['stock_id'].isin(ind) ]\n",
    "    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
    "    newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
    "    mat.append ( newDf )\n",
    "    \n",
    "    newDf = test.loc[test['stock_id'].isin(ind) ]    \n",
    "    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
    "    newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
    "    matTest.append ( newDf )\n",
    "    \n",
    "    n+=1\n",
    "    \n",
    "mat1 = pd.concat(mat).reset_index()\n",
    "mat1.drop(columns=['target'],inplace=True)\n",
    "\n",
    "mat2 = pd.concat(matTest).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87dadfd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-22T13:01:04.828412Z",
     "iopub.status.busy": "2021-09-22T13:01:04.827477Z",
     "iopub.status.idle": "2021-09-22T13:01:04.971477Z",
     "shell.execute_reply": "2021-09-22T13:01:04.971898Z",
     "shell.execute_reply.started": "2021-09-12T04:45:26.902473Z"
    },
    "papermill": {
     "duration": 0.174774,
     "end_time": "2021-09-22T13:01:04.972059",
     "exception": false,
     "start_time": "2021-09-22T13:01:04.797285",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-59019010fc55>:3: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "  mat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\n",
      "<ipython-input-9-59019010fc55>:7: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "  mat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\n"
     ]
    }
   ],
   "source": [
    "mat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])\n",
    "mat1 = mat1.pivot(index='time_id', columns='stock_id')\n",
    "mat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\n",
    "mat1.reset_index(inplace=True)\n",
    "\n",
    "mat2 = mat2.pivot(index='time_id', columns='stock_id')\n",
    "mat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\n",
    "mat2.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36b69bb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-22T13:01:05.029841Z",
     "iopub.status.busy": "2021-09-22T13:01:05.029045Z",
     "iopub.status.idle": "2021-09-22T13:01:12.752165Z",
     "shell.execute_reply": "2021-09-22T13:01:12.751419Z",
     "shell.execute_reply.started": "2021-09-12T04:45:27.082624Z"
    },
    "papermill": {
     "duration": 7.756278,
     "end_time": "2021-09-22T13:01:12.752413",
     "exception": false,
     "start_time": "2021-09-22T13:01:04.996135",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nnn = ['time_id',\n",
    "     'log_return1_realized_volatility_0c1',\n",
    "     'log_return1_realized_volatility_1c1',     \n",
    "     'log_return1_realized_volatility_3c1',\n",
    "     'log_return1_realized_volatility_4c1',     \n",
    "     'log_return1_realized_volatility_6c1',\n",
    "     'total_volume_sum_0c1',\n",
    "     'total_volume_sum_1c1', \n",
    "     'total_volume_sum_3c1',\n",
    "     'total_volume_sum_4c1', \n",
    "     'total_volume_sum_6c1',\n",
    "     'trade_size_sum_0c1',\n",
    "     'trade_size_sum_1c1', \n",
    "     'trade_size_sum_3c1',\n",
    "     'trade_size_sum_4c1', \n",
    "     'trade_size_sum_6c1',\n",
    "     'trade_order_count_sum_0c1',\n",
    "     'trade_order_count_sum_1c1',\n",
    "     'trade_order_count_sum_3c1',\n",
    "     'trade_order_count_sum_4c1',\n",
    "     'trade_order_count_sum_6c1',      \n",
    "     'price_spread_sum_0c1',\n",
    "     'price_spread_sum_1c1',\n",
    "     'price_spread_sum_3c1',\n",
    "     'price_spread_sum_4c1',\n",
    "     'price_spread_sum_6c1',   \n",
    "     'bid_spread_sum_0c1',\n",
    "     'bid_spread_sum_1c1',\n",
    "     'bid_spread_sum_3c1',\n",
    "     'bid_spread_sum_4c1',\n",
    "     'bid_spread_sum_6c1',       \n",
    "     'ask_spread_sum_0c1',\n",
    "     'ask_spread_sum_1c1',\n",
    "     'ask_spread_sum_3c1',\n",
    "     'ask_spread_sum_4c1',\n",
    "     'ask_spread_sum_6c1',   \n",
    "     'volume_imbalance_sum_0c1',\n",
    "     'volume_imbalance_sum_1c1',\n",
    "     'volume_imbalance_sum_3c1',\n",
    "     'volume_imbalance_sum_4c1',\n",
    "     'volume_imbalance_sum_6c1',       \n",
    "     'bid_ask_spread_sum_0c1',\n",
    "     'bid_ask_spread_sum_1c1',\n",
    "     'bid_ask_spread_sum_3c1',\n",
    "     'bid_ask_spread_sum_4c1',\n",
    "     'bid_ask_spread_sum_6c1',\n",
    "     'size_tau2_0c1',\n",
    "     'size_tau2_1c1',\n",
    "     'size_tau2_3c1',\n",
    "     'size_tau2_4c1',\n",
    "     'size_tau2_6c1'] \n",
    "train = pd.merge(train,mat1[nnn],how='left',on='time_id')\n",
    "test = pd.merge(test,mat2[nnn],how='left',on='time_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e2c29d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-22T13:01:12.918021Z",
     "iopub.status.busy": "2021-09-22T13:01:12.916739Z",
     "iopub.status.idle": "2021-09-22T13:01:12.920411Z",
     "shell.execute_reply": "2021-09-22T13:01:12.920883Z",
     "shell.execute_reply.started": "2021-09-12T04:45:35.376783Z"
    },
    "papermill": {
     "duration": 0.141972,
     "end_time": "2021-09-22T13:01:12.921049",
     "exception": false,
     "start_time": "2021-09-22T13:01:12.779077",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del mat1,mat2\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f553d5f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-22T13:01:12.987466Z",
     "iopub.status.busy": "2021-09-22T13:01:12.985548Z",
     "iopub.status.idle": "2021-09-22T13:19:14.209715Z",
     "shell.execute_reply": "2021-09-22T13:19:14.210562Z"
    },
    "papermill": {
     "duration": 1081.263497,
     "end_time": "2021-09-22T13:19:14.210862",
     "exception": false,
     "start_time": "2021-09-22T13:01:12.947365",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "\n",
    "seed0=2021\n",
    "params0 = {\n",
    "    'objective': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'max_depth': -1,\n",
    "    'max_bin':100,\n",
    "    'min_data_in_leaf':500,\n",
    "    'learning_rate': 0.05,\n",
    "    'subsample': 0.72,\n",
    "    'subsample_freq': 4,\n",
    "    'feature_fraction': 0.5,\n",
    "    'lambda_l1': 0.5,\n",
    "    'lambda_l2': 1.0,\n",
    "    'categorical_column':[0],\n",
    "    'seed':seed0,\n",
    "    'feature_fraction_seed': seed0,\n",
    "    'bagging_seed': seed0,\n",
    "    'drop_seed': seed0,\n",
    "    'data_random_seed': seed0,\n",
    "    'n_jobs':-1,\n",
    "    'verbose': -1}\n",
    "seed1=42\n",
    "params1 = {\n",
    "        'learning_rate': 0.1,        \n",
    "        'lambda_l1': 2,\n",
    "        'lambda_l2': 7,\n",
    "        'num_leaves': 800,\n",
    "        'min_sum_hessian_in_leaf': 20,\n",
    "        'feature_fraction': 0.8,\n",
    "        'feature_fraction_bynode': 0.8,\n",
    "        'bagging_fraction': 0.9,\n",
    "        'bagging_freq': 42,\n",
    "        'min_data_in_leaf': 700,\n",
    "        'max_depth': 4,\n",
    "        'categorical_column':[0],\n",
    "        'seed': seed1,\n",
    "        'feature_fraction_seed': seed1,\n",
    "        'bagging_seed': seed1,\n",
    "        'drop_seed': seed1,\n",
    "        'data_random_seed': seed1,\n",
    "        'objective': 'rmse',\n",
    "        'boosting': 'gbdt',\n",
    "        'verbosity': -1,\n",
    "        'n_jobs':-1,\n",
    "    }\n",
    "# Function to early stop with root mean squared percentage error\n",
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "def feval_rmspe(y_pred, lgb_train):\n",
    "    y_true = lgb_train.get_label()\n",
    "    return 'RMSPE', rmspe(y_true, y_pred), False\n",
    "\n",
    "def train_and_evaluate_lgb(train, test, params):\n",
    "    # Hyperparammeters (just basic)\n",
    "    \n",
    "    features = [col for col in train.columns if col not in {\"time_id\", \"target\", \"row_id\"}]\n",
    "    y = train['target']\n",
    "    # Create out of folds array\n",
    "    oof_predictions = np.zeros(train.shape[0])\n",
    "    # Create test array to store predictions\n",
    "    test_predictions = np.zeros(test.shape[0])\n",
    "    # Create a KFold object\n",
    "    kfold = KFold(n_splits = 5, random_state = 2021, shuffle = True)\n",
    "    # Iterate through each fold\n",
    "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(train)):\n",
    "        print(f'Training fold {fold + 1}')\n",
    "        x_train, x_val = train.iloc[trn_ind], train.iloc[val_ind]\n",
    "        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n",
    "        # Root mean squared percentage error weights\n",
    "        train_weights = 1 / np.square(y_train)\n",
    "        val_weights = 1 / np.square(y_val)\n",
    "        train_dataset = lgb.Dataset(x_train[features], y_train, weight = train_weights)\n",
    "        val_dataset = lgb.Dataset(x_val[features], y_val, weight = val_weights)\n",
    "        model = lgb.train(params = params,\n",
    "                          num_boost_round=1400,\n",
    "                          train_set = train_dataset, \n",
    "                          valid_sets = [train_dataset, val_dataset], \n",
    "                          verbose_eval = 250,\n",
    "                          early_stopping_rounds=30,\n",
    "                          feval = feval_rmspe)\n",
    "        # Add predictions to the out of folds array\n",
    "        oof_predictions[val_ind] = model.predict(x_val[features])\n",
    "        # Predict the test set\n",
    "        test_predictions += model.predict(test[features]) / 5\n",
    "    rmspe_score = rmspe(y, oof_predictions)\n",
    "    print(f'Our out of folds RMSPE is {rmspe_score}')\n",
    "    lgb.plot_importance(model,max_num_features=20)\n",
    "    # Return test predictions\n",
    "    return test_predictions\n",
    "# Traing and evaluate\n",
    "predictions_lgb1= train_and_evaluate_lgb(train, test,params0)\n",
    "#test['target'] = predictions_lgb\n",
    "#test[['row_id', 'target']].to_csv('submission.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c5ee636",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-22T13:19:14.343825Z",
     "iopub.status.busy": "2021-09-22T13:19:14.343025Z",
     "iopub.status.idle": "2021-09-22T13:37:50.971290Z",
     "shell.execute_reply": "2021-09-22T13:37:50.972344Z"
    },
    "papermill": {
     "duration": 1116.699571,
     "end_time": "2021-09-22T13:37:50.972558",
     "exception": false,
     "start_time": "2021-09-22T13:19:14.272987",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[250]\ttraining's rmse: 0.000428203\ttraining's RMSPE: 0.198249\tvalid_1's rmse: 0.000444544\tvalid_1's RMSPE: 0.205445\n",
      "[500]\ttraining's rmse: 0.000406459\ttraining's RMSPE: 0.188182\tvalid_1's rmse: 0.000429863\tvalid_1's RMSPE: 0.19866\n",
      "[750]\ttraining's rmse: 0.000393027\ttraining's RMSPE: 0.181963\tvalid_1's rmse: 0.000422827\tvalid_1's RMSPE: 0.195409\n",
      "[1000]\ttraining's rmse: 0.000382694\ttraining's RMSPE: 0.177179\tvalid_1's rmse: 0.000417773\tvalid_1's RMSPE: 0.193073\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1200]\ttraining's rmse: 0.000376045\ttraining's RMSPE: 0.174101\tvalid_1's rmse: 0.000415398\tvalid_1's RMSPE: 0.191975\n",
      "Training fold 2\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[250]\ttraining's rmse: 0.000429315\ttraining's RMSPE: 0.198558\tvalid_1's rmse: 0.000463175\tvalid_1's RMSPE: 0.214945\n",
      "[500]\ttraining's rmse: 0.000407322\ttraining's RMSPE: 0.188386\tvalid_1's rmse: 0.000447908\tvalid_1's RMSPE: 0.20786\n",
      "[750]\ttraining's rmse: 0.000394141\ttraining's RMSPE: 0.18229\tvalid_1's rmse: 0.000439609\tvalid_1's RMSPE: 0.204009\n",
      "[1000]\ttraining's rmse: 0.000383843\ttraining's RMSPE: 0.177527\tvalid_1's rmse: 0.000434419\tvalid_1's RMSPE: 0.2016\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1200]\ttraining's rmse: 0.000377397\ttraining's RMSPE: 0.174546\tvalid_1's rmse: 0.000432726\tvalid_1's RMSPE: 0.200814\n",
      "Training fold 3\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[250]\ttraining's rmse: 0.000428287\ttraining's RMSPE: 0.198307\tvalid_1's rmse: 0.000440318\tvalid_1's RMSPE: 0.203413\n",
      "[500]\ttraining's rmse: 0.000406561\ttraining's RMSPE: 0.188247\tvalid_1's rmse: 0.000425659\tvalid_1's RMSPE: 0.196642\n",
      "[750]\ttraining's rmse: 0.000392988\ttraining's RMSPE: 0.181963\tvalid_1's rmse: 0.000417953\tvalid_1's RMSPE: 0.193081\n",
      "[1000]\ttraining's rmse: 0.000382979\ttraining's RMSPE: 0.177328\tvalid_1's rmse: 0.000413454\tvalid_1's RMSPE: 0.191003\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1200]\ttraining's rmse: 0.000376398\ttraining's RMSPE: 0.174281\tvalid_1's rmse: 0.000410941\tvalid_1's RMSPE: 0.189842\n",
      "Training fold 4\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[250]\ttraining's rmse: 0.000428504\ttraining's RMSPE: 0.198068\tvalid_1's rmse: 0.000443053\tvalid_1's RMSPE: 0.206077\n",
      "[500]\ttraining's rmse: 0.00040637\ttraining's RMSPE: 0.187837\tvalid_1's rmse: 0.000428895\tvalid_1's RMSPE: 0.199492\n",
      "[750]\ttraining's rmse: 0.000392952\ttraining's RMSPE: 0.181635\tvalid_1's rmse: 0.000422792\tvalid_1's RMSPE: 0.196653\n",
      "[1000]\ttraining's rmse: 0.000383079\ttraining's RMSPE: 0.177071\tvalid_1's rmse: 0.000418812\tvalid_1's RMSPE: 0.194802\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1200]\ttraining's rmse: 0.000376725\ttraining's RMSPE: 0.174134\tvalid_1's rmse: 0.000417047\tvalid_1's RMSPE: 0.193981\n",
      "Training fold 5\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[250]\ttraining's rmse: 0.000427492\ttraining's RMSPE: 0.19807\tvalid_1's rmse: 0.000448381\tvalid_1's RMSPE: 0.206585\n",
      "[500]\ttraining's rmse: 0.000406021\ttraining's RMSPE: 0.188122\tvalid_1's rmse: 0.000433827\tvalid_1's RMSPE: 0.19988\n",
      "[750]\ttraining's rmse: 0.00039266\ttraining's RMSPE: 0.181931\tvalid_1's rmse: 0.000426841\tvalid_1's RMSPE: 0.196661\n",
      "[1000]\ttraining's rmse: 0.000382493\ttraining's RMSPE: 0.17722\tvalid_1's rmse: 0.000422981\tvalid_1's RMSPE: 0.194882\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1200]\ttraining's rmse: 0.000376161\ttraining's RMSPE: 0.174287\tvalid_1's rmse: 0.000420993\tvalid_1's RMSPE: 0.193967\n",
      "Our out of folds RMSPE is 0.19415068895379492\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "\n",
    "seed0=1111\n",
    "params111 = {\n",
    "    'objective': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'max_depth': -1,\n",
    "    'max_bin':100,\n",
    "    'min_data_in_leaf':500,\n",
    "    'learning_rate': 0.05,\n",
    "    'subsample': 0.72,\n",
    "    'subsample_freq': 4,\n",
    "    'feature_fraction': 0.5,\n",
    "    'lambda_l1': 0.5,\n",
    "    'lambda_l2': 1.0,\n",
    "    'categorical_column':[0],\n",
    "    'seed':seed0,\n",
    "    'feature_fraction_seed': seed0,\n",
    "    'bagging_seed': seed0,\n",
    "    'drop_seed': seed0,\n",
    "    'data_random_seed': seed0,\n",
    "    'n_jobs':-1,\n",
    "    'verbose': -1}\n",
    "seed1=42\n",
    "params1 = {\n",
    "        'learning_rate': 0.1,        \n",
    "        'lambda_l1': 2,\n",
    "        'lambda_l2': 7,\n",
    "        'num_leaves': 800,\n",
    "        'min_sum_hessian_in_leaf': 20,\n",
    "        'feature_fraction': 0.8,\n",
    "        'feature_fraction_bynode': 0.8,\n",
    "        'bagging_fraction': 0.9,\n",
    "        'bagging_freq': 42,\n",
    "        'min_data_in_leaf': 700,\n",
    "        'max_depth': 4,\n",
    "        'categorical_column':[0],\n",
    "        'seed': seed1,\n",
    "        'feature_fraction_seed': seed1,\n",
    "        'bagging_seed': seed1,\n",
    "        'drop_seed': seed1,\n",
    "        'data_random_seed': seed1,\n",
    "        'objective': 'rmse',\n",
    "        'boosting': 'gbdt',\n",
    "        'verbosity': -1,\n",
    "        'n_jobs':-1,\n",
    "    }\n",
    "# Function to early stop with root mean squared percentage error\n",
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "def feval_rmspe(y_pred, lgb_train):\n",
    "    y_true = lgb_train.get_label()\n",
    "    return 'RMSPE', rmspe(y_true, y_pred), False\n",
    "\n",
    "def train_and_evaluate_lgb(train, test, params):\n",
    "    # Hyperparammeters (just basic)\n",
    "    \n",
    "    features = [col for col in train.columns if col not in {\"time_id\", \"target\", \"row_id\"}]\n",
    "    y = train['target']\n",
    "    # Create out of folds array\n",
    "    oof_predictions = np.zeros(train.shape[0])\n",
    "    # Create test array to store predictions\n",
    "    test_predictions = np.zeros(test.shape[0])\n",
    "    # Create a KFold object\n",
    "    kfold = KFold(n_splits = 5, random_state = 1111, shuffle = True)\n",
    "    # Iterate through each fold\n",
    "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(train)):\n",
    "        print(f'Training fold {fold + 1}')\n",
    "        x_train, x_val = train.iloc[trn_ind], train.iloc[val_ind]\n",
    "        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n",
    "        # Root mean squared percentage error weights\n",
    "        train_weights = 1 / np.square(y_train)\n",
    "        val_weights = 1 / np.square(y_val)\n",
    "        train_dataset = lgb.Dataset(x_train[features], y_train, weight = train_weights)\n",
    "        val_dataset = lgb.Dataset(x_val[features], y_val, weight = val_weights)\n",
    "        model = lgb.train(params = params,\n",
    "                          num_boost_round=1200,\n",
    "                          train_set = train_dataset, \n",
    "                          valid_sets = [train_dataset, val_dataset], \n",
    "                          verbose_eval = 250,\n",
    "                          early_stopping_rounds=50,\n",
    "                          feval = feval_rmspe)\n",
    "        # Add predictions to the out of folds array\n",
    "        oof_predictions[val_ind] = model.predict(x_val[features])\n",
    "        # Predict the test set\n",
    "        test_predictions += model.predict(test[features]) / 5\n",
    "    rmspe_score = rmspe(y, oof_predictions)\n",
    "    print(f'Our out of folds RMSPE is {rmspe_score}')\n",
    "    lgb.plot_importance(model,max_num_features=20)\n",
    "    # Return test predictions\n",
    "    return test_predictions\n",
    "# Traing and evaluate\n",
    "predictions_lgb2= train_and_evaluate_lgb(train, test,params111)\n",
    "#test['target'] = predictions_lgb\n",
    "#test[['row_id', 'target']].to_csv('submission.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9cc9800",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-22T13:37:51.144686Z",
     "iopub.status.busy": "2021-09-22T13:37:51.143839Z",
     "iopub.status.idle": "2021-09-22T13:37:51.148820Z",
     "shell.execute_reply": "2021-09-22T13:37:51.149447Z"
    },
    "papermill": {
     "duration": 0.094141,
     "end_time": "2021-09-22T13:37:51.149633",
     "exception": false,
     "start_time": "2021-09-22T13:37:51.055492",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.00186894, 0.00190289, 0.00190289]),\n",
       " array([0.00165038, 0.00162601, 0.00162601]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_lgb2,predictions_lgb1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5beb9ddd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-22T13:37:51.279502Z",
     "iopub.status.busy": "2021-09-22T13:37:51.278861Z",
     "iopub.status.idle": "2021-09-22T13:37:55.419729Z",
     "shell.execute_reply": "2021-09-22T13:37:55.418854Z"
    },
    "papermill": {
     "duration": 4.192912,
     "end_time": "2021-09-22T13:37:55.419864",
     "exception": false,
     "start_time": "2021-09-22T13:37:51.226952",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-dc39976be1c7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_seed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "\n",
    "from numpy.random import seed\n",
    "seed(42)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "def root_mean_squared_per_error(y_true, y_pred):\n",
    "         return K.sqrt(K.mean(K.square( (y_true - y_pred)/ y_true )))\n",
    "    \n",
    "es = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=20, verbose=0,\n",
    "    mode='min',restore_best_weights=True)\n",
    "\n",
    "plateau = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.2, patience=7, verbose=0,\n",
    "    mode='min')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b70fa7c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-22T13:37:55.526303Z",
     "iopub.status.busy": "2021-09-22T13:37:55.525751Z",
     "iopub.status.idle": "2021-09-22T13:38:03.876534Z",
     "shell.execute_reply": "2021-09-22T13:38:03.875586Z"
    },
    "papermill": {
     "duration": 8.410314,
     "end_time": "2021-09-22T13:38:03.876683",
     "exception": false,
     "start_time": "2021-09-22T13:37:55.466369",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# kfold based on the knn++ algorithm\n",
    "\n",
    "out_train = pd.read_csv('input/optiver-realized-volatility-prediction/train.csv')\n",
    "out_train = out_train.pivot(index='time_id', columns='stock_id', values='target')\n",
    "\n",
    "#out_train[out_train.isna().any(axis=1)]\n",
    "out_train = out_train.fillna(out_train.mean())\n",
    "out_train.head()\n",
    "\n",
    "# code to add the just the read data after first execution\n",
    "\n",
    "# data separation based on knn ++\n",
    "nfolds = 5 # number of folds\n",
    "index = []\n",
    "totDist = []\n",
    "values = []\n",
    "# generates a matriz with the values of \n",
    "mat = out_train.values\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "mat = scaler.fit_transform(mat)\n",
    "\n",
    "nind = int(mat.shape[0]/nfolds) # number of individuals\n",
    "\n",
    "# adds index in the last column\n",
    "mat = np.c_[mat,np.arange(mat.shape[0])]\n",
    "\n",
    "\n",
    "lineNumber = np.random.choice(np.array(mat.shape[0]), size=nfolds, replace=False)\n",
    "\n",
    "lineNumber = np.sort(lineNumber)[::-1]\n",
    "\n",
    "for n in range(nfolds):\n",
    "    totDist.append(np.zeros(mat.shape[0]-nfolds))\n",
    "\n",
    "# saves index\n",
    "for n in range(nfolds):\n",
    "    \n",
    "    values.append([lineNumber[n]])    \n",
    "\n",
    "\n",
    "s=[]\n",
    "for n in range(nfolds):\n",
    "    s.append(mat[lineNumber[n],:])\n",
    "    \n",
    "    mat = np.delete(mat, obj=lineNumber[n], axis=0)\n",
    "\n",
    "for n in range(nind-1):    \n",
    "\n",
    "    luck = np.random.uniform(0,1,nfolds)\n",
    "    \n",
    "    for cycle in range(nfolds):\n",
    "         # saves the values of index           \n",
    "\n",
    "        s[cycle] = np.matlib.repmat(s[cycle], mat.shape[0], 1)\n",
    "\n",
    "        sumDist = np.sum( (mat[:,:-1] - s[cycle][:,:-1])**2 , axis=1)   \n",
    "        totDist[cycle] += sumDist        \n",
    "                \n",
    "        # probabilities\n",
    "        f = totDist[cycle]/np.sum(totDist[cycle]) # normalizing the totdist\n",
    "        j = 0\n",
    "        kn = 0\n",
    "        for val in f:\n",
    "            j += val        \n",
    "            if (j > luck[cycle]): # the column was selected\n",
    "                break\n",
    "            kn +=1\n",
    "        lineNumber[cycle] = kn\n",
    "        \n",
    "        # delete line of the value added    \n",
    "        for n_iter in range(nfolds):\n",
    "            \n",
    "            totDist[n_iter] = np.delete(totDist[n_iter],obj=lineNumber[cycle], axis=0)\n",
    "            j= 0\n",
    "        \n",
    "        s[cycle] = mat[lineNumber[cycle],:]\n",
    "        values[cycle].append(int(mat[lineNumber[cycle],-1]))\n",
    "        mat = np.delete(mat, obj=lineNumber[cycle], axis=0)\n",
    "\n",
    "\n",
    "for n_mod in range(nfolds):\n",
    "    values[n_mod] = out_train.index[values[n_mod]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72371af4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-22T13:38:03.984255Z",
     "iopub.status.busy": "2021-09-22T13:38:03.982759Z",
     "iopub.status.idle": "2021-09-22T13:38:27.431965Z",
     "shell.execute_reply": "2021-09-22T13:38:27.431434Z"
    },
    "papermill": {
     "duration": 23.505567,
     "end_time": "2021-09-22T13:38:27.432100",
     "exception": false,
     "start_time": "2021-09-22T13:38:03.926533",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#colNames.remove('row_id')\n",
    "train.replace([np.inf, -np.inf], np.nan,inplace=True)\n",
    "test.replace([np.inf, -np.inf], np.nan,inplace=True)\n",
    "qt_train = []\n",
    "train_nn=train[colNames].copy()\n",
    "test_nn=test[colNames].copy()\n",
    "for col in colNames:\n",
    "    #print(col)\n",
    "    qt = QuantileTransformer(random_state=21,n_quantiles=2000, output_distribution='normal')\n",
    "    train_nn[col] = qt.fit_transform(train_nn[[col]])\n",
    "    test_nn[col] = qt.transform(test_nn[[col]])    \n",
    "    qt_train.append(qt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17df8005",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-22T13:38:27.532564Z",
     "iopub.status.busy": "2021-09-22T13:38:27.531465Z",
     "iopub.status.idle": "2021-09-22T13:38:27.538706Z",
     "shell.execute_reply": "2021-09-22T13:38:27.538277Z"
    },
    "papermill": {
     "duration": 0.059939,
     "end_time": "2021-09-22T13:38:27.538811",
     "exception": false,
     "start_time": "2021-09-22T13:38:27.478872",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_nn[['stock_id','time_id','target']]=train[['stock_id','time_id','target']]\n",
    "test_nn[['stock_id','time_id']]=test[['stock_id','time_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59f5efaf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-22T13:38:27.642559Z",
     "iopub.status.busy": "2021-09-22T13:38:27.640087Z",
     "iopub.status.idle": "2021-09-22T13:38:29.093200Z",
     "shell.execute_reply": "2021-09-22T13:38:29.093577Z"
    },
    "papermill": {
     "duration": 1.508758,
     "end_time": "2021-09-22T13:38:29.093735",
     "exception": false,
     "start_time": "2021-09-22T13:38:27.584977",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 4 3 1 3 0 1 3 5 1 0 4 3 3 3 3 3 1 3 3 6 0 0 3 6 3 0 3 6 3 6 3 3 0 4 6 3\n",
      " 6 3 3 3 0 3 3 0 4 3 3 3 4 0 6 6 6 1 4 1 3 0 3 3 0 3 0 0 6 4 0 6 4 5 2 6 4\n",
      " 4 3 4 0 6 4 4 3 0 0 4 4 6 6 3 4 0 3 3 3 3 6 0 6 6 0 0 3 0 0 3 3 0 0 3 4 3\n",
      " 4]\n",
      "[5, 10, 22, 23, 29, 36, 44, 48, 56, 66, 69, 72, 73, 76, 87, 94, 95, 102, 109, 112, 113, 115, 116, 120, 122]\n",
      "[3, 6, 9, 18, 61, 63]\n",
      "[81]\n",
      "[0, 2, 4, 7, 13, 14, 15, 16, 17, 19, 20, 26, 28, 30, 32, 34, 35, 39, 41, 42, 43, 46, 47, 51, 52, 53, 64, 67, 68, 70, 85, 93, 100, 103, 104, 105, 107, 114, 118, 119, 123, 125]\n",
      "[1, 11, 37, 50, 55, 62, 75, 78, 83, 84, 86, 89, 90, 96, 97, 101, 124, 126]\n",
      "[8, 80]\n",
      "[21, 27, 31, 33, 38, 40, 58, 59, 60, 74, 77, 82, 88, 98, 99, 108, 110, 111]\n"
     ]
    }
   ],
   "source": [
    "# making agg features\n",
    "from sklearn.cluster import KMeans\n",
    "train_p = pd.read_csv('input/optiver-realized-volatility-prediction/train.csv')\n",
    "train_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n",
    "\n",
    "corr = train_p.corr()\n",
    "\n",
    "ids = corr.index\n",
    "\n",
    "kmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\n",
    "print(kmeans.labels_)\n",
    "\n",
    "l = []\n",
    "for n in range(7):\n",
    "    l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n",
    "    \n",
    "\n",
    "mat = []\n",
    "matTest = []\n",
    "\n",
    "n = 0\n",
    "for ind in l:\n",
    "    print(ind)\n",
    "    newDf = train_nn.loc[train_nn['stock_id'].isin(ind) ]\n",
    "    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
    "    newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
    "    mat.append ( newDf )\n",
    "    \n",
    "    newDf = test_nn.loc[test_nn['stock_id'].isin(ind) ]    \n",
    "    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
    "    newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
    "    matTest.append ( newDf )\n",
    "    \n",
    "    n+=1\n",
    "    \n",
    "mat1 = pd.concat(mat).reset_index()\n",
    "mat1.drop(columns=['target'],inplace=True)\n",
    "\n",
    "mat2 = pd.concat(matTest).reset_index()\n",
    "mat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35cc0e59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-22T13:38:29.194930Z",
     "iopub.status.busy": "2021-09-22T13:38:29.194418Z",
     "iopub.status.idle": "2021-09-22T13:38:29.198173Z",
     "shell.execute_reply": "2021-09-22T13:38:29.197734Z"
    },
    "papermill": {
     "duration": 0.056239,
     "end_time": "2021-09-22T13:38:29.198275",
     "exception": false,
     "start_time": "2021-09-22T13:38:29.142036",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nnn = ['time_id',\n",
    "     'log_return1_realized_volatility_0c1',\n",
    "     'log_return1_realized_volatility_1c1',     \n",
    "     'log_return1_realized_volatility_3c1',\n",
    "     'log_return1_realized_volatility_4c1',     \n",
    "     'log_return1_realized_volatility_6c1',\n",
    "     'total_volume_sum_0c1',\n",
    "     'total_volume_sum_1c1', \n",
    "     'total_volume_sum_3c1',\n",
    "     'total_volume_sum_4c1', \n",
    "     'total_volume_sum_6c1',\n",
    "     'trade_size_sum_0c1',\n",
    "     'trade_size_sum_1c1', \n",
    "     'trade_size_sum_3c1',\n",
    "     'trade_size_sum_4c1', \n",
    "     'trade_size_sum_6c1',\n",
    "     'trade_order_count_sum_0c1',\n",
    "     'trade_order_count_sum_1c1',\n",
    "     'trade_order_count_sum_3c1',\n",
    "     'trade_order_count_sum_4c1',\n",
    "     'trade_order_count_sum_6c1',      \n",
    "     'price_spread_sum_0c1',\n",
    "     'price_spread_sum_1c1',\n",
    "     'price_spread_sum_3c1',\n",
    "     'price_spread_sum_4c1',\n",
    "     'price_spread_sum_6c1',   \n",
    "     'bid_spread_sum_0c1',\n",
    "     'bid_spread_sum_1c1',\n",
    "     'bid_spread_sum_3c1',\n",
    "     'bid_spread_sum_4c1',\n",
    "     'bid_spread_sum_6c1',       \n",
    "     'ask_spread_sum_0c1',\n",
    "     'ask_spread_sum_1c1',\n",
    "     'ask_spread_sum_3c1',\n",
    "     'ask_spread_sum_4c1',\n",
    "     'ask_spread_sum_6c1',   \n",
    "     'volume_imbalance_sum_0c1',\n",
    "     'volume_imbalance_sum_1c1',\n",
    "     'volume_imbalance_sum_3c1',\n",
    "     'volume_imbalance_sum_4c1',\n",
    "     'volume_imbalance_sum_6c1',       \n",
    "     'bid_ask_spread_sum_0c1',\n",
    "     'bid_ask_spread_sum_1c1',\n",
    "     'bid_ask_spread_sum_3c1',\n",
    "     'bid_ask_spread_sum_4c1',\n",
    "     'bid_ask_spread_sum_6c1',\n",
    "     'size_tau2_0c1',\n",
    "     'size_tau2_1c1',\n",
    "     'size_tau2_3c1',\n",
    "     'size_tau2_4c1',\n",
    "     'size_tau2_6c1'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d41471a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-22T13:38:29.299966Z",
     "iopub.status.busy": "2021-09-22T13:38:29.298509Z",
     "iopub.status.idle": "2021-09-22T13:38:29.442502Z",
     "shell.execute_reply": "2021-09-22T13:38:29.442879Z"
    },
    "papermill": {
     "duration": 0.197355,
     "end_time": "2021-09-22T13:38:29.443043",
     "exception": false,
     "start_time": "2021-09-22T13:38:29.245688",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-79e19df2b5ab>:2: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "  mat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\n",
      "<ipython-input-19-79e19df2b5ab>:6: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "  mat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\n"
     ]
    }
   ],
   "source": [
    "mat1 = mat1.pivot(index='time_id', columns='stock_id')\n",
    "mat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\n",
    "mat1.reset_index(inplace=True)\n",
    "\n",
    "mat2 = mat2.pivot(index='time_id', columns='stock_id')\n",
    "mat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\n",
    "mat2.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a436dd2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-22T13:38:29.546790Z",
     "iopub.status.busy": "2021-09-22T13:38:29.546001Z",
     "iopub.status.idle": "2021-09-22T13:38:34.355582Z",
     "shell.execute_reply": "2021-09-22T13:38:34.356033Z"
    },
    "papermill": {
     "duration": 4.864081,
     "end_time": "2021-09-22T13:38:34.356250",
     "exception": false,
     "start_time": "2021-09-22T13:38:29.492169",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "train_nn = pd.merge(train_nn,mat1[nnn],how='left',on='time_id')\n",
    "test_nn = pd.merge(test_nn,mat2[nnn],how='left',on='time_id')\n",
    "\n",
    "train1=train_nn\n",
    "test1=test_nn\n",
    "del mat1,mat2\n",
    "del train,test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4cde7198",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-22T13:38:34.461744Z",
     "iopub.status.busy": "2021-09-22T13:38:34.461207Z",
     "iopub.status.idle": "2021-09-22T13:38:34.474714Z",
     "shell.execute_reply": "2021-09-22T13:38:34.474305Z"
    },
    "papermill": {
     "duration": 0.069775,
     "end_time": "2021-09-22T13:38:34.474825",
     "exception": false,
     "start_time": "2021-09-22T13:38:34.405050",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#https://bignerdranch.com/blog/implementing-swish-activation-function-in-keras/\n",
    "from keras.backend import sigmoid\n",
    "def swish(x, beta = 1):\n",
    "    return (x * sigmoid(beta * x))\n",
    "\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from keras.layers import Activation\n",
    "get_custom_objects().update({'swish': Activation(swish)})\n",
    "\n",
    "hidden_units = (128,64,32)\n",
    "stock_embedding_size = 24\n",
    "\n",
    "cat_data = train_nn['stock_id']\n",
    "\n",
    "def base_model():\n",
    "    \n",
    "    # Each instance will consist of two inputs: a single user id, and a single movie id\n",
    "    stock_id_input = keras.Input(shape=(1,), name='stock_id')\n",
    "    num_input = keras.Input(shape=(244,), name='num_data')\n",
    "\n",
    "\n",
    "    #embedding, flatenning and concatenating\n",
    "    stock_embedded = keras.layers.Embedding(max(cat_data)+1, stock_embedding_size, \n",
    "                                           input_length=1, name='stock_embedding')(stock_id_input)\n",
    "    stock_flattened = keras.layers.Flatten()(stock_embedded)\n",
    "    out = keras.layers.Concatenate()([stock_flattened, num_input])\n",
    "    \n",
    "    # Add one or more hidden layers\n",
    "    for n_hidden in hidden_units:\n",
    "\n",
    "        out = keras.layers.Dense(n_hidden, activation='swish')(out)\n",
    "        \n",
    "\n",
    "    #out = keras.layers.Concatenate()([out, num_input])\n",
    "\n",
    "    # A single output: our predicted rating\n",
    "    out = keras.layers.Dense(1, activation='linear', name='prediction')(out)\n",
    "    \n",
    "    model = keras.Model(\n",
    "    inputs = [stock_id_input, num_input],\n",
    "    outputs = out,\n",
    "    )\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a161c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "train1.to_pickle(\"temp/train3.pkl\")\n",
    "test1.to_pickle(\"temp/test3.pkl\")\n",
    "train_nn.to_pickle(\"temp/train_nn.pkl\")\n",
    "test_nn.to_pickle(\"temp/test_nn.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "321409e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-22T13:38:34.576044Z",
     "iopub.status.busy": "2021-09-22T13:38:34.575364Z",
     "iopub.status.idle": "2021-09-22T13:38:34.577704Z",
     "shell.execute_reply": "2021-09-22T13:38:34.578085Z"
    },
    "papermill": {
     "duration": 0.055165,
     "end_time": "2021-09-22T13:38:34.578213",
     "exception": false,
     "start_time": "2021-09-22T13:38:34.523048",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to calculate the root mean squared percentage error\n",
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "# Function to early stop with root mean squared percentage error\n",
    "def feval_rmspe(y_pred, lgb_train):\n",
    "    y_true = lgb_train.get_label()\n",
    "    return 'RMSPE', rmspe(y_true, y_pred), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c4d70226",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-22T13:38:34.705566Z",
     "iopub.status.busy": "2021-09-22T13:38:34.702799Z",
     "iopub.status.idle": "2021-09-22T13:44:13.145714Z",
     "shell.execute_reply": "2021-09-22T13:44:13.146147Z"
    },
    "papermill": {
     "duration": 338.519594,
     "end_time": "2021-09-22T13:44:13.146305",
     "exception": false,
     "start_time": "2021-09-22T13:38:34.626711",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV 1/5\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 3s 9ms/step - loss: 23.7555 - val_loss: 0.9235\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.8764 - val_loss: 0.8198\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.7276 - val_loss: 0.6526\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.6396 - val_loss: 0.5807\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.6639 - val_loss: 0.7482\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.6174 - val_loss: 0.5198\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.5926 - val_loss: 0.6141\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.5895 - val_loss: 0.6901\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.5606 - val_loss: 0.4756\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.4885 - val_loss: 0.5369\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.4752 - val_loss: 0.3889\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.4177 - val_loss: 0.3883\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.4155 - val_loss: 1.5891\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.7751 - val_loss: 0.2492\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.3538 - val_loss: 0.2188\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2279 - val_loss: 0.2288\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2288 - val_loss: 0.2557\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2423 - val_loss: 0.2270\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2294 - val_loss: 0.2844\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2458 - val_loss: 0.2593\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2459 - val_loss: 0.2717\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2620 - val_loss: 0.2152\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.3227 - val_loss: 0.2235\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2319 - val_loss: 0.3024\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2441 - val_loss: 0.2242\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2414 - val_loss: 0.2298\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2496 - val_loss: 0.2118\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2444 - val_loss: 0.2137\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.4115 - val_loss: 2.5313\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.7436 - val_loss: 0.2479\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2853 - val_loss: 0.2294\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2286 - val_loss: 0.2376\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2357 - val_loss: 0.2274\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2279 - val_loss: 0.2137\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2128 - val_loss: 0.2121\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2127 - val_loss: 0.2126\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2126 - val_loss: 0.2148\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2132 - val_loss: 0.2119\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2121 - val_loss: 0.2124\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2113 - val_loss: 0.2146\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2125 - val_loss: 0.2115\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2113 - val_loss: 0.2139\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2130 - val_loss: 0.2135\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2116 - val_loss: 0.2119\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2112 - val_loss: 0.2149\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2130 - val_loss: 0.2116\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2109 - val_loss: 0.2121\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2107 - val_loss: 0.2164\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2092 - val_loss: 0.2098\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2079 - val_loss: 0.2104\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2090 - val_loss: 0.2108\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2083 - val_loss: 0.2106\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2080 - val_loss: 0.2102\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2081 - val_loss: 0.2098\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2080 - val_loss: 0.2101\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2074 - val_loss: 0.2103\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2063 - val_loss: 0.2097\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2069 - val_loss: 0.2096\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2058 - val_loss: 0.2097\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2070 - val_loss: 0.2097\n",
      "Epoch 61/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2064 - val_loss: 0.2102\n",
      "Epoch 62/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2066 - val_loss: 0.2098\n",
      "Epoch 63/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2068 - val_loss: 0.2096\n",
      "Epoch 64/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2061 - val_loss: 0.2097\n",
      "Epoch 65/1000\n",
      "168/168 [==============================] - 1s 4ms/step - loss: 0.2068 - val_loss: 0.2095\n",
      "Epoch 66/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2066 - val_loss: 0.2095\n",
      "Epoch 67/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2059 - val_loss: 0.2096\n",
      "Epoch 68/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2072 - val_loss: 0.2095\n",
      "Epoch 69/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2056 - val_loss: 0.2096\n",
      "Epoch 70/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2061 - val_loss: 0.2094\n",
      "Epoch 71/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2060 - val_loss: 0.2097\n",
      "Epoch 72/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2074 - val_loss: 0.2096\n",
      "Epoch 73/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2057 - val_loss: 0.2097\n",
      "Epoch 74/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2057 - val_loss: 0.2097\n",
      "Epoch 75/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2067 - val_loss: 0.2097\n",
      "Epoch 76/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2062 - val_loss: 0.2095\n",
      "Epoch 77/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2065 - val_loss: 0.2095\n",
      "Epoch 78/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2063 - val_loss: 0.2096\n",
      "Epoch 79/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2069 - val_loss: 0.2095\n",
      "Epoch 80/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2060 - val_loss: 0.2095\n",
      "Epoch 81/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2067 - val_loss: 0.2095\n",
      "Epoch 82/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2057 - val_loss: 0.2096\n",
      "Epoch 83/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2061 - val_loss: 0.2095\n",
      "Epoch 84/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2062 - val_loss: 0.2095\n",
      "Epoch 85/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2066 - val_loss: 0.2095\n",
      "Epoch 86/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2059 - val_loss: 0.2095\n",
      "Epoch 87/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2059 - val_loss: 0.2095\n",
      "Epoch 88/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2061 - val_loss: 0.2095\n",
      "Epoch 89/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2066 - val_loss: 0.2095\n",
      "Epoch 90/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2060 - val_loss: 0.2095\n",
      "Fold 1 NN: 0.20945\n",
      "CV 2/5\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 2s 6ms/step - loss: 25.6681 - val_loss: 0.9251\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 2.5876 - val_loss: 0.4671\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.6203 - val_loss: 0.9499\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.7524 - val_loss: 0.8264\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.7081 - val_loss: 0.6862\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.6664 - val_loss: 1.0901\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.9228 - val_loss: 1.1157\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.5387 - val_loss: 0.2578\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2988 - val_loss: 0.3039\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.3060 - val_loss: 0.2973\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 1.1527 - val_loss: 2.0891\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.5812 - val_loss: 0.2543\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2489 - val_loss: 0.2785\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2537 - val_loss: 0.2308\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2631 - val_loss: 0.2879\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2603 - val_loss: 0.3016\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2840 - val_loss: 0.3032\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2680 - val_loss: 0.2972\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 1.1310 - val_loss: 0.4523\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.3233 - val_loss: 0.2267\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2365 - val_loss: 0.2567\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2313 - val_loss: 0.2771\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2384 - val_loss: 0.2733\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2384 - val_loss: 0.3255\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2457 - val_loss: 0.2340\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2402 - val_loss: 0.2496\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2529 - val_loss: 0.2397\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2125 - val_loss: 0.2174\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2081 - val_loss: 0.2159\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2070 - val_loss: 0.2159\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2083 - val_loss: 0.2156\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2078 - val_loss: 0.2152\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2080 - val_loss: 0.2170\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2071 - val_loss: 0.2159\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2058 - val_loss: 0.2134\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2064 - val_loss: 0.2203\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2095 - val_loss: 0.2169\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2073 - val_loss: 0.2141\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2067 - val_loss: 0.2146\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2061 - val_loss: 0.2189\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2077 - val_loss: 0.2270\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2095 - val_loss: 0.2159\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2038 - val_loss: 0.2166\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2029 - val_loss: 0.2143\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2025 - val_loss: 0.2128\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2031 - val_loss: 0.2159\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2025 - val_loss: 0.2127\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2028 - val_loss: 0.2136\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2027 - val_loss: 0.2128\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2022 - val_loss: 0.2134\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2023 - val_loss: 0.2127\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2026 - val_loss: 0.2116\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2020 - val_loss: 0.2142\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2033 - val_loss: 0.2136\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2025 - val_loss: 0.2156\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2025 - val_loss: 0.2141\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2028 - val_loss: 0.2126\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2020 - val_loss: 0.2161\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2020 - val_loss: 0.2161\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2008 - val_loss: 0.2136\n",
      "Epoch 61/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2004 - val_loss: 0.2135\n",
      "Epoch 62/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.1998 - val_loss: 0.2120\n",
      "Epoch 63/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2007 - val_loss: 0.2138\n",
      "Epoch 64/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2001 - val_loss: 0.2134\n",
      "Epoch 65/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2004 - val_loss: 0.2134\n",
      "Epoch 66/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2008 - val_loss: 0.2130\n",
      "Epoch 67/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2000 - val_loss: 0.2129\n",
      "Epoch 68/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2002 - val_loss: 0.2128\n",
      "Epoch 69/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.1999 - val_loss: 0.2129\n",
      "Epoch 70/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1998 - val_loss: 0.2126\n",
      "Epoch 71/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.1997 - val_loss: 0.2130\n",
      "Epoch 72/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.1995 - val_loss: 0.2130\n",
      "Fold 2 NN: 0.2116\n",
      "CV 3/5\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 25.1842 - val_loss: 0.8314\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.5856 - val_loss: 0.2902\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.3702 - val_loss: 0.5103\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.3444 - val_loss: 0.4298\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.3448 - val_loss: 0.3819\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 1.3377 - val_loss: 0.2794\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2684 - val_loss: 0.2851\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2643 - val_loss: 0.2381\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.3693 - val_loss: 0.4186\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.3562 - val_loss: 0.2201\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2671 - val_loss: 0.2267\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2632 - val_loss: 0.2431\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2588 - val_loss: 0.2518\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2636 - val_loss: 0.2400\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2829 - val_loss: 0.2440\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2519 - val_loss: 0.2500\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2811 - val_loss: 0.3282\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2226 - val_loss: 0.2142\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2093 - val_loss: 0.2128\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2090 - val_loss: 0.2100\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2080 - val_loss: 0.2126\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2081 - val_loss: 0.2097\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2092 - val_loss: 0.2107\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2080 - val_loss: 0.2102\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2101 - val_loss: 0.2136\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2109 - val_loss: 0.2129\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2117 - val_loss: 0.2099\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2088 - val_loss: 0.2125\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2125 - val_loss: 0.2354\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2085 - val_loss: 0.2102\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2041 - val_loss: 0.2099\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2052 - val_loss: 0.2107\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2054 - val_loss: 0.2088\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2043 - val_loss: 0.2098\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2047 - val_loss: 0.2123\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2042 - val_loss: 0.2090\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2035 - val_loss: 0.2120\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2036 - val_loss: 0.2100\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2034 - val_loss: 0.2102\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2044 - val_loss: 0.2099\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2029 - val_loss: 0.2090\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2040 - val_loss: 0.2089\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2029 - val_loss: 0.2091\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2023 - val_loss: 0.2091\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2029 - val_loss: 0.2090\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2025 - val_loss: 0.2096\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2027 - val_loss: 0.2091\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2023 - val_loss: 0.2090\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2020 - val_loss: 0.2090\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2023 - val_loss: 0.2090\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2021 - val_loss: 0.2088\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2023 - val_loss: 0.2087\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2034 - val_loss: 0.2090\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2019 - val_loss: 0.2088\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2018 - val_loss: 0.2088\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2022 - val_loss: 0.2089\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2025 - val_loss: 0.2088\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2024 - val_loss: 0.2089\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2022 - val_loss: 0.2090\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2024 - val_loss: 0.2089\n",
      "Epoch 61/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2023 - val_loss: 0.2088\n",
      "Epoch 62/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2028 - val_loss: 0.2088\n",
      "Epoch 63/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2025 - val_loss: 0.2087\n",
      "Epoch 64/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2016 - val_loss: 0.2088\n",
      "Epoch 65/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2023 - val_loss: 0.2088\n",
      "Epoch 66/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2023 - val_loss: 0.2089\n",
      "Epoch 67/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2026 - val_loss: 0.2088\n",
      "Epoch 68/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2023 - val_loss: 0.2088\n",
      "Epoch 69/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2026 - val_loss: 0.2088\n",
      "Epoch 70/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2022 - val_loss: 0.2088\n",
      "Epoch 71/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2014 - val_loss: 0.2089\n",
      "Epoch 72/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2023 - val_loss: 0.2088\n",
      "Fold 3 NN: 0.20872\n",
      "CV 4/5\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 20.6863 - val_loss: 0.8231\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.6806 - val_loss: 0.6694\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.7769 - val_loss: 0.6872\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.6959 - val_loss: 1.8726\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 1.1647 - val_loss: 0.6439\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.5495 - val_loss: 0.6028\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.6265 - val_loss: 0.5354\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.8916 - val_loss: 0.2908\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.3007 - val_loss: 0.2360\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2413 - val_loss: 0.2314\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2334 - val_loss: 0.2308\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2548 - val_loss: 0.2337\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2585 - val_loss: 0.3068\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2548 - val_loss: 0.3128\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2611 - val_loss: 0.2420\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2562 - val_loss: 0.2951\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2699 - val_loss: 0.3721\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2664 - val_loss: 0.2459\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2109 - val_loss: 0.2193\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2067 - val_loss: 0.2323\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2080 - val_loss: 0.2159\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2072 - val_loss: 0.2333\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2111 - val_loss: 0.2219\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2078 - val_loss: 0.2210\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2075 - val_loss: 0.2181\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2067 - val_loss: 0.2164\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2083 - val_loss: 0.2232\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2134 - val_loss: 0.2179\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2039 - val_loss: 0.2157\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 1s 4ms/step - loss: 0.2035 - val_loss: 0.2165\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2044 - val_loss: 0.2151\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2041 - val_loss: 0.2153\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2032 - val_loss: 0.2162\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2036 - val_loss: 0.2157\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2032 - val_loss: 0.2171\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2038 - val_loss: 0.2165\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2041 - val_loss: 0.2166\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2035 - val_loss: 0.2164\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2024 - val_loss: 0.2153\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2028 - val_loss: 0.2151\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2026 - val_loss: 0.2153\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2019 - val_loss: 0.2156\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2023 - val_loss: 0.2152\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2021 - val_loss: 0.2157\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2019 - val_loss: 0.2149\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2023 - val_loss: 0.2155\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2019 - val_loss: 0.2158\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2020 - val_loss: 0.2161\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2019 - val_loss: 0.2159\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2020 - val_loss: 0.2154\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2015 - val_loss: 0.2159\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2016 - val_loss: 0.2154\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2019 - val_loss: 0.2154\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2016 - val_loss: 0.2154\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2023 - val_loss: 0.2156\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2014 - val_loss: 0.2155\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2015 - val_loss: 0.2153\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2017 - val_loss: 0.2154\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2012 - val_loss: 0.2155\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2019 - val_loss: 0.2155\n",
      "Epoch 61/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2017 - val_loss: 0.2154\n",
      "Epoch 62/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2010 - val_loss: 0.2153\n",
      "Epoch 63/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2017 - val_loss: 0.2154\n",
      "Epoch 64/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2015 - val_loss: 0.2154\n",
      "Epoch 65/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2011 - val_loss: 0.2154\n",
      "Fold 4 NN: 0.2149\n",
      "CV 5/5\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 2s 7ms/step - loss: 32.7981 - val_loss: 1.6087\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 1.0053 - val_loss: 0.6712\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.6021 - val_loss: 0.3151\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.3338 - val_loss: 0.7693\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.6788 - val_loss: 0.2675\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.3035 - val_loss: 0.2785\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2884 - val_loss: 0.3479\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2943 - val_loss: 0.3006\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2993 - val_loss: 0.2436\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.3325 - val_loss: 12.1446\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 5.2460 - val_loss: 0.2482\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2688 - val_loss: 0.2311\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2395 - val_loss: 0.2864\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2651 - val_loss: 0.2236\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2388 - val_loss: 0.2860\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.4581 - val_loss: 0.4584\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.3999 - val_loss: 0.4167\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.3728 - val_loss: 0.3667\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.3439 - val_loss: 0.3433\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.3102 - val_loss: 0.2293\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.3096 - val_loss: 0.2291\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2147 - val_loss: 0.2172\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2128 - val_loss: 0.2173\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2092 - val_loss: 0.2184\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2120 - val_loss: 0.2188\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2093 - val_loss: 0.2167\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2101 - val_loss: 0.2273\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2095 - val_loss: 0.2179\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2086 - val_loss: 0.2151\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2081 - val_loss: 0.2174\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2074 - val_loss: 0.2181\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2100 - val_loss: 0.2158\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2085 - val_loss: 0.2165\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2088 - val_loss: 0.2168\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2082 - val_loss: 0.2157\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2070 - val_loss: 0.2182\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2045 - val_loss: 0.2146\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2041 - val_loss: 0.2150\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2036 - val_loss: 0.2156\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2040 - val_loss: 0.2148\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2039 - val_loss: 0.2147\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2040 - val_loss: 0.2146\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2038 - val_loss: 0.2148\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2040 - val_loss: 0.2146\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2035 - val_loss: 0.2140\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2031 - val_loss: 0.2143\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2036 - val_loss: 0.2145\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2034 - val_loss: 0.2138\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2038 - val_loss: 0.2143\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2035 - val_loss: 0.2141\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2034 - val_loss: 0.2143\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2030 - val_loss: 0.2142\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2034 - val_loss: 0.2139\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2030 - val_loss: 0.2138\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2042 - val_loss: 0.2146\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2026 - val_loss: 0.2139\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2025 - val_loss: 0.2139\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2037 - val_loss: 0.2138\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2021 - val_loss: 0.2139\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2025 - val_loss: 0.2140\n",
      "Epoch 61/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2027 - val_loss: 0.2138\n",
      "Epoch 62/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2031 - val_loss: 0.2139\n",
      "Epoch 63/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2023 - val_loss: 0.2139\n",
      "Epoch 64/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2022 - val_loss: 0.2139\n",
      "Epoch 65/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2027 - val_loss: 0.2139\n",
      "Epoch 66/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2025 - val_loss: 0.2139\n",
      "Epoch 67/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2027 - val_loss: 0.2139\n",
      "Epoch 68/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2031 - val_loss: 0.2139\n",
      "Epoch 69/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2030 - val_loss: 0.2139\n",
      "Epoch 70/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2024 - val_loss: 0.2138\n",
      "Epoch 71/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2028 - val_loss: 0.2139\n",
      "Epoch 72/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2024 - val_loss: 0.2139\n",
      "Epoch 73/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2041 - val_loss: 0.2140\n",
      "Epoch 74/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2029 - val_loss: 0.2139\n",
      "Fold 5 NN: 0.21377\n"
     ]
    }
   ],
   "source": [
    "target_name='target'\n",
    "scores_folds = {}\n",
    "model_name = 'NN'\n",
    "pred_name = 'pred_{}'.format(model_name)\n",
    "\n",
    "n_folds = 5\n",
    "kf = model_selection.KFold(n_splits=n_folds, shuffle=True, random_state=2020)\n",
    "scores_folds[model_name] = []\n",
    "counter = 1\n",
    "\n",
    "features_to_consider = list(train_nn)\n",
    "\n",
    "features_to_consider.remove('time_id')\n",
    "features_to_consider.remove('target')\n",
    "try:\n",
    "    features_to_consider.remove('pred_NN')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "train_nn[features_to_consider] = train_nn[features_to_consider].fillna(train_nn[features_to_consider].mean())\n",
    "test_nn[features_to_consider] = test_nn[features_to_consider].fillna(train_nn[features_to_consider].mean())\n",
    "\n",
    "train_nn[pred_name] = 0\n",
    "test_nn[target_name] = 0\n",
    "test_predictions_nn = np.zeros(test_nn.shape[0])\n",
    "\n",
    "for n_count in range(n_folds):\n",
    "    print('CV {}/{}'.format(counter, n_folds))\n",
    "    \n",
    "    indexes = np.arange(nfolds).astype(int)    \n",
    "    indexes = np.delete(indexes,obj=n_count, axis=0) \n",
    "    \n",
    "    indexes = np.r_[values[indexes[0]],values[indexes[1]],values[indexes[2]],values[indexes[3]]]\n",
    "    \n",
    "    X_train = train_nn.loc[train_nn.time_id.isin(indexes), features_to_consider]\n",
    "    y_train = train_nn.loc[train_nn.time_id.isin(indexes), target_name]\n",
    "    X_test = train_nn.loc[train_nn.time_id.isin(values[n_count]), features_to_consider]\n",
    "    y_test = train_nn.loc[train_nn.time_id.isin(values[n_count]), target_name]\n",
    "    \n",
    "    #############################################################################################\n",
    "    # NN\n",
    "    #############################################################################################\n",
    "    \n",
    "    model = base_model()\n",
    "    \n",
    "    model.compile(\n",
    "        keras.optimizers.Adam(learning_rate=0.006),\n",
    "        loss=root_mean_squared_per_error\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        features_to_consider.remove('stock_id')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    num_data = X_train[features_to_consider]\n",
    "    \n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))         \n",
    "    num_data = scaler.fit_transform(num_data.values)    \n",
    "    \n",
    "    cat_data = X_train['stock_id']    \n",
    "    target =  y_train\n",
    "    \n",
    "    num_data_test = X_test[features_to_consider]\n",
    "    num_data_test = scaler.transform(num_data_test.values)\n",
    "    cat_data_test = X_test['stock_id']\n",
    "\n",
    "    model.fit([cat_data, num_data], \n",
    "              target,               \n",
    "              batch_size=2048,\n",
    "              epochs=1000,\n",
    "              validation_data=([cat_data_test, num_data_test], y_test),\n",
    "              callbacks=[es, plateau],\n",
    "              validation_batch_size=len(y_test),\n",
    "              shuffle=True,\n",
    "             verbose = 1)\n",
    "\n",
    "    preds = model.predict([cat_data_test, num_data_test]).reshape(1,-1)[0]\n",
    "    \n",
    "    score = round(rmspe(y_true = y_test, y_pred = preds),5)\n",
    "    print('Fold {} {}: {}'.format(counter, model_name, score))\n",
    "    scores_folds[model_name].append(score)\n",
    "    \n",
    "    tt =scaler.transform(test_nn[features_to_consider].values)\n",
    "    #test_nn[target_name] += model.predict([test_nn['stock_id'], tt]).reshape(1,-1)[0].clip(0,1e10)\n",
    "    test_predictions_nn += model.predict([test_nn['stock_id'], tt]).reshape(1,-1)[0].clip(0,1e10)/n_folds\n",
    "    #test[target_name] += model.predict([test['stock_id'], test[features_to_consider]]).reshape(1,-1)[0].clip(0,1e10)\n",
    "       \n",
    "    counter += 1\n",
    "    features_to_consider.append('stock_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "57f44ecd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-22T13:44:16.293662Z",
     "iopub.status.busy": "2021-09-22T13:44:16.292926Z",
     "iopub.status.idle": "2021-09-22T13:44:16.295789Z",
     "shell.execute_reply": "2021-09-22T13:44:16.296213Z"
    },
    "papermill": {
     "duration": 1.679244,
     "end_time": "2021-09-22T13:44:16.296348",
     "exception": false,
     "start_time": "2021-09-22T13:44:14.617104",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00303967, 0.00228824, 0.00228824])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6d4b612d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-22T13:44:19.597460Z",
     "iopub.status.busy": "2021-09-22T13:44:19.596033Z",
     "iopub.status.idle": "2021-09-22T13:49:38.730081Z",
     "shell.execute_reply": "2021-09-22T13:49:38.730523Z"
    },
    "papermill": {
     "duration": 320.960772,
     "end_time": "2021-09-22T13:49:38.730703",
     "exception": false,
     "start_time": "2021-09-22T13:44:17.769931",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV 1/5\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 26.3417 - val_loss: 2.2222\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 1.3250 - val_loss: 0.8120\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.6903 - val_loss: 0.8509\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.3671 - val_loss: 0.2380\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.3329 - val_loss: 0.4385\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.6697 - val_loss: 0.7839\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.5902 - val_loss: 0.4757\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.5081 - val_loss: 0.4954\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.4951 - val_loss: 0.4226\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 1s 4ms/step - loss: 0.4666 - val_loss: 0.4278\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 1s 4ms/step - loss: 0.4730 - val_loss: 0.2904\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2285 - val_loss: 0.2143\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2161 - val_loss: 0.2134\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2141 - val_loss: 0.2133\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2128 - val_loss: 0.2199\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2134 - val_loss: 0.2218\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2138 - val_loss: 0.2146\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2123 - val_loss: 0.2128\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2125 - val_loss: 0.2123\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2115 - val_loss: 0.2123\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2117 - val_loss: 0.2116\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2117 - val_loss: 0.2124\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2118 - val_loss: 0.2262\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2206 - val_loss: 0.2128\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2164 - val_loss: 0.2127\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2114 - val_loss: 0.2300\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 1s 4ms/step - loss: 0.2188 - val_loss: 0.2291\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2152 - val_loss: 0.2124\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2077 - val_loss: 0.2096\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2073 - val_loss: 0.2096\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2065 - val_loss: 0.2110\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2062 - val_loss: 0.2100\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2070 - val_loss: 0.2097\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2051 - val_loss: 0.2122\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2062 - val_loss: 0.2141\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2076 - val_loss: 0.2109\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2044 - val_loss: 0.2091\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2046 - val_loss: 0.2091\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 1s 4ms/step - loss: 0.2039 - val_loss: 0.2094\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 1s 4ms/step - loss: 0.2044 - val_loss: 0.2090\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2049 - val_loss: 0.2091\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2047 - val_loss: 0.2096\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2046 - val_loss: 0.2093\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2040 - val_loss: 0.2094\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2045 - val_loss: 0.2091\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2042 - val_loss: 0.2092\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2035 - val_loss: 0.2089\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2035 - val_loss: 0.2090\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2038 - val_loss: 0.2095\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2037 - val_loss: 0.2091\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 1s 4ms/step - loss: 0.2037 - val_loss: 0.2090\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2038 - val_loss: 0.2091\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2033 - val_loss: 0.2091\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2042 - val_loss: 0.2091\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2037 - val_loss: 0.2090\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2032 - val_loss: 0.2090\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2034 - val_loss: 0.2091\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2038 - val_loss: 0.2090\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2032 - val_loss: 0.2092\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2037 - val_loss: 0.2090\n",
      "Epoch 61/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2030 - val_loss: 0.2091\n",
      "Epoch 62/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2039 - val_loss: 0.2090\n",
      "Epoch 63/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2033 - val_loss: 0.2090\n",
      "Epoch 64/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2034 - val_loss: 0.2090\n",
      "Epoch 65/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2034 - val_loss: 0.2091\n",
      "Epoch 66/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2025 - val_loss: 0.2090\n",
      "Epoch 67/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2044 - val_loss: 0.2090\n",
      "Fold 1 NN: 0.20894\n",
      "CV 2/5\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 2s 9ms/step - loss: 24.7529 - val_loss: 0.9232\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.9413 - val_loss: 0.6201\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.8854 - val_loss: 1.0569\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.7872 - val_loss: 0.6008\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.7487 - val_loss: 0.5538\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.6815 - val_loss: 0.6626\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.6264 - val_loss: 0.4971\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.6020 - val_loss: 0.6757\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.6895 - val_loss: 0.2424\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2476 - val_loss: 0.4849\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.3248 - val_loss: 0.2409\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.9617 - val_loss: 0.2426\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2411 - val_loss: 0.2739\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2385 - val_loss: 0.2906\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 1s 4ms/step - loss: 0.4535 - val_loss: 0.3455\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.3583 - val_loss: 0.2710\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2399 - val_loss: 0.2258\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2259 - val_loss: 0.2343\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 1s 4ms/step - loss: 0.2276 - val_loss: 0.2272\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2357 - val_loss: 0.2305\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2377 - val_loss: 0.3013\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2487 - val_loss: 0.2272\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2295 - val_loss: 0.2291\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2648 - val_loss: 0.2225\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2420 - val_loss: 0.3138\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2361 - val_loss: 0.3169\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2554 - val_loss: 0.2174\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.4245 - val_loss: 0.2777\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2353 - val_loss: 0.2174\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2336 - val_loss: 0.2309\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2269 - val_loss: 0.2268\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2406 - val_loss: 0.2562\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2269 - val_loss: 0.2315\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2274 - val_loss: 0.2188\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2064 - val_loss: 0.2134\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2064 - val_loss: 0.2195\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2054 - val_loss: 0.2164\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2059 - val_loss: 0.2142\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2052 - val_loss: 0.2155\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2057 - val_loss: 0.2156\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2056 - val_loss: 0.2119\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 2s 10ms/step - loss: 0.2037 - val_loss: 0.2128\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2046 - val_loss: 0.2138\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2058 - val_loss: 0.2131\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2067 - val_loss: 0.2145\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2049 - val_loss: 0.2182\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2057 - val_loss: 0.2134\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2057 - val_loss: 0.2191\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2026 - val_loss: 0.2128\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2019 - val_loss: 0.2136\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2020 - val_loss: 0.2151\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2015 - val_loss: 0.2146\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2014 - val_loss: 0.2124\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2018 - val_loss: 0.2139\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2015 - val_loss: 0.2132\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2012 - val_loss: 0.2125\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2006 - val_loss: 0.2129\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2007 - val_loss: 0.2125\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2004 - val_loss: 0.2126\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2005 - val_loss: 0.2123\n",
      "Epoch 61/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2008 - val_loss: 0.2133\n",
      "Fold 2 NN: 0.21187\n",
      "CV 3/5\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 26.2792 - val_loss: 0.7389\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.6455 - val_loss: 0.6143\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.6169 - val_loss: 0.5611\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.5537 - val_loss: 0.5640\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.8889 - val_loss: 0.5867\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.4794 - val_loss: 0.4564\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.4612 - val_loss: 0.7895\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.4684 - val_loss: 0.5019\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.4082 - val_loss: 0.3919\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2916 - val_loss: 0.2193\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2442 - val_loss: 0.3926\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2851 - val_loss: 0.2334\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2342 - val_loss: 0.2567\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2753 - val_loss: 0.2231\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2417 - val_loss: 0.2510\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2515 - val_loss: 0.6322\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2920 - val_loss: 0.2529\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2142 - val_loss: 0.2122\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2092 - val_loss: 0.2148\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2082 - val_loss: 0.2113\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2076 - val_loss: 0.2109\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2074 - val_loss: 0.2122\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2091 - val_loss: 0.2134\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2096 - val_loss: 0.2171\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2092 - val_loss: 0.2133\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2085 - val_loss: 0.2111\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2107 - val_loss: 0.2174\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2091 - val_loss: 0.2189\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2066 - val_loss: 0.2097\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2048 - val_loss: 0.2097\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2046 - val_loss: 0.2102\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2047 - val_loss: 0.2096\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2040 - val_loss: 0.2097\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2049 - val_loss: 0.2096\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2048 - val_loss: 0.2108\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2038 - val_loss: 0.2114\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2033 - val_loss: 0.2096\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2039 - val_loss: 0.2095\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2031 - val_loss: 0.2096\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2038 - val_loss: 0.2097\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2030 - val_loss: 0.2105\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2040 - val_loss: 0.2094\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2037 - val_loss: 0.2097\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2032 - val_loss: 0.2101\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2029 - val_loss: 0.2099\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2029 - val_loss: 0.2098\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2036 - val_loss: 0.2095\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2032 - val_loss: 0.2099\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2030 - val_loss: 0.2097\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2028 - val_loss: 0.2096\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2023 - val_loss: 0.2097\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2028 - val_loss: 0.2096\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2028 - val_loss: 0.2095\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2033 - val_loss: 0.2096\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2016 - val_loss: 0.2099\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2029 - val_loss: 0.2099\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2022 - val_loss: 0.2097\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2029 - val_loss: 0.2096\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2026 - val_loss: 0.2095\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2027 - val_loss: 0.2096\n",
      "Epoch 61/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2024 - val_loss: 0.2096\n",
      "Epoch 62/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2021 - val_loss: 0.2096\n",
      "Fold 3 NN: 0.2094\n",
      "CV 4/5\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 25.7243 - val_loss: 3.5043\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 1.5458 - val_loss: 1.1711\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.6213 - val_loss: 0.9469\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.6611 - val_loss: 0.6593\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 1.2260 - val_loss: 0.2703\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.4232 - val_loss: 0.8409\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.9293 - val_loss: 1.0378\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.9500 - val_loss: 0.4441\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.5847 - val_loss: 0.3781\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.5797 - val_loss: 0.5845\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.6229 - val_loss: 0.4562\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.6255 - val_loss: 0.4678\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2475 - val_loss: 0.2255\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2141 - val_loss: 0.2305\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2150 - val_loss: 0.2229\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2119 - val_loss: 0.2220\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2117 - val_loss: 0.2223\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2105 - val_loss: 0.2299\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2123 - val_loss: 0.2205\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2101 - val_loss: 0.2199\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2138 - val_loss: 0.2205\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2143 - val_loss: 0.2225\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2128 - val_loss: 0.2195\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2114 - val_loss: 0.2393\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2139 - val_loss: 0.2252\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2117 - val_loss: 0.2209\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2118 - val_loss: 0.2206\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2129 - val_loss: 0.2244\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2154 - val_loss: 0.2261\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2152 - val_loss: 0.2230\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2058 - val_loss: 0.2181\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2044 - val_loss: 0.2180\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2050 - val_loss: 0.2171\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2043 - val_loss: 0.2183\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2049 - val_loss: 0.2190\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 1s 4ms/step - loss: 0.2045 - val_loss: 0.2183\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 1s 4ms/step - loss: 0.2053 - val_loss: 0.2172\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2046 - val_loss: 0.2167\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2057 - val_loss: 0.2200\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2045 - val_loss: 0.2192\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2048 - val_loss: 0.2176\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2042 - val_loss: 0.2174\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2040 - val_loss: 0.2177\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2048 - val_loss: 0.2164\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2057 - val_loss: 0.2184\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2040 - val_loss: 0.2171\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2056 - val_loss: 0.2171\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2052 - val_loss: 0.2191\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2061 - val_loss: 0.2192\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2040 - val_loss: 0.2173\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2037 - val_loss: 0.2216\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2023 - val_loss: 0.2159\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2012 - val_loss: 0.2165\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2019 - val_loss: 0.2172\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2013 - val_loss: 0.2167\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2008 - val_loss: 0.2171\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2015 - val_loss: 0.2161\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2008 - val_loss: 0.2166\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2012 - val_loss: 0.2160\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2004 - val_loss: 0.2163\n",
      "Epoch 61/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.1999 - val_loss: 0.2160\n",
      "Epoch 62/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2006 - val_loss: 0.2158\n",
      "Epoch 63/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2006 - val_loss: 0.2163\n",
      "Epoch 64/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2006 - val_loss: 0.2160\n",
      "Epoch 65/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2007 - val_loss: 0.2163\n",
      "Epoch 66/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2006 - val_loss: 0.2161\n",
      "Epoch 67/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2005 - val_loss: 0.2160\n",
      "Epoch 68/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2006 - val_loss: 0.2160\n",
      "Epoch 69/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1999 - val_loss: 0.2160\n",
      "Epoch 70/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2003 - val_loss: 0.2161\n",
      "Epoch 71/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2008 - val_loss: 0.2161\n",
      "Epoch 72/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.1998 - val_loss: 0.2160\n",
      "Epoch 73/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2004 - val_loss: 0.2160\n",
      "Epoch 74/1000\n",
      "168/168 [==============================] - 1s 4ms/step - loss: 0.1996 - val_loss: 0.2159\n",
      "Epoch 75/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2003 - val_loss: 0.2160\n",
      "Epoch 76/1000\n",
      "168/168 [==============================] - 1s 4ms/step - loss: 0.1998 - val_loss: 0.2160\n",
      "Epoch 77/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2006 - val_loss: 0.2159\n",
      "Epoch 78/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2007 - val_loss: 0.2160\n",
      "Epoch 79/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2004 - val_loss: 0.2159\n",
      "Epoch 80/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.1998 - val_loss: 0.2160\n",
      "Epoch 81/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2001 - val_loss: 0.2160\n",
      "Epoch 82/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2002 - val_loss: 0.2160\n",
      "Fold 4 NN: 0.21584\n",
      "CV 5/5\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 2s 6ms/step - loss: 29.6795 - val_loss: 2.4449\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 1.6775 - val_loss: 0.6789\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.6017 - val_loss: 0.5418\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.5819 - val_loss: 0.4085\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.5685 - val_loss: 0.4747\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.5988 - val_loss: 0.4648\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.5082 - val_loss: 0.8314\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.3877 - val_loss: 0.2489\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 1s 4ms/step - loss: 0.2601 - val_loss: 0.2932\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.3009 - val_loss: 0.2438\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 1s 4ms/step - loss: 0.2829 - val_loss: 0.3604\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 1s 4ms/step - loss: 0.2757 - val_loss: 0.2282\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2933 - val_loss: 0.3830\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 1.8112 - val_loss: 0.3091\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.3167 - val_loss: 0.3220\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2707 - val_loss: 0.2451\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2311 - val_loss: 0.2227\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2263 - val_loss: 0.2414\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2302 - val_loss: 0.2227\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2379 - val_loss: 0.2239\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2319 - val_loss: 0.2204\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2331 - val_loss: 0.3053\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2525 - val_loss: 0.2374\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2696 - val_loss: 0.2350\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2425 - val_loss: 0.2195\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2513 - val_loss: 0.2299\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2286 - val_loss: 0.2386\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2341 - val_loss: 0.2830\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2446 - val_loss: 0.3000\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2411 - val_loss: 0.2335\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2385 - val_loss: 0.2265\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2393 - val_loss: 0.2652\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 1s 4ms/step - loss: 0.2120 - val_loss: 0.2155\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2060 - val_loss: 0.2155\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2059 - val_loss: 0.2137\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2060 - val_loss: 0.2172\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2053 - val_loss: 0.2135\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2044 - val_loss: 0.2150\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2054 - val_loss: 0.2160\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 1s 4ms/step - loss: 0.2050 - val_loss: 0.2145\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2054 - val_loss: 0.2169\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2071 - val_loss: 0.2142\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2071 - val_loss: 0.2274\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2079 - val_loss: 0.2197\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2039 - val_loss: 0.2133\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2026 - val_loss: 0.2140\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2029 - val_loss: 0.2135\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2018 - val_loss: 0.2139\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2029 - val_loss: 0.2139\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2022 - val_loss: 0.2138\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2015 - val_loss: 0.2141\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2018 - val_loss: 0.2134\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2016 - val_loss: 0.2130\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2014 - val_loss: 0.2132\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2015 - val_loss: 0.2137\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2015 - val_loss: 0.2134\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2012 - val_loss: 0.2138\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 1s 4ms/step - loss: 0.2020 - val_loss: 0.2136\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2014 - val_loss: 0.2134\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2011 - val_loss: 0.2131\n",
      "Epoch 61/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2013 - val_loss: 0.2133\n",
      "Epoch 62/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2013 - val_loss: 0.2131\n",
      "Epoch 63/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2016 - val_loss: 0.2134\n",
      "Epoch 64/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2011 - val_loss: 0.2137\n",
      "Epoch 65/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2011 - val_loss: 0.2135\n",
      "Epoch 66/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2008 - val_loss: 0.2133\n",
      "Epoch 67/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2014 - val_loss: 0.2134\n",
      "Epoch 68/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2015 - val_loss: 0.2133\n",
      "Epoch 69/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2007 - val_loss: 0.2133\n",
      "Epoch 70/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2009 - val_loss: 0.2134\n",
      "Epoch 71/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2008 - val_loss: 0.2133\n",
      "Epoch 72/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2016 - val_loss: 0.2133\n",
      "Epoch 73/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2015 - val_loss: 0.2134\n",
      "Fold 5 NN: 0.21304\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(42)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(41)\n",
    "from tensorflow import keras\n",
    "\n",
    "target_name='target'\n",
    "scores_folds = {}\n",
    "model_name = 'NN'\n",
    "pred_name = 'pred_{}'.format(model_name)\n",
    "\n",
    "n_folds = 5\n",
    "kf = model_selection.KFold(n_splits=n_folds, shuffle=True, random_state=20)\n",
    "scores_folds[model_name] = []\n",
    "counter = 1\n",
    "\n",
    "features_to_consider = list(train1)\n",
    "\n",
    "features_to_consider.remove('time_id')\n",
    "features_to_consider.remove('target')\n",
    "try:\n",
    "    features_to_consider.remove('pred_NN')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "train1[features_to_consider] = train1[features_to_consider].fillna(train1[features_to_consider].mean())\n",
    "test1[features_to_consider] = test1[features_to_consider].fillna(train1[features_to_consider].mean())\n",
    "\n",
    "train1[pred_name] = 0\n",
    "test1[target_name] = 0\n",
    "test_predictions_nn1 = np.zeros(test_nn.shape[0])\n",
    "\n",
    "for n_count in range(n_folds):\n",
    "    print('CV {}/{}'.format(counter, n_folds))\n",
    "    \n",
    "    indexes = np.arange(nfolds).astype(int)    \n",
    "    indexes = np.delete(indexes,obj=n_count, axis=0) \n",
    "    \n",
    "    indexes = np.r_[values[indexes[0]],values[indexes[1]],values[indexes[2]],values[indexes[3]]]\n",
    "    \n",
    "    X_train = train1.loc[train1.time_id.isin(indexes), features_to_consider]\n",
    "    y_train = train1.loc[train1.time_id.isin(indexes), target_name]\n",
    "    X_test = train1.loc[train1.time_id.isin(values[n_count]), features_to_consider]\n",
    "    y_test = train1.loc[train1.time_id.isin(values[n_count]), target_name]\n",
    "    \n",
    "    #############################################################################################\n",
    "    # NN\n",
    "    #############################################################################################\n",
    "    \n",
    "    model = base_model()\n",
    "    \n",
    "    model.compile(\n",
    "        keras.optimizers.Adam(learning_rate=0.006),\n",
    "        loss=root_mean_squared_per_error\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        features_to_consider.remove('stock_id')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    num_data = X_train[features_to_consider]\n",
    "    \n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))         \n",
    "    num_data = scaler.fit_transform(num_data.values)    \n",
    "    \n",
    "    cat_data = X_train['stock_id']    \n",
    "    target =  y_train\n",
    "    \n",
    "    num_data_test = X_test[features_to_consider]\n",
    "    num_data_test = scaler.transform(num_data_test.values)\n",
    "    cat_data_test = X_test['stock_id']\n",
    "\n",
    "    model.fit([cat_data, num_data], \n",
    "              target,               \n",
    "              batch_size=2048,\n",
    "              epochs=1000,\n",
    "              validation_data=([cat_data_test, num_data_test], y_test),\n",
    "              callbacks=[es, plateau],\n",
    "              validation_batch_size=len(y_test),\n",
    "              shuffle=True,\n",
    "             verbose = 1)\n",
    "\n",
    "    preds = model.predict([cat_data_test, num_data_test]).reshape(1,-1)[0]\n",
    "    \n",
    "    score = round(rmspe(y_true = y_test, y_pred = preds),5)\n",
    "    print('Fold {} {}: {}'.format(counter, model_name, score))\n",
    "    scores_folds[model_name].append(score)\n",
    "    \n",
    "    tt =scaler.transform(test_nn[features_to_consider].values)\n",
    "    #test_nn[target_name] += model.predict([test_nn['stock_id'], tt]).reshape(1,-1)[0].clip(0,1e10)\n",
    "    test_predictions_nn1 += model.predict([test1['stock_id'], tt]).reshape(1,-1)[0].clip(0,1e10)/n_folds\n",
    "    #test[target_name] += model.predict([test['stock_id'], test[features_to_consider]]).reshape(1,-1)[0].clip(0,1e10)\n",
    "       \n",
    "    counter += 1\n",
    "    features_to_consider.append('stock_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "01ac9297",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-22T13:49:44.346133Z",
     "iopub.status.busy": "2021-09-22T13:49:44.345284Z",
     "iopub.status.idle": "2021-09-22T13:49:44.368079Z",
     "shell.execute_reply": "2021-09-22T13:49:44.367562Z"
    },
    "papermill": {
     "duration": 2.844731,
     "end_time": "2021-09-22T13:49:44.368191",
     "exception": false,
     "start_time": "2021-09-22T13:49:41.523460",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wap1_sum</th>\n",
       "      <th>wap1_std</th>\n",
       "      <th>wap2_sum</th>\n",
       "      <th>wap2_std</th>\n",
       "      <th>wap3_sum</th>\n",
       "      <th>wap3_std</th>\n",
       "      <th>wap4_sum</th>\n",
       "      <th>wap4_std</th>\n",
       "      <th>log_return1_realized_volatility</th>\n",
       "      <th>log_return2_realized_volatility</th>\n",
       "      <th>...</th>\n",
       "      <th>bid_ask_spread_sum_1c1</th>\n",
       "      <th>bid_ask_spread_sum_3c1</th>\n",
       "      <th>bid_ask_spread_sum_4c1</th>\n",
       "      <th>bid_ask_spread_sum_6c1</th>\n",
       "      <th>size_tau2_0c1</th>\n",
       "      <th>size_tau2_1c1</th>\n",
       "      <th>size_tau2_3c1</th>\n",
       "      <th>size_tau2_4c1</th>\n",
       "      <th>size_tau2_6c1</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-2.429043</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-2.723074</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-2.722340</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-3.714581</td>\n",
       "      <td>-3.421779</td>\n",
       "      <td>-3.467215</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.199337</td>\n",
       "      <td>0.035373</td>\n",
       "      <td>0.221821</td>\n",
       "      <td>0.431936</td>\n",
       "      <td>-0.086921</td>\n",
       "      <td>3.161571</td>\n",
       "      <td>0.590543</td>\n",
       "      <td>-0.161021</td>\n",
       "      <td>-0.482473</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000281</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000382</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000429</td>\n",
       "      <td>0.001459</td>\n",
       "      <td>0.001485</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.212994</td>\n",
       "      <td>0.035373</td>\n",
       "      <td>0.221821</td>\n",
       "      <td>0.431936</td>\n",
       "      <td>-0.086921</td>\n",
       "      <td>-0.085898</td>\n",
       "      <td>0.590543</td>\n",
       "      <td>-0.161021</td>\n",
       "      <td>-0.482473</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000281</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000382</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000429</td>\n",
       "      <td>0.001459</td>\n",
       "      <td>0.001485</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.212994</td>\n",
       "      <td>0.035373</td>\n",
       "      <td>0.221821</td>\n",
       "      <td>0.431936</td>\n",
       "      <td>-0.086921</td>\n",
       "      <td>-0.085898</td>\n",
       "      <td>0.590543</td>\n",
       "      <td>-0.161021</td>\n",
       "      <td>-0.482473</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 247 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   wap1_sum  wap1_std  wap2_sum  wap2_std  wap3_sum  wap3_std  wap4_sum  \\\n",
       "0 -5.199338 -2.429043 -5.199338 -2.723074 -5.199338 -2.722340 -5.199338   \n",
       "1  0.000022  0.000281  0.000025  0.000382  0.000019  0.000336  0.000018   \n",
       "2  0.000022  0.000281  0.000025  0.000382  0.000019  0.000336  0.000018   \n",
       "\n",
       "   wap4_std  log_return1_realized_volatility  log_return2_realized_volatility  \\\n",
       "0 -3.714581                        -3.421779                        -3.467215   \n",
       "1  0.000429                         0.001459                         0.001485   \n",
       "2  0.000429                         0.001459                         0.001485   \n",
       "\n",
       "   ...  bid_ask_spread_sum_1c1  bid_ask_spread_sum_3c1  \\\n",
       "0  ...               -5.199337                0.035373   \n",
       "1  ...               -0.212994                0.035373   \n",
       "2  ...               -0.212994                0.035373   \n",
       "\n",
       "   bid_ask_spread_sum_4c1  bid_ask_spread_sum_6c1  size_tau2_0c1  \\\n",
       "0                0.221821                0.431936      -0.086921   \n",
       "1                0.221821                0.431936      -0.086921   \n",
       "2                0.221821                0.431936      -0.086921   \n",
       "\n",
       "   size_tau2_1c1  size_tau2_3c1  size_tau2_4c1  size_tau2_6c1  target  \n",
       "0       3.161571       0.590543      -0.161021      -0.482473       0  \n",
       "1      -0.085898       0.590543      -0.161021      -0.482473       0  \n",
       "2      -0.085898       0.590543      -0.161021      -0.482473       0  \n",
       "\n",
       "[3 rows x 247 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "46a8680f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-22T13:49:50.249717Z",
     "iopub.status.busy": "2021-09-22T13:49:50.248937Z",
     "iopub.status.idle": "2021-09-22T13:49:50.264928Z",
     "shell.execute_reply": "2021-09-22T13:49:50.264500Z"
    },
    "papermill": {
     "duration": 2.869853,
     "end_time": "2021-09-22T13:49:50.265055",
     "exception": false,
     "start_time": "2021-09-22T13:49:47.395202",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0-4</td>\n",
       "      <td>0.002946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0-32</td>\n",
       "      <td>0.002107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0-34</td>\n",
       "      <td>0.002107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  row_id    target\n",
       "0    0-4  0.002946\n",
       "1   0-32  0.002107\n",
       "2   0-34  0.002107"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test=pd.read_csv(\"../input/optiver-realized-volatility-prediction/test.csv\")\n",
    "a=test_predictions_nn*0.5+predictions_lgb2*0.5\n",
    "b=test_predictions_nn1*0.57+predictions_lgb1*0.43\n",
    "test[target_name] = (a+b)/2\n",
    "\n",
    "display(test[['row_id', target_name]].head(3))\n",
    "test[['row_id', target_name]].to_csv('submission.csv',index = False)\n",
    "#test[['row_id', target_name]].to_csv('submission.csv',index = False)\n",
    "#kmeans N=5 [0.2101, 0.21399, 0.20923, 0.21398, 0.21175]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1beaabf7",
   "metadata": {
    "papermill": {
     "duration": 2.806018,
     "end_time": "2021-09-22T13:49:55.851799",
     "exception": false,
     "start_time": "2021-09-22T13:49:53.045781",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e92f72c",
   "metadata": {
    "papermill": {
     "duration": 2.776083,
     "end_time": "2021-09-22T13:50:01.640030",
     "exception": false,
     "start_time": "2021-09-22T13:49:58.863947",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ce041f",
   "metadata": {
    "papermill": {
     "duration": 2.794954,
     "end_time": "2021-09-22T13:50:07.259245",
     "exception": false,
     "start_time": "2021-09-22T13:50:04.464291",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9b91ab",
   "metadata": {
    "papermill": {
     "duration": 2.802349,
     "end_time": "2021-09-22T13:50:13.651020",
     "exception": false,
     "start_time": "2021-09-22T13:50:10.848671",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c728de0",
   "metadata": {
    "papermill": {
     "duration": 3.023079,
     "end_time": "2021-09-22T13:50:19.483875",
     "exception": false,
     "start_time": "2021-09-22T13:50:16.460796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2974.350334,
   "end_time": "2021-09-22T13:50:24.461410",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-09-22T13:00:50.111076",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
