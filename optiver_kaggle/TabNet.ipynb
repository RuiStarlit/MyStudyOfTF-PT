{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cbe7756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy.matlib\n",
    "\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import shutil\n",
    "import glob\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from pytorch_tabnet.metrics import Metric\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts\n",
    "\n",
    "\n",
    "# setting some globl config\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "orange_black = [\n",
    "    '#fdc029', '#df861d', '#FF6347', '#aa3d01', '#a30e15', '#800000', '#171820'\n",
    "]\n",
    "plt.rcParams['figure.figsize'] = (16,9)\n",
    "plt.rcParams[\"figure.facecolor\"] = '#FFFACD'\n",
    "plt.rcParams[\"axes.facecolor\"] = '#FFFFE0'\n",
    "plt.rcParams[\"axes.grid\"] = True\n",
    "plt.rcParams[\"grid.color\"] = orange_black[3]\n",
    "plt.rcParams[\"grid.alpha\"] = 0.5\n",
    "plt.rcParams[\"grid.linestyle\"] = '--'\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9421273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import psutil\n",
    "psutil.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e52e4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_train_test():\n",
    "    # Function to read our base train and test set\n",
    "    \n",
    "    train = pd.read_csv('input/optiver-realized-volatility-prediction/train.csv')\n",
    "    test = pd.read_csv('input/optiver-realized-volatility-prediction/test.csv')\n",
    "\n",
    "    # Create a key to merge with book and trade data\n",
    "    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n",
    "    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n",
    "    print(f'Our training set has {train.shape[0]} rows')\n",
    "    print(f'Our test set has {test.shape[0]} rows')\n",
    "    print(f'Our training set has {train.isna().sum().sum()} missing values')\n",
    "    print(f'Our test set has {test.isna().sum().sum()} missing values')\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a590375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\python_project\\jupyter\\optiver_kaggle\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.path.abspath('.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1341cb56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our training set has 428932 rows\n",
      "Our test set has 3 rows\n",
      "Our training set has 0 missing values\n",
      "Our test set has 0 missing values\n"
     ]
    }
   ],
   "source": [
    "train, test = read_train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30867403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data directory\n",
    "data_dir = 'input/optiver-realized-volatility-prediction/'\n",
    "\n",
    "def calc_wap1(df):\n",
    "    # Function to calculate first WAP\n",
    "    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "    return wap\n",
    "\n",
    "def calc_wap2(df):\n",
    "    # Function to calculate second WAP\n",
    "    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "    return wap\n",
    "\n",
    "def log_return(series):\n",
    "    # Function to calculate the log of the return\n",
    "    return np.log(series).diff()\n",
    "\n",
    "def realized_volatility(series):\n",
    "    # Calculate the realized volatility\n",
    "    return np.sqrt(np.sum(series**2))\n",
    "\n",
    "def count_unique(series):\n",
    "    # Function to count unique elements of a series\n",
    "    return len(np.unique(series))\n",
    "\n",
    "def book_preprocessor(file_path):\n",
    "    # Function to preprocess book data (for each stock id)\n",
    "    \n",
    "    df = pd.read_parquet(file_path)\n",
    "    \n",
    "    # Calculate Wap\n",
    "    df['wap1'] = calc_wap1(df)\n",
    "    df['wap2'] = calc_wap2(df)\n",
    "    \n",
    "    # Calculate log returns\n",
    "    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n",
    "    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n",
    "    \n",
    "    # Calculate wap balance\n",
    "    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n",
    "    \n",
    "    # Calculate spread\n",
    "    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n",
    "    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) / ((df['ask_price2'] + df['bid_price2']) / 2)\n",
    "    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n",
    "    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n",
    "    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n",
    "    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n",
    "    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n",
    "    \n",
    "    # Dict for aggregations\n",
    "    create_feature_dict = {\n",
    "        'wap1': [np.sum, np.mean, np.std],\n",
    "        'wap2': [np.sum, np.mean, np.std],\n",
    "        'log_return1': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'log_return2': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'wap_balance': [np.sum, np.mean, np.std],\n",
    "        'price_spread':[np.sum, np.mean, np.std],\n",
    "        'price_spread2':[np.sum, np.mean, np.std],\n",
    "        'bid_spread':[np.sum, np.mean, np.std],\n",
    "        'ask_spread':[np.sum, np.mean, np.std],\n",
    "        'total_volume':[np.sum, np.mean, np.std],\n",
    "        'volume_imbalance':[np.sum, np.mean, np.std],\n",
    "        \"bid_ask_spread\":[np.sum, np.mean, np.std],\n",
    "    }\n",
    "    \n",
    "    def get_stats_window(seconds_in_bucket, add_suffix = False):\n",
    "        # Function to get group stats for different windows (seconds in bucket)\n",
    "        \n",
    "        # Group by the window\n",
    "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n",
    "        \n",
    "        # Rename columns joining suffix\n",
    "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "        \n",
    "        # Add a suffix to differentiate windows\n",
    "        if add_suffix:\n",
    "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
    "        return df_feature\n",
    "    \n",
    "    # Get the stats for different windows\n",
    "    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n",
    "    df_feature_400 = get_stats_window(seconds_in_bucket = 400, add_suffix = True)\n",
    "    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n",
    "    df_feature_200 = get_stats_window(seconds_in_bucket = 200, add_suffix = True)\n",
    "    \n",
    "    # Merge all\n",
    "    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n",
    "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
    "    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n",
    "\n",
    "    # Drop unnecesary time_ids\n",
    "    df_feature.drop(['time_id__400', 'time_id__300', 'time_id__200'], axis = 1, inplace = True)\n",
    "    \n",
    "    \n",
    "    # Create row_id so we can merge\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n",
    "    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n",
    "    \n",
    "    return df_feature\n",
    "\n",
    "\n",
    "def trade_preprocessor(file_path):\n",
    "    # Function to preprocess trade data (for each stock id)\n",
    "    \n",
    "    df = pd.read_parquet(file_path)\n",
    "    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n",
    "    \n",
    "    # Dict for aggregations\n",
    "    create_feature_dict = {\n",
    "        'log_return':[realized_volatility],\n",
    "        'seconds_in_bucket':[count_unique],\n",
    "        'size':[np.sum, realized_volatility, np.mean, np.std, np.max, np.min],\n",
    "        'order_count':[np.mean,np.sum,np.max],\n",
    "    }\n",
    "    \n",
    "    def get_stats_window(seconds_in_bucket, add_suffix = False):\n",
    "        # Function to get group stats for different windows (seconds in bucket)\n",
    "        \n",
    "        # Group by the window\n",
    "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n",
    "        \n",
    "        # Rename columns joining suffix\n",
    "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "        \n",
    "        # Add a suffix to differentiate windows\n",
    "        if add_suffix:\n",
    "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
    "        return df_feature\n",
    "    \n",
    "    # Get the stats for different windows\n",
    "    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n",
    "    df_feature_400 = get_stats_window(seconds_in_bucket = 400, add_suffix = True)\n",
    "    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n",
    "    df_feature_200 = get_stats_window(seconds_in_bucket = 200, add_suffix = True)\n",
    "    \n",
    "    def tendency(price, vol):    \n",
    "        df_diff = np.diff(price)\n",
    "        val = (df_diff/price[1:])*100\n",
    "        power = np.sum(val*vol[1:])\n",
    "        return(power)\n",
    "    \n",
    "    lis = []\n",
    "    for n_time_id in df['time_id'].unique():\n",
    "        df_id = df[df['time_id'] == n_time_id]        \n",
    "        tendencyV = tendency(df_id['price'].values, df_id['size'].values)      \n",
    "        f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n",
    "        f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n",
    "        df_max =  np.sum(np.diff(df_id['price'].values) > 0)\n",
    "        df_min =  np.sum(np.diff(df_id['price'].values) < 0)\n",
    "        abs_diff = np.median(np.abs( df_id['price'].values - np.mean(df_id['price'].values)))        \n",
    "        energy = np.mean(df_id['price'].values**2)\n",
    "        iqr_p = np.percentile(df_id['price'].values,75) - np.percentile(df_id['price'].values,25)\n",
    "        abs_diff_v = np.median(np.abs( df_id['size'].values - np.mean(df_id['size'].values)))        \n",
    "        energy_v = np.sum(df_id['size'].values**2)\n",
    "        iqr_p_v = np.percentile(df_id['size'].values,75) - np.percentile(df_id['size'].values,25)\n",
    "        \n",
    "        lis.append({'time_id':n_time_id,'tendency':tendencyV,'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,\n",
    "                   'abs_diff':abs_diff,'energy':energy,'iqr_p':iqr_p,'abs_diff_v':abs_diff_v,'energy_v':energy_v,'iqr_p_v':iqr_p_v})\n",
    "    \n",
    "    df_lr = pd.DataFrame(lis)\n",
    "        \n",
    "   \n",
    "    df_feature = df_feature.merge(df_lr, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n",
    "    \n",
    "    # Merge all\n",
    "    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n",
    "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
    "    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n",
    "\n",
    "    # Drop unnecesary time_ids\n",
    "    df_feature.drop(['time_id__400', 'time_id__300', 'time_id__200','time_id'], axis = 1, inplace = True)\n",
    "    df_feature = df_feature.add_prefix('trade_')\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n",
    "    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n",
    "    \n",
    "    def order_sum(df, sec:str):\n",
    "        new_col = 'size_tau' + sec\n",
    "        bucket_col = 'trade_seconds_in_bucket_count_unique' + sec\n",
    "        df[new_col] = np.sqrt(1/df[bucket_col])\n",
    "        \n",
    "        new_col2 = 'size_tau2' + sec\n",
    "        order_col = 'trade_order_count_sum' + sec\n",
    "        df[new_col2] = np.sqrt(1/df[order_col])\n",
    "        \n",
    "        if sec == '400_':\n",
    "            df['size_tau2_d'] = df['size_tau2_400'] - df['size_tau2']\n",
    "        \n",
    "\n",
    "    \n",
    "    for sec in ['','_200','_300','_400']:\n",
    "        order_sum(df_feature, sec)\n",
    "        \n",
    "    df_feature['size_tau2_d'] = df_feature['size_tau2_400'] - df_feature['size_tau2']\n",
    "    \n",
    "    return df_feature\n",
    "\n",
    "\n",
    "def get_time_stock(df):\n",
    "    # Function to get group stats for the stock_id and time_id\n",
    "    \n",
    "    # Get realized volatility columns\n",
    "    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_400', 'log_return2_realized_volatility_400', \n",
    "                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_200', 'log_return2_realized_volatility_200', \n",
    "                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_400', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_200']\n",
    "\n",
    "    # Group by the stock id\n",
    "    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
    "    \n",
    "    # Rename columns joining suffix\n",
    "    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n",
    "    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n",
    "\n",
    "    # Group by the stock id\n",
    "    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
    "    \n",
    "    # Rename columns joining suffix\n",
    "    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n",
    "    df_time_id = df_time_id.add_suffix('_' + 'time')\n",
    "    \n",
    "    # Merge with original dataframe\n",
    "    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n",
    "    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n",
    "    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_agg_features(train, test):\n",
    "\n",
    "    # Making agg features\n",
    "\n",
    "    train_p = pd.read_csv('input/optiver-realized-volatility-prediction/train.csv')\n",
    "    train_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n",
    "    corr = train_p.corr()\n",
    "    ids = corr.index\n",
    "    kmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\n",
    "    l = []\n",
    "    for n in range(7):\n",
    "        l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n",
    "\n",
    "    mat = []\n",
    "    matTest = []\n",
    "    n = 0\n",
    "    for ind in l:\n",
    "        newDf = train.loc[train['stock_id'].isin(ind) ]\n",
    "        newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
    "        newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
    "        mat.append ( newDf )\n",
    "        newDf = test.loc[test['stock_id'].isin(ind) ]    \n",
    "        newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
    "        newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
    "        matTest.append ( newDf )\n",
    "        n+=1\n",
    "\n",
    "    mat1 = pd.concat(mat).reset_index()\n",
    "    mat1.drop(columns=['target'],inplace=True)\n",
    "    mat2 = pd.concat(matTest).reset_index()\n",
    "    \n",
    "    mat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])\n",
    "    \n",
    "    mat1 = mat1.pivot(index='time_id', columns='stock_id')\n",
    "    mat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\n",
    "    mat1.reset_index(inplace=True)\n",
    "    \n",
    "    mat2 = mat2.pivot(index='time_id', columns='stock_id')\n",
    "    mat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\n",
    "    mat2.reset_index(inplace=True)\n",
    "    \n",
    "    prefix = ['log_return1_realized_volatility', 'total_volume_mean', 'trade_size_mean', 'trade_order_count_mean','price_spread_mean','bid_spread_mean','ask_spread_mean',\n",
    "              'volume_imbalance_mean', 'bid_ask_spread_mean','size_tau2']\n",
    "    selected_cols=mat1.filter(regex='|'.join(f'^{x}.(0|1|3|4|6)c1' for x in prefix)).columns.tolist()\n",
    "    selected_cols.append('time_id')\n",
    "    \n",
    "    train_m = pd.merge(train,mat1[selected_cols],how='left',on='time_id')\n",
    "    test_m = pd.merge(test,mat2[selected_cols],how='left',on='time_id')\n",
    "    \n",
    "    # filling missing values with train means\n",
    "\n",
    "    features = [col for col in train_m.columns.tolist() if col not in ['time_id','target','row_id']]\n",
    "    train_m[features] = train_m[features].fillna(train_m[features].mean())\n",
    "    test_m[features] = test_m[features].fillna(train_m[features].mean())\n",
    "\n",
    "    return train_m, test_m\n",
    "    \n",
    "    \n",
    "def preprocessor(list_stock_ids, is_train = True):\n",
    "    # Funtion to make preprocessing function in parallel (for each stock id)\n",
    "    \n",
    "    # Parrallel for loop\n",
    "    def for_joblib(stock_id):\n",
    "        # Train\n",
    "        if is_train:\n",
    "            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n",
    "        # Test\n",
    "        else:\n",
    "            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n",
    "    \n",
    "        # Preprocess book and trade data and merge them\n",
    "        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n",
    "        \n",
    "        # Return the merge dataframe\n",
    "        return df_tmp\n",
    "    \n",
    "    # Use parallel api to call paralle for loop\n",
    "    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n",
    "    \n",
    "    # Concatenate all the dataframes that return from Parallel\n",
    "    df = pd.concat(df, ignore_index = True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a77efd5",
   "metadata": {},
   "source": [
    "Loding the and doing some feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3d8141",
   "metadata": {},
   "source": [
    "# Get unique stock ids \n",
    "train_stock_ids = train['stock_id'].unique()\n",
    "\n",
    "# Preprocess them using Parallel and our single stock id functions\n",
    "train_ = preprocessor(train_stock_ids, is_train = True)\n",
    "train = train.merge(train_, on = ['row_id'], how = 'left')\n",
    "\n",
    "# Get unique stock ids \n",
    "test_stock_ids = test['stock_id'].unique()\n",
    "\n",
    "# Preprocess them using Parallel and our single stock id functions\n",
    "test_ = preprocessor(test_stock_ids, is_train = False)\n",
    "test = test.merge(test_, on = ['row_id'], how = 'left')\n",
    "\n",
    "# Get group stats of time_id and stock_id\n",
    "train = get_time_stock(train)\n",
    "test = get_time_stock(test)\n",
    "\n",
    "# Fill inf values\n",
    "train.replace([np.inf, -np.inf], np.nan,inplace=True)\n",
    "test.replace([np.inf, -np.inf], np.nan,inplace=True)\n",
    "\n",
    "# Aggregating some features\n",
    "train, test = create_agg_features(train,test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff07c57",
   "metadata": {},
   "source": [
    "import pickle\n",
    "train.to_pickle(\"TabNet_temp/train.pkl\")\n",
    "test.to_pickle(\"TabNet_temp/test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd6356f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "train = pd.read_pickle(\"TabNet_temp/train.pkl\")\n",
    "test = pd.read_pickle(\"TabNet_temp/test.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0192df70",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2db724e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(['row_id', 'target', 'time_id'], axis = 1)\n",
    "y = train['target']\n",
    "X_test=test.copy()\n",
    "X_test.drop(['time_id','row_id'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfcf7d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmspe(y_true, y_pred):\n",
    "    # Function to calculate the root mean squared percentage error\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "class RMSPE(Metric):\n",
    "    def __init__(self):\n",
    "        self._name = \"rmspe\"\n",
    "        self._maximize = False\n",
    "\n",
    "    def __call__(self, y_true, y_score):\n",
    "        \n",
    "        return np.sqrt(np.mean(np.square((y_true - y_score) / y_true)))\n",
    "    \n",
    "\n",
    "\n",
    "def RMSPELoss(y_pred, y_true):\n",
    "    return torch.sqrt(torch.mean( ((y_true - y_pred) / y_true) ** 2 )).clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08930991",
   "metadata": {},
   "outputs": [],
   "source": [
    "nunique = X.nunique()\n",
    "types = X.dtypes\n",
    "\n",
    "categorical_columns = []\n",
    "categorical_dims =  {}\n",
    "\n",
    "for col in X.columns:\n",
    "    if  col == 'stock_id':\n",
    "        l_enc = LabelEncoder()\n",
    "        X[col] = l_enc.fit_transform(X[col].values)\n",
    "        X_test[col] = l_enc.transform(X_test[col].values)\n",
    "        categorical_columns.append(col)\n",
    "        categorical_dims[col] = len(l_enc.classes_)\n",
    "    else:\n",
    "        scaler = StandardScaler()\n",
    "        X[col] = scaler.fit_transform(X[col].values.reshape(-1, 1))\n",
    "        X_test[col] = scaler.transform(X_test[col].values.reshape(-1, 1))\n",
    "        \n",
    "\n",
    "\n",
    "cat_idxs = [ i for i, f in enumerate(X.columns.tolist()) if f in categorical_columns]\n",
    "\n",
    "cat_dims = [ categorical_dims[f] for i, f in enumerate(X.columns.tolist()) if f in categorical_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15d9f1e",
   "metadata": {},
   "source": [
    "TabNet parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82abe32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabnet_params = dict(\n",
    "    cat_idxs=cat_idxs,\n",
    "    cat_dims=cat_dims,\n",
    "    cat_emb_dim=1,\n",
    "    n_d = 16,\n",
    "    n_a = 16,\n",
    "    n_steps = 2,\n",
    "    gamma = 2,\n",
    "    n_independent = 2,\n",
    "    n_shared = 2,\n",
    "    lambda_sparse = 0,\n",
    "    optimizer_fn = Adam,\n",
    "    optimizer_params = dict(lr = (2e-2)),\n",
    "    mask_type = \"entmax\",\n",
    "    scheduler_params = dict(T_0=200, T_mult=1, eta_min=1e-4, last_epoch=-1, verbose=False),\n",
    "    scheduler_fn = CosineAnnealingWarmRestarts,\n",
    "    seed = 42,\n",
    "    verbose = 1\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb3f394a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c690b27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Oct 13 16:53:38 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 472.12       Driver Version: 472.12       CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "| N/A   51C    P5    19W /  N/A |   1552MiB /  6144MiB |     45%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A       440    C+G   ...ne\\bin\\webwallpaper32.exe    N/A      |\n",
      "|    0   N/A  N/A      1552    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A      7700    C+G   ...me\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A      7768    C+G   ...in7x64\\steamwebhelper.exe    N/A      |\n",
      "|    0   N/A  N/A      9224    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A     10492    C+G   ..._dt26b99r8h8gj\\RtkUWP.exe    N/A      |\n",
      "|    0   N/A  N/A     11744    C+G   ...perience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A     12144    C+G   ...perience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A     12636    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A     13320    C+G   ...ekyb3d8bbwe\\YourPhone.exe    N/A      |\n",
      "|    0   N/A  N/A     13668    C+G   ...cw5n1h2txyewy\\LockApp.exe    N/A      |\n",
      "|    0   N/A  N/A     15140    C+G   ...nputApp\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     16176    C+G   ...ram Files\\LGHUB\\lghub.exe    N/A      |\n",
      "|    0   N/A  N/A     17088    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A     17544    C+G   ...7pnf6hceqser\\snipaste.exe    N/A      |\n",
      "|    0   N/A  N/A     18088    C+G   ...8wekyb3d8bbwe\\Cortana.exe    N/A      |\n",
      "|    0   N/A  N/A     18352    C+G   ...6bftszj\\TranslucentTB.exe    N/A      |\n",
      "|    0   N/A  N/A     21164    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     22012    C+G   ...8wekyb3d8bbwe\\GameBar.exe    N/A      |\n",
      "|    0   N/A  N/A     22176    C+G   ...wekyb3d8bbwe\\Video.UI.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4816efce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stock_id',\n",
       " 'wap1_sum',\n",
       " 'wap1_mean',\n",
       " 'wap1_std',\n",
       " 'wap2_sum',\n",
       " 'wap2_mean',\n",
       " 'wap2_std',\n",
       " 'log_return1_sum',\n",
       " 'log_return1_realized_volatility',\n",
       " 'log_return1_mean',\n",
       " 'log_return1_std',\n",
       " 'log_return2_sum',\n",
       " 'log_return2_realized_volatility',\n",
       " 'log_return2_mean',\n",
       " 'log_return2_std',\n",
       " 'wap_balance_sum',\n",
       " 'wap_balance_mean',\n",
       " 'wap_balance_std',\n",
       " 'price_spread_sum',\n",
       " 'price_spread_mean',\n",
       " 'price_spread_std',\n",
       " 'price_spread2_sum',\n",
       " 'price_spread2_mean',\n",
       " 'price_spread2_std',\n",
       " 'bid_spread_sum',\n",
       " 'bid_spread_mean',\n",
       " 'bid_spread_std',\n",
       " 'ask_spread_sum',\n",
       " 'ask_spread_mean',\n",
       " 'ask_spread_std',\n",
       " 'total_volume_sum',\n",
       " 'total_volume_mean',\n",
       " 'total_volume_std',\n",
       " 'volume_imbalance_sum',\n",
       " 'volume_imbalance_mean',\n",
       " 'volume_imbalance_std',\n",
       " 'bid_ask_spread_sum',\n",
       " 'bid_ask_spread_mean',\n",
       " 'bid_ask_spread_std',\n",
       " 'wap1_sum_400',\n",
       " 'wap1_mean_400',\n",
       " 'wap1_std_400',\n",
       " 'wap2_sum_400',\n",
       " 'wap2_mean_400',\n",
       " 'wap2_std_400',\n",
       " 'log_return1_sum_400',\n",
       " 'log_return1_realized_volatility_400',\n",
       " 'log_return1_mean_400',\n",
       " 'log_return1_std_400',\n",
       " 'log_return2_sum_400',\n",
       " 'log_return2_realized_volatility_400',\n",
       " 'log_return2_mean_400',\n",
       " 'log_return2_std_400',\n",
       " 'wap_balance_sum_400',\n",
       " 'wap_balance_mean_400',\n",
       " 'wap_balance_std_400',\n",
       " 'price_spread_sum_400',\n",
       " 'price_spread_mean_400',\n",
       " 'price_spread_std_400',\n",
       " 'price_spread2_sum_400',\n",
       " 'price_spread2_mean_400',\n",
       " 'price_spread2_std_400',\n",
       " 'bid_spread_sum_400',\n",
       " 'bid_spread_mean_400',\n",
       " 'bid_spread_std_400',\n",
       " 'ask_spread_sum_400',\n",
       " 'ask_spread_mean_400',\n",
       " 'ask_spread_std_400',\n",
       " 'total_volume_sum_400',\n",
       " 'total_volume_mean_400',\n",
       " 'total_volume_std_400',\n",
       " 'volume_imbalance_sum_400',\n",
       " 'volume_imbalance_mean_400',\n",
       " 'volume_imbalance_std_400',\n",
       " 'bid_ask_spread_sum_400',\n",
       " 'bid_ask_spread_mean_400',\n",
       " 'bid_ask_spread_std_400',\n",
       " 'wap1_sum_300',\n",
       " 'wap1_mean_300',\n",
       " 'wap1_std_300',\n",
       " 'wap2_sum_300',\n",
       " 'wap2_mean_300',\n",
       " 'wap2_std_300',\n",
       " 'log_return1_sum_300',\n",
       " 'log_return1_realized_volatility_300',\n",
       " 'log_return1_mean_300',\n",
       " 'log_return1_std_300',\n",
       " 'log_return2_sum_300',\n",
       " 'log_return2_realized_volatility_300',\n",
       " 'log_return2_mean_300',\n",
       " 'log_return2_std_300',\n",
       " 'wap_balance_sum_300',\n",
       " 'wap_balance_mean_300',\n",
       " 'wap_balance_std_300',\n",
       " 'price_spread_sum_300',\n",
       " 'price_spread_mean_300',\n",
       " 'price_spread_std_300',\n",
       " 'price_spread2_sum_300',\n",
       " 'price_spread2_mean_300',\n",
       " 'price_spread2_std_300',\n",
       " 'bid_spread_sum_300',\n",
       " 'bid_spread_mean_300',\n",
       " 'bid_spread_std_300',\n",
       " 'ask_spread_sum_300',\n",
       " 'ask_spread_mean_300',\n",
       " 'ask_spread_std_300',\n",
       " 'total_volume_sum_300',\n",
       " 'total_volume_mean_300',\n",
       " 'total_volume_std_300',\n",
       " 'volume_imbalance_sum_300',\n",
       " 'volume_imbalance_mean_300',\n",
       " 'volume_imbalance_std_300',\n",
       " 'bid_ask_spread_sum_300',\n",
       " 'bid_ask_spread_mean_300',\n",
       " 'bid_ask_spread_std_300',\n",
       " 'wap1_sum_200',\n",
       " 'wap1_mean_200',\n",
       " 'wap1_std_200',\n",
       " 'wap2_sum_200',\n",
       " 'wap2_mean_200',\n",
       " 'wap2_std_200',\n",
       " 'log_return1_sum_200',\n",
       " 'log_return1_realized_volatility_200',\n",
       " 'log_return1_mean_200',\n",
       " 'log_return1_std_200',\n",
       " 'log_return2_sum_200',\n",
       " 'log_return2_realized_volatility_200',\n",
       " 'log_return2_mean_200',\n",
       " 'log_return2_std_200',\n",
       " 'wap_balance_sum_200',\n",
       " 'wap_balance_mean_200',\n",
       " 'wap_balance_std_200',\n",
       " 'price_spread_sum_200',\n",
       " 'price_spread_mean_200',\n",
       " 'price_spread_std_200',\n",
       " 'price_spread2_sum_200',\n",
       " 'price_spread2_mean_200',\n",
       " 'price_spread2_std_200',\n",
       " 'bid_spread_sum_200',\n",
       " 'bid_spread_mean_200',\n",
       " 'bid_spread_std_200',\n",
       " 'ask_spread_sum_200',\n",
       " 'ask_spread_mean_200',\n",
       " 'ask_spread_std_200',\n",
       " 'total_volume_sum_200',\n",
       " 'total_volume_mean_200',\n",
       " 'total_volume_std_200',\n",
       " 'volume_imbalance_sum_200',\n",
       " 'volume_imbalance_mean_200',\n",
       " 'volume_imbalance_std_200',\n",
       " 'bid_ask_spread_sum_200',\n",
       " 'bid_ask_spread_mean_200',\n",
       " 'bid_ask_spread_std_200',\n",
       " 'trade_log_return_realized_volatility',\n",
       " 'trade_seconds_in_bucket_count_unique',\n",
       " 'trade_size_sum',\n",
       " 'trade_size_realized_volatility',\n",
       " 'trade_size_mean',\n",
       " 'trade_size_std',\n",
       " 'trade_size_amax',\n",
       " 'trade_size_amin',\n",
       " 'trade_order_count_mean',\n",
       " 'trade_order_count_sum',\n",
       " 'trade_order_count_amax',\n",
       " 'trade_tendency',\n",
       " 'trade_f_max',\n",
       " 'trade_f_min',\n",
       " 'trade_df_max',\n",
       " 'trade_df_min',\n",
       " 'trade_abs_diff',\n",
       " 'trade_energy',\n",
       " 'trade_iqr_p',\n",
       " 'trade_abs_diff_v',\n",
       " 'trade_energy_v',\n",
       " 'trade_iqr_p_v',\n",
       " 'trade_log_return_realized_volatility_400',\n",
       " 'trade_seconds_in_bucket_count_unique_400',\n",
       " 'trade_size_sum_400',\n",
       " 'trade_size_realized_volatility_400',\n",
       " 'trade_size_mean_400',\n",
       " 'trade_size_std_400',\n",
       " 'trade_size_amax_400',\n",
       " 'trade_size_amin_400',\n",
       " 'trade_order_count_mean_400',\n",
       " 'trade_order_count_sum_400',\n",
       " 'trade_order_count_amax_400',\n",
       " 'trade_log_return_realized_volatility_300',\n",
       " 'trade_seconds_in_bucket_count_unique_300',\n",
       " 'trade_size_sum_300',\n",
       " 'trade_size_realized_volatility_300',\n",
       " 'trade_size_mean_300',\n",
       " 'trade_size_std_300',\n",
       " 'trade_size_amax_300',\n",
       " 'trade_size_amin_300',\n",
       " 'trade_order_count_mean_300',\n",
       " 'trade_order_count_sum_300',\n",
       " 'trade_order_count_amax_300',\n",
       " 'trade_log_return_realized_volatility_200',\n",
       " 'trade_seconds_in_bucket_count_unique_200',\n",
       " 'trade_size_sum_200',\n",
       " 'trade_size_realized_volatility_200',\n",
       " 'trade_size_mean_200',\n",
       " 'trade_size_std_200',\n",
       " 'trade_size_amax_200',\n",
       " 'trade_size_amin_200',\n",
       " 'trade_order_count_mean_200',\n",
       " 'trade_order_count_sum_200',\n",
       " 'trade_order_count_amax_200',\n",
       " 'size_tau',\n",
       " 'size_tau2',\n",
       " 'size_tau_200',\n",
       " 'size_tau2_200',\n",
       " 'size_tau_300',\n",
       " 'size_tau2_300',\n",
       " 'size_tau_400',\n",
       " 'size_tau2_400',\n",
       " 'size_tau2_d',\n",
       " 'log_return1_realized_volatility_mean_stock',\n",
       " 'log_return1_realized_volatility_std_stock',\n",
       " 'log_return1_realized_volatility_max_stock',\n",
       " 'log_return1_realized_volatility_min_stock',\n",
       " 'log_return2_realized_volatility_mean_stock',\n",
       " 'log_return2_realized_volatility_std_stock',\n",
       " 'log_return2_realized_volatility_max_stock',\n",
       " 'log_return2_realized_volatility_min_stock',\n",
       " 'log_return1_realized_volatility_400_mean_stock',\n",
       " 'log_return1_realized_volatility_400_std_stock',\n",
       " 'log_return1_realized_volatility_400_max_stock',\n",
       " 'log_return1_realized_volatility_400_min_stock',\n",
       " 'log_return2_realized_volatility_400_mean_stock',\n",
       " 'log_return2_realized_volatility_400_std_stock',\n",
       " 'log_return2_realized_volatility_400_max_stock',\n",
       " 'log_return2_realized_volatility_400_min_stock',\n",
       " 'log_return1_realized_volatility_300_mean_stock',\n",
       " 'log_return1_realized_volatility_300_std_stock',\n",
       " 'log_return1_realized_volatility_300_max_stock',\n",
       " 'log_return1_realized_volatility_300_min_stock',\n",
       " 'log_return2_realized_volatility_300_mean_stock',\n",
       " 'log_return2_realized_volatility_300_std_stock',\n",
       " 'log_return2_realized_volatility_300_max_stock',\n",
       " 'log_return2_realized_volatility_300_min_stock',\n",
       " 'log_return1_realized_volatility_200_mean_stock',\n",
       " 'log_return1_realized_volatility_200_std_stock',\n",
       " 'log_return1_realized_volatility_200_max_stock',\n",
       " 'log_return1_realized_volatility_200_min_stock',\n",
       " 'log_return2_realized_volatility_200_mean_stock',\n",
       " 'log_return2_realized_volatility_200_std_stock',\n",
       " 'log_return2_realized_volatility_200_max_stock',\n",
       " 'log_return2_realized_volatility_200_min_stock',\n",
       " 'trade_log_return_realized_volatility_mean_stock',\n",
       " 'trade_log_return_realized_volatility_std_stock',\n",
       " 'trade_log_return_realized_volatility_max_stock',\n",
       " 'trade_log_return_realized_volatility_min_stock',\n",
       " 'trade_log_return_realized_volatility_400_mean_stock',\n",
       " 'trade_log_return_realized_volatility_400_std_stock',\n",
       " 'trade_log_return_realized_volatility_400_max_stock',\n",
       " 'trade_log_return_realized_volatility_400_min_stock',\n",
       " 'trade_log_return_realized_volatility_300_mean_stock',\n",
       " 'trade_log_return_realized_volatility_300_std_stock',\n",
       " 'trade_log_return_realized_volatility_300_max_stock',\n",
       " 'trade_log_return_realized_volatility_300_min_stock',\n",
       " 'trade_log_return_realized_volatility_200_mean_stock',\n",
       " 'trade_log_return_realized_volatility_200_std_stock',\n",
       " 'trade_log_return_realized_volatility_200_max_stock',\n",
       " 'trade_log_return_realized_volatility_200_min_stock',\n",
       " 'log_return1_realized_volatility_mean_time',\n",
       " 'log_return1_realized_volatility_std_time',\n",
       " 'log_return1_realized_volatility_max_time',\n",
       " 'log_return1_realized_volatility_min_time',\n",
       " 'log_return2_realized_volatility_mean_time',\n",
       " 'log_return2_realized_volatility_std_time',\n",
       " 'log_return2_realized_volatility_max_time',\n",
       " 'log_return2_realized_volatility_min_time',\n",
       " 'log_return1_realized_volatility_400_mean_time',\n",
       " 'log_return1_realized_volatility_400_std_time',\n",
       " 'log_return1_realized_volatility_400_max_time',\n",
       " 'log_return1_realized_volatility_400_min_time',\n",
       " 'log_return2_realized_volatility_400_mean_time',\n",
       " 'log_return2_realized_volatility_400_std_time',\n",
       " 'log_return2_realized_volatility_400_max_time',\n",
       " 'log_return2_realized_volatility_400_min_time',\n",
       " 'log_return1_realized_volatility_300_mean_time',\n",
       " 'log_return1_realized_volatility_300_std_time',\n",
       " 'log_return1_realized_volatility_300_max_time',\n",
       " 'log_return1_realized_volatility_300_min_time',\n",
       " 'log_return2_realized_volatility_300_mean_time',\n",
       " 'log_return2_realized_volatility_300_std_time',\n",
       " 'log_return2_realized_volatility_300_max_time',\n",
       " 'log_return2_realized_volatility_300_min_time',\n",
       " 'log_return1_realized_volatility_200_mean_time',\n",
       " 'log_return1_realized_volatility_200_std_time',\n",
       " 'log_return1_realized_volatility_200_max_time',\n",
       " 'log_return1_realized_volatility_200_min_time',\n",
       " 'log_return2_realized_volatility_200_mean_time',\n",
       " 'log_return2_realized_volatility_200_std_time',\n",
       " 'log_return2_realized_volatility_200_max_time',\n",
       " 'log_return2_realized_volatility_200_min_time',\n",
       " 'trade_log_return_realized_volatility_mean_time',\n",
       " 'trade_log_return_realized_volatility_std_time',\n",
       " 'trade_log_return_realized_volatility_max_time',\n",
       " 'trade_log_return_realized_volatility_min_time',\n",
       " 'trade_log_return_realized_volatility_400_mean_time',\n",
       " 'trade_log_return_realized_volatility_400_std_time',\n",
       " 'trade_log_return_realized_volatility_400_max_time',\n",
       " 'trade_log_return_realized_volatility_400_min_time',\n",
       " 'trade_log_return_realized_volatility_300_mean_time',\n",
       " 'trade_log_return_realized_volatility_300_std_time',\n",
       " 'trade_log_return_realized_volatility_300_max_time',\n",
       " 'trade_log_return_realized_volatility_300_min_time',\n",
       " 'trade_log_return_realized_volatility_200_mean_time',\n",
       " 'trade_log_return_realized_volatility_200_std_time',\n",
       " 'trade_log_return_realized_volatility_200_max_time',\n",
       " 'trade_log_return_realized_volatility_200_min_time',\n",
       " 'log_return1_realized_volatility_0c1',\n",
       " 'log_return1_realized_volatility_1c1',\n",
       " 'log_return1_realized_volatility_3c1',\n",
       " 'log_return1_realized_volatility_4c1',\n",
       " 'log_return1_realized_volatility_6c1',\n",
       " 'price_spread_mean_0c1',\n",
       " 'price_spread_mean_1c1',\n",
       " 'price_spread_mean_3c1',\n",
       " 'price_spread_mean_4c1',\n",
       " 'price_spread_mean_6c1',\n",
       " 'bid_spread_mean_0c1',\n",
       " 'bid_spread_mean_1c1',\n",
       " 'bid_spread_mean_3c1',\n",
       " 'bid_spread_mean_4c1',\n",
       " 'bid_spread_mean_6c1',\n",
       " 'ask_spread_mean_0c1',\n",
       " 'ask_spread_mean_1c1',\n",
       " 'ask_spread_mean_3c1',\n",
       " 'ask_spread_mean_4c1',\n",
       " 'ask_spread_mean_6c1',\n",
       " 'total_volume_mean_0c1',\n",
       " 'total_volume_mean_1c1',\n",
       " 'total_volume_mean_3c1',\n",
       " 'total_volume_mean_4c1',\n",
       " 'total_volume_mean_6c1',\n",
       " 'volume_imbalance_mean_0c1',\n",
       " 'volume_imbalance_mean_1c1',\n",
       " 'volume_imbalance_mean_3c1',\n",
       " 'volume_imbalance_mean_4c1',\n",
       " 'volume_imbalance_mean_6c1',\n",
       " 'bid_ask_spread_mean_0c1',\n",
       " 'bid_ask_spread_mean_1c1',\n",
       " 'bid_ask_spread_mean_3c1',\n",
       " 'bid_ask_spread_mean_4c1',\n",
       " 'bid_ask_spread_mean_6c1',\n",
       " 'trade_size_mean_0c1',\n",
       " 'trade_size_mean_1c1',\n",
       " 'trade_size_mean_3c1',\n",
       " 'trade_size_mean_4c1',\n",
       " 'trade_size_mean_6c1',\n",
       " 'trade_order_count_mean_0c1',\n",
       " 'trade_order_count_mean_1c1',\n",
       " 'trade_order_count_mean_3c1',\n",
       " 'trade_order_count_mean_4c1',\n",
       " 'trade_order_count_mean_6c1',\n",
       " 'size_tau2_0c1',\n",
       " 'size_tau2_1c1',\n",
       " 'size_tau2_3c1',\n",
       " 'size_tau2_4c1',\n",
       " 'size_tau2_6c1']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c884537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 5.2175  | val_0_rmspe: 0.34984 |  0:00:37s\n",
      "epoch 1  | loss: 0.25937 | val_0_rmspe: 0.3049  |  0:01:13s\n",
      "epoch 2  | loss: 0.24757 | val_0_rmspe: 0.23969 |  0:01:49s\n",
      "epoch 3  | loss: 0.23498 | val_0_rmspe: 0.24236 |  0:02:25s\n",
      "epoch 4  | loss: 0.23202 | val_0_rmspe: 0.27555 |  0:03:00s\n",
      "epoch 5  | loss: 0.22771 | val_0_rmspe: 0.22234 |  0:03:36s\n",
      "epoch 6  | loss: 0.23246 | val_0_rmspe: 0.35818 |  0:04:11s\n",
      "epoch 7  | loss: 0.26744 | val_0_rmspe: 0.28444 |  0:04:47s\n",
      "epoch 8  | loss: 0.39606 | val_0_rmspe: 0.24779 |  0:05:22s\n",
      "epoch 9  | loss: 0.22533 | val_0_rmspe: 0.21325 |  0:05:57s\n",
      "epoch 10 | loss: 0.22378 | val_0_rmspe: 0.22174 |  0:06:32s\n",
      "epoch 11 | loss: 0.22957 | val_0_rmspe: 0.23348 |  0:07:07s\n",
      "epoch 12 | loss: 0.22213 | val_0_rmspe: 0.24243 |  0:07:42s\n",
      "epoch 13 | loss: 0.22249 | val_0_rmspe: 0.20983 |  0:08:17s\n",
      "epoch 14 | loss: 0.24455 | val_0_rmspe: 0.23236 |  0:08:52s\n",
      "epoch 15 | loss: 0.22268 | val_0_rmspe: 0.30126 |  0:09:27s\n",
      "epoch 16 | loss: 0.30767 | val_0_rmspe: 0.22851 |  0:10:02s\n",
      "epoch 17 | loss: 0.22569 | val_0_rmspe: 0.36104 |  0:10:37s\n",
      "epoch 18 | loss: 0.22355 | val_0_rmspe: 0.25148 |  0:11:12s\n",
      "epoch 19 | loss: 0.23754 | val_0_rmspe: 0.34006 |  0:11:48s\n",
      "epoch 20 | loss: 0.225   | val_0_rmspe: 0.22276 |  0:12:23s\n",
      "epoch 21 | loss: 0.24318 | val_0_rmspe: 0.24199 |  0:12:58s\n",
      "epoch 22 | loss: 0.25139 | val_0_rmspe: 0.42089 |  0:13:33s\n",
      "epoch 23 | loss: 0.22372 | val_0_rmspe: 0.26032 |  0:14:08s\n",
      "epoch 24 | loss: 0.2274  | val_0_rmspe: 0.53531 |  0:14:43s\n",
      "epoch 25 | loss: 0.22166 | val_0_rmspe: 0.28785 |  0:15:18s\n",
      "epoch 26 | loss: 0.55096 | val_0_rmspe: 0.25731 |  0:15:53s\n",
      "epoch 27 | loss: 0.24514 | val_0_rmspe: 0.29206 |  0:16:28s\n",
      "epoch 28 | loss: 0.23964 | val_0_rmspe: 0.23046 |  0:17:03s\n",
      "epoch 29 | loss: 0.23217 | val_0_rmspe: 0.5115  |  0:17:38s\n",
      "epoch 30 | loss: 0.21893 | val_0_rmspe: 5.51312 |  0:18:13s\n",
      "epoch 31 | loss: 0.21945 | val_0_rmspe: 0.28908 |  0:18:48s\n",
      "epoch 32 | loss: 0.22086 | val_0_rmspe: 0.23469 |  0:19:23s\n",
      "epoch 33 | loss: 0.21772 | val_0_rmspe: 0.23981 |  0:19:58s\n",
      "epoch 34 | loss: 0.2177  | val_0_rmspe: 0.21691 |  0:20:33s\n",
      "epoch 35 | loss: 0.2169  | val_0_rmspe: 0.22455 |  0:21:08s\n",
      "epoch 36 | loss: 0.21833 | val_0_rmspe: 0.22772 |  0:21:43s\n",
      "epoch 37 | loss: 0.33064 | val_0_rmspe: 0.21838 |  0:22:18s\n",
      "epoch 38 | loss: 0.2187  | val_0_rmspe: 0.24035 |  0:22:53s\n",
      "epoch 39 | loss: 0.21608 | val_0_rmspe: 0.26108 |  0:23:28s\n",
      "epoch 40 | loss: 0.21583 | val_0_rmspe: 0.2124  |  0:24:03s\n",
      "epoch 41 | loss: 0.21903 | val_0_rmspe: 0.32247 |  0:24:38s\n",
      "epoch 42 | loss: 0.21627 | val_0_rmspe: 0.22102 |  0:25:13s\n",
      "epoch 43 | loss: 0.21618 | val_0_rmspe: 0.50276 |  0:25:48s\n",
      "epoch 44 | loss: 0.21492 | val_0_rmspe: 0.21632 |  0:26:23s\n",
      "epoch 45 | loss: 0.21579 | val_0_rmspe: 0.25151 |  0:26:58s\n",
      "epoch 46 | loss: 0.222   | val_0_rmspe: 0.22241 |  0:27:33s\n",
      "epoch 47 | loss: 0.2153  | val_0_rmspe: 0.21559 |  0:28:08s\n",
      "epoch 48 | loss: 0.22097 | val_0_rmspe: 0.24688 |  0:28:42s\n",
      "epoch 49 | loss: 0.22306 | val_0_rmspe: 3.27884 |  0:29:17s\n",
      "epoch 50 | loss: 0.21657 | val_0_rmspe: 2.65952 |  0:29:52s\n",
      "epoch 51 | loss: 0.21823 | val_0_rmspe: 0.22918 |  0:30:27s\n",
      "epoch 52 | loss: 0.21449 | val_0_rmspe: 0.22017 |  0:31:02s\n",
      "epoch 53 | loss: 0.2147  | val_0_rmspe: 0.25484 |  0:31:37s\n",
      "epoch 54 | loss: 0.2136  | val_0_rmspe: 0.44375 |  0:32:13s\n",
      "epoch 55 | loss: 0.21493 | val_0_rmspe: 0.22893 |  0:32:48s\n",
      "epoch 56 | loss: 0.217   | val_0_rmspe: 0.33041 |  0:33:23s\n",
      "epoch 57 | loss: 0.21392 | val_0_rmspe: 0.32849 |  0:33:58s\n",
      "epoch 58 | loss: 0.29422 | val_0_rmspe: 68.02117|  0:34:33s\n",
      "epoch 59 | loss: 0.23142 | val_0_rmspe: 10.8677 |  0:35:08s\n",
      "epoch 60 | loss: 0.21442 | val_0_rmspe: 254.26554|  0:35:43s\n",
      "epoch 61 | loss: 0.21468 | val_0_rmspe: 67.48055|  0:36:18s\n",
      "epoch 62 | loss: 0.21606 | val_0_rmspe: 0.33566 |  0:36:53s\n",
      "epoch 63 | loss: 0.23441 | val_0_rmspe: 36.8934 |  0:37:29s\n",
      "\n",
      "Early stopping occurred at epoch 63 with best_epoch = 13 and best_val_0_rmspe = 0.20983\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at ./fold0.zip\n",
      "Training fold 2\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 6.15762 | val_0_rmspe: 0.26025 |  0:00:35s\n",
      "epoch 1  | loss: 0.26406 | val_0_rmspe: 0.2425  |  0:01:10s\n",
      "epoch 2  | loss: 0.23774 | val_0_rmspe: 0.25836 |  0:01:46s\n",
      "epoch 3  | loss: 0.23204 | val_0_rmspe: 0.22015 |  0:02:21s\n",
      "epoch 4  | loss: 0.2287  | val_0_rmspe: 0.22771 |  0:02:57s\n",
      "epoch 5  | loss: 0.25946 | val_0_rmspe: 0.26181 |  0:03:32s\n",
      "epoch 6  | loss: 0.22548 | val_0_rmspe: 0.22748 |  0:04:07s\n",
      "epoch 7  | loss: 0.22503 | val_0_rmspe: 0.28613 |  0:04:42s\n",
      "epoch 8  | loss: 0.22438 | val_0_rmspe: 0.21617 |  0:05:17s\n",
      "epoch 9  | loss: 0.22178 | val_0_rmspe: 0.23134 |  0:05:52s\n",
      "epoch 10 | loss: 0.22051 | val_0_rmspe: 0.20944 |  0:06:28s\n",
      "epoch 11 | loss: 0.21939 | val_0_rmspe: 0.23787 |  0:07:04s\n",
      "epoch 12 | loss: 0.21927 | val_0_rmspe: 0.21728 |  0:07:39s\n",
      "epoch 13 | loss: 0.21853 | val_0_rmspe: 0.20797 |  0:08:15s\n",
      "epoch 14 | loss: 0.21893 | val_0_rmspe: 0.20919 |  0:08:50s\n",
      "epoch 15 | loss: 0.21773 | val_0_rmspe: 0.20634 |  0:09:25s\n",
      "epoch 16 | loss: 0.2174  | val_0_rmspe: 0.2175  |  0:10:00s\n",
      "epoch 17 | loss: 0.22617 | val_0_rmspe: 0.21384 |  0:10:35s\n",
      "epoch 18 | loss: 0.21736 | val_0_rmspe: 0.20613 |  0:11:11s\n",
      "epoch 19 | loss: 0.21575 | val_0_rmspe: 0.20913 |  0:11:46s\n",
      "epoch 20 | loss: 0.21625 | val_0_rmspe: 0.2159  |  0:12:21s\n",
      "epoch 21 | loss: 0.21526 | val_0_rmspe: 0.20322 |  0:12:56s\n",
      "epoch 22 | loss: 0.21526 | val_0_rmspe: 0.26474 |  0:13:31s\n",
      "epoch 23 | loss: 0.2144  | val_0_rmspe: 0.20993 |  0:14:06s\n",
      "epoch 24 | loss: 0.21428 | val_0_rmspe: 0.21393 |  0:14:41s\n",
      "epoch 25 | loss: 0.2133  | val_0_rmspe: 0.22647 |  0:15:17s\n",
      "epoch 26 | loss: 0.21289 | val_0_rmspe: 0.22301 |  0:15:52s\n",
      "epoch 27 | loss: 0.22861 | val_0_rmspe: 1.0261  |  0:16:27s\n",
      "epoch 28 | loss: 0.21803 | val_0_rmspe: 0.22681 |  0:17:02s\n",
      "epoch 29 | loss: 0.21262 | val_0_rmspe: 0.2068  |  0:17:37s\n",
      "epoch 30 | loss: 0.21157 | val_0_rmspe: 0.2152  |  0:18:14s\n",
      "epoch 31 | loss: 0.21144 | val_0_rmspe: 0.21454 |  0:18:58s\n",
      "epoch 32 | loss: 0.21245 | val_0_rmspe: 0.2227  |  0:19:44s\n",
      "epoch 33 | loss: 0.21128 | val_0_rmspe: 0.20558 |  0:20:45s\n",
      "epoch 34 | loss: 0.21053 | val_0_rmspe: 0.20715 |  0:21:50s\n",
      "epoch 35 | loss: 0.21013 | val_0_rmspe: 0.20729 |  0:22:52s\n",
      "epoch 36 | loss: 0.20984 | val_0_rmspe: 0.21621 |  0:23:42s\n",
      "epoch 37 | loss: 0.21002 | val_0_rmspe: 0.20106 |  0:24:33s\n",
      "epoch 38 | loss: 0.22127 | val_0_rmspe: 0.82416 |  0:25:35s\n",
      "epoch 39 | loss: 0.23587 | val_0_rmspe: 6.14191 |  0:26:40s\n",
      "epoch 40 | loss: 0.25755 | val_0_rmspe: 0.2127  |  0:27:39s\n",
      "epoch 41 | loss: 0.20922 | val_0_rmspe: 0.2002  |  0:28:47s\n",
      "epoch 42 | loss: 0.2083  | val_0_rmspe: 0.21442 |  0:29:43s\n",
      "epoch 43 | loss: 0.20953 | val_0_rmspe: 0.21291 |  0:30:31s\n",
      "epoch 44 | loss: 0.20884 | val_0_rmspe: 0.20522 |  0:31:29s\n",
      "epoch 45 | loss: 0.20915 | val_0_rmspe: 0.21699 |  0:32:21s\n",
      "epoch 46 | loss: 0.20763 | val_0_rmspe: 0.61913 |  0:33:16s\n",
      "epoch 47 | loss: 0.20946 | val_0_rmspe: 0.20827 |  0:34:13s\n",
      "epoch 48 | loss: 0.20706 | val_0_rmspe: 0.21198 |  0:35:09s\n",
      "epoch 49 | loss: 0.20624 | val_0_rmspe: 0.19931 |  0:35:57s\n",
      "epoch 50 | loss: 0.20633 | val_0_rmspe: 0.20767 |  0:36:39s\n",
      "epoch 51 | loss: 0.20624 | val_0_rmspe: 0.21119 |  0:37:19s\n",
      "epoch 52 | loss: 0.20578 | val_0_rmspe: 0.23132 |  0:38:06s\n",
      "epoch 53 | loss: 0.2058  | val_0_rmspe: 0.19987 |  0:39:14s\n",
      "epoch 54 | loss: 0.20496 | val_0_rmspe: 0.21253 |  0:40:02s\n",
      "epoch 55 | loss: 0.20543 | val_0_rmspe: 0.19747 |  0:40:45s\n",
      "epoch 56 | loss: 0.20481 | val_0_rmspe: 0.2045  |  0:41:24s\n",
      "epoch 57 | loss: 0.20718 | val_0_rmspe: 0.20551 |  0:42:03s\n",
      "epoch 58 | loss: 0.20806 | val_0_rmspe: 0.20096 |  0:42:44s\n",
      "epoch 59 | loss: 0.206   | val_0_rmspe: 0.20441 |  0:43:29s\n",
      "epoch 60 | loss: 0.20423 | val_0_rmspe: 0.21904 |  0:44:30s\n",
      "epoch 61 | loss: 0.20393 | val_0_rmspe: 0.19928 |  0:45:19s\n",
      "epoch 62 | loss: 0.20378 | val_0_rmspe: 0.20149 |  0:46:14s\n",
      "epoch 63 | loss: 0.20356 | val_0_rmspe: 0.20145 |  0:47:07s\n",
      "epoch 64 | loss: 0.20328 | val_0_rmspe: 0.20412 |  0:48:03s\n",
      "epoch 65 | loss: 0.20292 | val_0_rmspe: 0.20389 |  0:48:53s\n",
      "epoch 66 | loss: 0.20233 | val_0_rmspe: 0.21082 |  0:49:44s\n",
      "epoch 67 | loss: 0.20324 | val_0_rmspe: 0.21312 |  0:50:46s\n",
      "epoch 68 | loss: 0.20321 | val_0_rmspe: 0.20933 |  0:51:49s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 69 | loss: 0.20493 | val_0_rmspe: 0.20559 |  0:52:43s\n",
      "epoch 70 | loss: 0.20248 | val_0_rmspe: 0.19735 |  0:53:42s\n",
      "epoch 71 | loss: 0.20258 | val_0_rmspe: 0.19898 |  0:54:40s\n",
      "epoch 72 | loss: 0.20218 | val_0_rmspe: 0.20147 |  0:55:37s\n",
      "epoch 73 | loss: 0.20219 | val_0_rmspe: 0.20366 |  0:56:31s\n",
      "epoch 74 | loss: 0.20198 | val_0_rmspe: 0.19564 |  0:57:25s\n",
      "epoch 75 | loss: 0.20086 | val_0_rmspe: 0.20115 |  0:58:16s\n",
      "epoch 76 | loss: 0.20093 | val_0_rmspe: 0.20266 |  0:59:13s\n",
      "epoch 77 | loss: 0.2008  | val_0_rmspe: 0.21418 |  1:00:07s\n",
      "epoch 78 | loss: 0.20143 | val_0_rmspe: 0.20203 |  1:01:02s\n",
      "epoch 79 | loss: 0.20855 | val_0_rmspe: 0.2     |  1:01:54s\n",
      "epoch 80 | loss: 0.20022 | val_0_rmspe: 0.20073 |  1:02:47s\n",
      "epoch 81 | loss: 0.20072 | val_0_rmspe: 0.20108 |  1:03:39s\n",
      "epoch 82 | loss: 0.2011  | val_0_rmspe: 0.20972 |  1:04:35s\n",
      "epoch 83 | loss: 0.20036 | val_0_rmspe: 1.31638 |  1:05:30s\n",
      "epoch 84 | loss: 0.19983 | val_0_rmspe: 0.97621 |  1:06:27s\n",
      "epoch 85 | loss: 0.2014  | val_0_rmspe: 0.31097 |  1:07:18s\n",
      "epoch 86 | loss: 0.19928 | val_0_rmspe: 0.63665 |  1:08:13s\n",
      "epoch 87 | loss: 0.199   | val_0_rmspe: 0.70363 |  1:09:13s\n",
      "epoch 88 | loss: 0.47045 | val_0_rmspe: 0.2106  |  1:10:12s\n",
      "epoch 89 | loss: 0.20835 | val_0_rmspe: 0.20028 |  1:11:12s\n",
      "epoch 90 | loss: 0.19865 | val_0_rmspe: 0.2007  |  1:12:13s\n",
      "epoch 91 | loss: 0.19863 | val_0_rmspe: 0.20107 |  1:13:07s\n",
      "epoch 92 | loss: 0.19938 | val_0_rmspe: 0.19934 |  1:14:22s\n",
      "epoch 93 | loss: 0.19924 | val_0_rmspe: 0.20524 |  1:15:35s\n",
      "epoch 94 | loss: 0.19944 | val_0_rmspe: 0.21269 |  1:16:31s\n",
      "epoch 95 | loss: 0.19814 | val_0_rmspe: 0.19668 |  1:17:38s\n",
      "epoch 96 | loss: 0.19802 | val_0_rmspe: 0.20535 |  1:18:32s\n",
      "epoch 97 | loss: 0.1979  | val_0_rmspe: 0.21443 |  1:19:23s\n",
      "epoch 98 | loss: 0.19879 | val_0_rmspe: 0.20167 |  1:20:12s\n",
      "epoch 99 | loss: 0.28818 | val_0_rmspe: 0.19608 |  1:20:53s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 74 and best_val_0_rmspe = 0.19564\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at ./fold1.zip\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values (100) does not match length of index (64)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\RUISAV~1\\AppData\\Local\\Temp/ipykernel_8100/1708113472.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mfeature_importances\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34mf\"importance_fold{fold}+1\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m     \u001b[0mstats\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34mf'fold{fold+1}_train_rmspe'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m     \u001b[0mstats\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34mf'fold{fold+1}_val_rmspe'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_0_rmspe'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Users\\RuiSavior\\anaconda3\\envs\\optiver_kaggle\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3610\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3611\u001b[0m             \u001b[1;31m# set column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3612\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3613\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3614\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Users\\RuiSavior\\anaconda3\\envs\\optiver_kaggle\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3782\u001b[0m         \u001b[0mensure\u001b[0m \u001b[0mhomogeneity\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3783\u001b[0m         \"\"\"\n\u001b[1;32m-> 3784\u001b[1;33m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3785\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3786\u001b[0m         if (\n",
      "\u001b[1;32md:\\Users\\RuiSavior\\anaconda3\\envs\\optiver_kaggle\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   4507\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4508\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4509\u001b[1;33m             \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequire_length_match\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4510\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msanitize_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4511\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Users\\RuiSavior\\anaconda3\\envs\\optiver_kaggle\\lib\\site-packages\\pandas\\core\\common.py\u001b[0m in \u001b[0;36mrequire_length_match\u001b[1;34m(data, index)\u001b[0m\n\u001b[0;32m    529\u001b[0m     \"\"\"\n\u001b[0;32m    530\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m         raise ValueError(\n\u001b[0m\u001b[0;32m    532\u001b[0m             \u001b[1;34m\"Length of values \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m             \u001b[1;34mf\"({len(data)}) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Length of values (100) does not match length of index (64)"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits = 5, random_state = 2021, shuffle = True)\n",
    "# Create out of folds array\n",
    "oof_predictions = np.zeros((X.shape[0], 1))\n",
    "test_predictions = np.zeros(X_test.shape[0])\n",
    "feature_importances = pd.DataFrame()\n",
    "feature_importances[\"feature\"] = X.columns.tolist()\n",
    "stats = pd.DataFrame()\n",
    "explain_matrices = []\n",
    "masks_ = []\n",
    "\n",
    "for fold,(trn_ind, val_ind) in enumerate(kfold.split(X)):\n",
    "    print(f'Training fold {fold + 1}')\n",
    "    X_train, X_val = X.iloc[trn_ind].values, X.iloc[val_ind].values\n",
    "    y_train, y_val = y.iloc[trn_ind].values.reshape(-1,1), y.iloc[val_ind].values.reshape(-1,1)\n",
    "    clf = TabNetRegressor(**tabnet_params)\n",
    "    clf.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    max_epochs = 100,\n",
    "    patience = 50,\n",
    "    batch_size = 128*2,\n",
    "    virtual_batch_size = 128*1,\n",
    "    num_workers = 0,\n",
    "    drop_last = False,\n",
    "    eval_metric = [RMSPE],\n",
    "    loss_fn = RMSPELoss\n",
    "    \n",
    "    )\n",
    "    saving_path_name = f\"./fold{fold}\"\n",
    "    saved_filepath = clf.save_model(saving_path_name)\n",
    "    \n",
    "    explain_matrix, masks = clf.explain(X_val)\n",
    "    explain_matrices.append(explain_matrix)\n",
    "    masks_.append(masks[0])\n",
    "    masks_.append(masks[1])\n",
    "      \n",
    "    oof_predictions[val_ind] = clf.predict(X_val)\n",
    "    test_predictions+=clf.predict(X_test.values).flatten()/5\n",
    "    feature_importances[f\"importance_fold{fold}+1\"] = clf.feature_importances_\n",
    "    \n",
    "    stats[f'fold{fold+1}_train_rmspe']=clf.history['loss']\n",
    "    stats[f'fold{fold+1}_val_rmspe']=clf.history['val_0_rmspe']\n",
    "    \n",
    "print(f'OOF score across folds: {rmspe(y, oof_predictions.flatten())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f883ff5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:optiver_kaggle]",
   "language": "python",
   "name": "conda-env-optiver_kaggle-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
