{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import glob\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "from itertools import islice\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Lambda, Dot, Activation, Concatenate\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import psutil\n",
    "psutil.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NTHREADS = psutil.cpu_count()-2\n",
    "SEED = 42\n",
    "TRAIN_BATCH_SIZE = 256\n",
    "TEST_BATCH_SIZE = 256\n",
    "BUCKET_WINDOWS2 = [(0, 100), (100, 200), (200, 300), (300, 400), (400, 500), (500, 600)]\n",
    "\n",
    "DATA_PATH = 'input/optiver-realized-volatility-prediction'\n",
    "BOOK_TRAIN_PATH = 'input/optiver-realized-volatility-prediction/book_train.parquet'\n",
    "TRADE_TRAIN_PATH = 'input/optiver-realized-volatility-prediction/trade_train.parquet'\n",
    "BOOK_TEST_PATH = 'input/optiver-realized-volatility-prediction/book_test.parquet'\n",
    "TRADE_TEST_PATH = 'input/optiver-realized-volatility-prediction/trade_test.parquet'\n",
    "CHECKPOINT = 'model_checkpoint/model_01'\n",
    "\n",
    "book_skip_columns = trade_skip_columns = ['time_id', 'row_id', 'target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "data_input = open('LSTMtemp/np_train.pkl','rb')\n",
    "np_train = pickle.load(data_input)\n",
    "data_input.close()\n",
    "\n",
    "data_input = open('LSTMtemp/targets.pkl','rb')\n",
    "targets = pickle.load(data_input)\n",
    "data_input.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.arange(np_train.shape[0])\n",
    "train_idx, valid_idx = train_test_split(idx, shuffle=False, test_size=0.1, random_state=2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaler\n",
    "transformers = []\n",
    "for i in tqdm(range(np_train.shape[1])):\n",
    "    a = np.nan_to_num(np_train[train_idx, i, :])\n",
    "    b = np.nan_to_num(np_train[valid_idx, i, :])\n",
    "\n",
    "    transformer = StandardScaler() # StandardScaler is very useful!\n",
    "    np_train[train_idx, i, :] = transformer.fit_transform(a)\n",
    "    np_train[valid_idx, i, :] = transformer.transform(b)\n",
    "    transformers.append(transformer) # Save Scalers for the inference stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np_train = np.nan_to_num(np_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pickle\n",
    "data_output = open('LSTMtemp/np_train.pkl','wb')\n",
    "pickle.dump(np_train,data_output)\n",
    "data_output.close()\n",
    "\n",
    "data_output = open('LSTMtemp/targets.pkl','wb')\n",
    "pickle.dump(targets,data_output)\n",
    "data_output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "def rmspe(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square((y_true - y_pred) / y_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/philipperemy/keras-attention-mechanism\n",
    "class Attention(Layer):\n",
    "\n",
    "    def __init__(self, units=128, **kwargs):\n",
    "        self.units = units\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        \"\"\"\n",
    "        Many-to-one attention mechanism for Keras.\n",
    "        @param inputs: 3D tensor with shape (batch_size, time_steps, input_dim).\n",
    "        @return: 2D tensor with shape (batch_size, 128)\n",
    "        @author: felixhao28, philipperemy.\n",
    "        \"\"\"\n",
    "        hidden_states = inputs\n",
    "        hidden_size = int(hidden_states.shape[2])\n",
    "        print('h_t shape:',hidden_states.shape)\n",
    "        # Inside dense layer\n",
    "        #              hidden_states            dot               W            =>           score_first_part\n",
    "        # (batch_size, time_steps, hidden_size) dot (hidden_size, hidden_size) => (batch_size, time_steps, hidden_size)\n",
    "        # W is the trainable weight matrix of attention Luong's multiplicative style score\n",
    "        score_first_part = Dense(hidden_size, use_bias=False, name='attention_score_vec')(hidden_states)\n",
    "        #            score_first_part           dot        last_hidden_state     => attention_weights\n",
    "        # (batch_size, time_steps, hidden_size) dot   (batch_size, hidden_size)  => (batch_size, time_steps)\n",
    "        h_t = Lambda(lambda x: x[:, -1, :], output_shape=(hidden_size,), name='last_hidden_state')(hidden_states)\n",
    "        score = Dot(axes=[1, 2], name='attention_score')([h_t, score_first_part])\n",
    "        attention_weights = Activation('softmax', name='attention_weight')(score)\n",
    "        # (batch_size, time_steps, hidden_size) dot (batch_size, time_steps) => (batch_size, hidden_size)\n",
    "        context_vector = Dot(axes=[1, 1], name='context_vector')([hidden_states, attention_weights])\n",
    "        pre_activation = Concatenate(name='attention_output')([context_vector, h_t])\n",
    "        attention_vector = Dense(self.units, use_bias=False, activation='tanh', name='attention_vector')(pre_activation)\n",
    "        return attention_vector\n",
    "\n",
    "    def get_config(self):\n",
    "        return {'units': self.units}\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, ds, targets, batch_size, shape=(32,32,32), shuffle=True):\n",
    "        'Initialization'\n",
    "        self.batch_size = batch_size\n",
    "        self.targets = targets\n",
    "        self.shape = shape\n",
    "        self.ds = ds\n",
    "        self.ids = np.arange(ds.shape[0])\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.ids) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.ids[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        ids_temp = [self.ids[k] for k in indexes]\n",
    "\n",
    "\n",
    "        x = self.ds[ids_temp, :, :]\n",
    "        y = self.targets[ids_temp]\n",
    "        \n",
    "        return x, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.ids = np.arange(self.ds.shape[0])\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_v1():\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.LSTM(50, input_shape=(np_train.shape[1], np_train.shape[2]), return_sequences=False))\n",
    "#     model.add(tf.keras.layers.LSTM(50, input_shape=(np_train.shape[1], np_train.shape[2]), return_sequences=False))\n",
    "#     model.add(Attention(256)) # the gain is small, but ...\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "    model.compile(loss=rmspe, optimizer='adam')\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 50)                16000     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 16,051\n",
      "Trainable params: 16,051\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "training_generator = DataGenerator(np_train[train_idx, :, :], targets[train_idx], batch_size=TRAIN_BATCH_SIZE)\n",
    "validation_generator = DataGenerator(np_train[valid_idx, :, :], targets[valid_idx], batch_size=TRAIN_BATCH_SIZE)\n",
    "\n",
    "model = get_model_v1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1507/1507 [==============================] - 45s 20ms/step - loss: 5.0338 - val_loss: 1.2197\n",
      "Epoch 2/50\n",
      "1507/1507 [==============================] - 29s 19ms/step - loss: 1.5452 - val_loss: 0.6461\n",
      "Epoch 3/50\n",
      "1507/1507 [==============================] - 29s 19ms/step - loss: 0.9308 - val_loss: 0.5977\n",
      "Epoch 4/50\n",
      "1507/1507 [==============================] - 29s 19ms/step - loss: 0.6404 - val_loss: 0.4763\n",
      "Epoch 5/50\n",
      "1507/1507 [==============================] - 29s 19ms/step - loss: 0.4851 - val_loss: 0.3512\n",
      "Epoch 6/50\n",
      "1507/1507 [==============================] - 28s 19ms/step - loss: 0.3948 - val_loss: 0.2609\n",
      "Epoch 7/50\n",
      "1507/1507 [==============================] - 28s 19ms/step - loss: 0.3348 - val_loss: 0.2760\n",
      "Epoch 8/50\n",
      "1507/1507 [==============================] - 29s 19ms/step - loss: 0.2986 - val_loss: 0.2562\n",
      "Epoch 9/50\n",
      "1507/1507 [==============================] - 29s 19ms/step - loss: 0.2780 - val_loss: 0.2807\n",
      "Epoch 10/50\n",
      "1507/1507 [==============================] - 29s 19ms/step - loss: 0.2675 - val_loss: 0.2357\n",
      "Epoch 11/50\n",
      "1507/1507 [==============================] - 29s 19ms/step - loss: 0.2638 - val_loss: 0.2333\n",
      "Epoch 12/50\n",
      "1507/1507 [==============================] - 29s 19ms/step - loss: 0.2613 - val_loss: 0.2392\n",
      "Epoch 13/50\n",
      "1507/1507 [==============================] - 28s 19ms/step - loss: 0.2580 - val_loss: 0.2667\n",
      "Epoch 14/50\n",
      "1507/1507 [==============================] - 28s 19ms/step - loss: 0.2584 - val_loss: 0.2280\n",
      "Epoch 15/50\n",
      "1507/1507 [==============================] - 29s 19ms/step - loss: 0.2563 - val_loss: 0.2277\n",
      "Epoch 16/50\n",
      "1507/1507 [==============================] - 29s 19ms/step - loss: 0.2556 - val_loss: 0.2288\n",
      "Epoch 17/50\n",
      "1507/1507 [==============================] - 29s 19ms/step - loss: 0.2528 - val_loss: 0.2294\n",
      "Epoch 18/50\n",
      "1507/1507 [==============================] - 29s 19ms/step - loss: 0.2546 - val_loss: 0.2308\n",
      "Epoch 19/50\n",
      "  64/1507 [>.............................] - ETA: 27s - loss: 0.2451"
     ]
    }
   ],
   "source": [
    "# training_generator = DataGenerator(np_train[train_idx, :, :], targets[train_idx], batch_size=TRAIN_BATCH_SIZE)\n",
    "# validation_generator = DataGenerator(np_train[valid_idx, :, :], targets[valid_idx], batch_size=TRAIN_BATCH_SIZE)\n",
    "\n",
    "# model = get_model_v1()\n",
    "\n",
    "checkpoint_filepath = CHECKPOINT\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True)\n",
    "\n",
    "model_earlystopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "NEPOCHS = 25\n",
    "history = model.fit_generator(generator=training_generator, \n",
    "                              callbacks=[model_checkpoint_callback, model_earlystopping_callback], \n",
    "                              epochs=NEPOCHS, \n",
    "                              validation_data=validation_generator, \n",
    "                              use_multiprocessing=False, \n",
    "                              workers=NTHREADS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\RUISAV~1\\AppData\\Local\\Temp/ipykernel_28180/2996440943.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'The best val_loss is {a:.4f}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "a = np.min(history.history['val_loss'])\n",
    "print(f'The best val_loss is {a:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np_books' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\RUISAV~1\\AppData\\Local\\Temp/ipykernel_28180/2708967761.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mdel\u001b[0m \u001b[0mnp_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp_books\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp_trades\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np_books' is not defined"
     ]
    }
   ],
   "source": [
    "del np_train, np_books, np_trades\n",
    "z = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pool = Pool(NTHREADS)\n",
    "r = pool.map(process_book_test_chunk, book_test_chunks)\n",
    "pool.close()\n",
    "\n",
    "a1, _ = zip(*r)\n",
    "np_books = [np.concatenate(a1[i], axis=0) for i in range(len(a1))]\n",
    "np_books = np.concatenate(np_books, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pool = Pool(NTHREADS)\n",
    "r = pool.map(process_trade_test_chunk, trade_test_chunks)\n",
    "pool.close()\n",
    "\n",
    "a1, _ = zip(*r)\n",
    "np_trades = [np.concatenate(a1[i], axis=0) for i in range(len(a1))]\n",
    "np_trades = np.concatenate(np_trades, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np_books.shape, np_trades.shape)\n",
    "np_test = np.concatenate((np_books, np_trades), axis=2)\n",
    "print(np_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaler\n",
    "for i in tqdm(range(np_test.shape[1])):\n",
    "    transformer = transformers[i]\n",
    "    np_test[:, i, :] = transformer.transform(np.nan_to_num(np_test[:, i, :]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_test = np.nan_to_num(np_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(checkpoint_filepath)\n",
    "res = model.predict(np_test, batch_size=TEST_BATCH_SIZE)\n",
    "res = np.clip(res, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree('./model_checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf21]",
   "language": "python",
   "name": "conda-env-tf21-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
