{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import glob\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from sklearn import preprocessing, model_selection\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import numpy.matlib\n",
    "\n",
    "\n",
    "path_submissions = '/'\n",
    "\n",
    "target_name = 'target'\n",
    "scores_folds = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "train1 = pd.read_pickle(\"temp/train3.pkl\")\n",
    "test1 = pd.read_pickle(\"temp/test3.pkl\")\n",
    "train_nn = pd.read_pickle(\"temp/train_nn.pkl\")\n",
    "test_nn = pd.read_pickle(\"temp/test_nn.pkl\")\n",
    "\n",
    "# values = []\n",
    "# file=open(r\"temp/values.pkl\",\"rb\")\n",
    "# values = pickle.load(file)\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Act_op(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Act_op, self).__init__()\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = x * torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import EarlyStopping\n",
    "from pytorchtools import EarlyStopping\n",
    "early_stopping = EarlyStopping(20, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#利用pytorch构建NN\n",
    "hidden_units = (128,64,32)\n",
    "stock_embedding_size = 24\n",
    "\n",
    "cat_data = train_nn['stock_id']\n",
    "n_cata = max(cat_data)\n",
    "\n",
    "class NNet(nn.Module):\n",
    "    def __init__(self,n_features,n_cata,stock_em):\n",
    "        super(NNet, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.n_features = n_features\n",
    "        self.n_cata = n_cata\n",
    "        self.stock_em = stock_em\n",
    "        self.Act = Act_op()\n",
    "        self.f1 = torch.nn.Embedding(self.n_cata+1, self.stock_em)\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Linear(24+n_features, 128),\n",
    "            self.Act,\n",
    "            nn.Linear(128, 64),\n",
    "            self.Act,\n",
    "            nn.Linear(64, 32),\n",
    "            self.Act,\n",
    "            nn.Linear(32,1)\n",
    "        )\n",
    "#         self.linear_stack = nn.Linear(24+n_features, 1)\n",
    "\n",
    "    def forward(self, x_cat, x_num):\n",
    "        stock_embedded = self.f1(x_cat)\n",
    "        stock_flattened = self.flatten(stock_embedded)\n",
    "        out = torch.cat((stock_flattened, x_num),1).float()\n",
    "        out = self.linear_stack(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x_cat: np.ndarray, x_num: np.ndarray, y:np.ndarray):\n",
    "        super().__init__()\n",
    "        self.x_cat = x_cat\n",
    "        self.x_num = x_num\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_num)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_1 = torch.LongTensor(self.x_cat[idx:idx+1].values).cuda()\n",
    "        x_2 = torch.from_numpy(self.x_num[idx:idx+1][0]).cuda()\n",
    "        y = torch.LongTensor(self.y[idx:idx+1].values).cuda()\n",
    "#         print('Get data using '+str(time.time90-t)+'s')\n",
    "        return [x_1, x_2], y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, y_true, y_pred):\n",
    "        return torch.sqrt(torch.mean(torch.pow(((y_true-y_pred)/y_pred),2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the root mean squared percentage error\n",
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "# Function to early stop with root mean squared percentage error\n",
    "def feval_rmspe(y_pred, lgb_train):\n",
    "    y_true = lgb_train.get_label()\n",
    "    return 'RMSPE', rmspe(y_true, y_pred), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_name='target'\n",
    "model_name = 'NN'\n",
    "pred_name = 'pred_{}'.format(model_name)\n",
    "\n",
    "n_folds = 5\n",
    "kf = model_selection.KFold(n_splits=n_folds, shuffle=True, random_state=2021)\n",
    "scores_folds= []\n",
    "counter = 1\n",
    "\n",
    "features_to_consider = list(train_nn)\n",
    "\n",
    "features_to_consider.remove('time_id')\n",
    "features_to_consider.remove('target')\n",
    "try:\n",
    "    features_to_consider.remove('pred_NN')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "train_nn[features_to_consider] = train_nn[features_to_consider].fillna(train_nn[features_to_consider].mean())\n",
    "test_nn[features_to_consider] = test_nn[features_to_consider].fillna(train_nn[features_to_consider].mean())\n",
    "\n",
    "train_nn[pred_name] = 0\n",
    "test_nn[target_name] = 0\n",
    "test_predictions_nn = np.zeros(test_nn.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'values' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\RUISAV~1\\AppData\\Local\\Temp/ipykernel_10432/4257179670.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mindexes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_folds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mindexes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mindexes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mr_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindexes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindexes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindexes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindexes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'values' is not defined"
     ]
    }
   ],
   "source": [
    "n_folds = 5\n",
    "n_count = 1\n",
    "indexes = np.arange(n_folds).astype(int)\n",
    "indexes = np.delete(indexes,obj=n_count, axis=0)\n",
    "indexes = np.r_[values[indexes[0]],values[indexes[1]],values[indexes[2]],values[indexes[3]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV 1/5\n",
      "Epoch:  1  \tTraining Loss: 1.002541\n",
      "Epoch:  2  \tTraining Loss: 0.396461\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch:  3  \tTraining Loss: 0.400624\n",
      "Epoch:  4  \tTraining Loss: 0.419714\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch:  5  \tTraining Loss: 0.405101\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch:  6  \tTraining Loss: 0.407024\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch:  7  \tTraining Loss: 0.404739\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch:  8  \tTraining Loss: 0.373764\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch:  9  \tTraining Loss: 0.453036\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch:  10  \tTraining Loss: 0.319108\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch:  11  \tTraining Loss: 0.408104\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch:  12  \tTraining Loss: 0.373327\n",
      "EarlyStopping counter: 9 out of 20\n",
      "Epoch:  13  \tTraining Loss: 0.421930\n",
      "EarlyStopping counter: 10 out of 20\n",
      "Epoch:  14  \tTraining Loss: 0.615720\n",
      "EarlyStopping counter: 11 out of 20\n",
      "Epoch:  15  \tTraining Loss: 0.311919\n",
      "EarlyStopping counter: 12 out of 20\n",
      "Epoch:  16  \tTraining Loss: 0.278554\n",
      "Epoch:  17  \tTraining Loss: 0.258547\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch:  18  \tTraining Loss: 0.251643\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch:  19  \tTraining Loss: 0.251038\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch:  20  \tTraining Loss: 0.250509\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch:  21  \tTraining Loss: 0.258030\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch:  22  \tTraining Loss: 0.258653\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch:  23  \tTraining Loss: 0.266843\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch:  24  \tTraining Loss: 0.266473\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch:  25  \tTraining Loss: 0.253243\n",
      "EarlyStopping counter: 9 out of 20\n",
      "Epoch:  26  \tTraining Loss: 0.253641\n",
      "EarlyStopping counter: 10 out of 20\n",
      "Epoch:  27  \tTraining Loss: 0.258954\n",
      "EarlyStopping counter: 11 out of 20\n",
      "Epoch:  28  \tTraining Loss: 0.250361\n",
      "Epoch:  29  \tTraining Loss: 0.237798\n",
      "Epoch:  30  \tTraining Loss: 0.231511\n",
      "Epoch:  31  \tTraining Loss: 0.227119\n",
      "Epoch:  32  \tTraining Loss: 0.224089\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch:  33  \tTraining Loss: 0.222010\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch:  34  \tTraining Loss: 0.220506\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch:  35  \tTraining Loss: 0.219322\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch:  36  \tTraining Loss: 0.218324\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch:  37  \tTraining Loss: 0.217458\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch:  38  \tTraining Loss: 0.216715\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch:  39  \tTraining Loss: 0.216075\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch:  40  \tTraining Loss: 0.215519\n",
      "EarlyStopping counter: 9 out of 20\n",
      "Epoch:  41  \tTraining Loss: 0.215021\n",
      "EarlyStopping counter: 10 out of 20\n",
      "Epoch:  42  \tTraining Loss: 0.214582\n",
      "EarlyStopping counter: 11 out of 20\n",
      "Epoch:  43  \tTraining Loss: 0.219152\n",
      "Epoch:  44  \tTraining Loss: 0.217716\n",
      "Epoch:  45  \tTraining Loss: 0.217321\n",
      "Epoch:  46  \tTraining Loss: 0.217005\n",
      "Epoch:  47  \tTraining Loss: 0.216731\n",
      "Epoch:  48  \tTraining Loss: 0.216482\n",
      "Epoch:  49  \tTraining Loss: 0.216251\n",
      "Epoch:  50  \tTraining Loss: 0.216035\n",
      "Epoch:  51  \tTraining Loss: 0.215829\n",
      "Epoch:  52  \tTraining Loss: 0.215633\n",
      "Epoch:  53  \tTraining Loss: 0.215445\n",
      "Epoch:  54  \tTraining Loss: 0.215264\n",
      "Epoch:  55  \tTraining Loss: 0.215090\n",
      "Epoch:  56  \tTraining Loss: 0.214922\n",
      "Epoch:  57  \tTraining Loss: 0.214760\n",
      "Epoch:  58  \tTraining Loss: 0.214603\n",
      "Epoch:  59  \tTraining Loss: 0.214452\n",
      "Epoch:  60  \tTraining Loss: 0.214305\n",
      "Epoch:  61  \tTraining Loss: 0.214162\n",
      "Epoch:  62  \tTraining Loss: 0.214024\n",
      "Epoch:  63  \tTraining Loss: 0.213890\n",
      "Epoch:  64  \tTraining Loss: 0.213760\n",
      "Epoch:  65  \tTraining Loss: 0.213633\n",
      "Epoch:  66  \tTraining Loss: 0.213510\n",
      "Epoch:  67  \tTraining Loss: 0.213390\n",
      "Epoch:  68  \tTraining Loss: 0.213274\n",
      "Epoch:  69  \tTraining Loss: 0.213160\n",
      "Epoch:  70  \tTraining Loss: 0.213050\n",
      "Epoch:  71  \tTraining Loss: 0.212942\n",
      "Epoch:  72  \tTraining Loss: 0.212837\n",
      "Epoch:  73  \tTraining Loss: 0.212735\n",
      "Epoch:  74  \tTraining Loss: 0.212635\n",
      "Epoch:  75  \tTraining Loss: 0.212537\n",
      "Epoch:  76  \tTraining Loss: 0.212442\n",
      "Epoch:  77  \tTraining Loss: 0.212350\n",
      "Epoch:  78  \tTraining Loss: 0.212259\n",
      "Epoch:  79  \tTraining Loss: 0.212171\n",
      "Epoch:  80  \tTraining Loss: 0.212085\n",
      "Epoch:  81  \tTraining Loss: 0.212001\n",
      "Epoch:  82  \tTraining Loss: 0.211919\n",
      "Epoch:  83  \tTraining Loss: 0.211838\n",
      "Epoch:  84  \tTraining Loss: 0.211760\n",
      "Epoch:  85  \tTraining Loss: 0.211683\n",
      "Epoch:  86  \tTraining Loss: 0.211608\n",
      "Epoch:  87  \tTraining Loss: 0.211535\n",
      "Epoch:  88  \tTraining Loss: 0.211464\n",
      "Epoch:  89  \tTraining Loss: 0.211394\n",
      "Epoch:  90  \tTraining Loss: 0.211325\n",
      "Epoch:  91  \tTraining Loss: 0.211258\n",
      "Epoch:  92  \tTraining Loss: 0.211193\n",
      "Epoch:  93  \tTraining Loss: 0.211129\n",
      "Epoch:  94  \tTraining Loss: 0.211066\n",
      "Epoch:  95  \tTraining Loss: 0.211004\n",
      "Epoch:  96  \tTraining Loss: 0.210944\n",
      "Epoch:  97  \tTraining Loss: 0.210885\n",
      "Epoch:  98  \tTraining Loss: 0.210827\n",
      "Epoch:  99  \tTraining Loss: 0.210771\n"
     ]
    }
   ],
   "source": [
    "for n_count in range(n_folds):\n",
    "    print('CV {}/{}'.format(counter, n_folds))\n",
    "    \n",
    "    indexes = np.arange(n_folds).astype(int)    \n",
    "    indexes = np.delete(indexes,obj=n_count, axis=0) \n",
    "    \n",
    "    indexes = np.r_[values[indexes[0]],values[indexes[1]],values[indexes[2]],values[indexes[3]]]\n",
    "    \n",
    "    X_train = train_nn.loc[train_nn.time_id.isin(indexes), features_to_consider]\n",
    "    y_train = train_nn.loc[train_nn.time_id.isin(indexes), target_name]\n",
    "    X_test = train_nn.loc[train_nn.time_id.isin(values[n_count]), features_to_consider]\n",
    "    y_test = train_nn.loc[train_nn.time_id.isin(values[n_count]), target_name]\n",
    "    \n",
    "    #############################################################################################\n",
    "    # NN\n",
    "    #############################################################################################\n",
    "    \n",
    "    try:\n",
    "        features_to_consider.remove('stock_id')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    num_data = X_train[features_to_consider]\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))         \n",
    "    num_data = scaler.fit_transform(num_data.values)    \n",
    "\n",
    "    cat_data = X_train['stock_id']    \n",
    "    target =  y_train\n",
    "\n",
    "    num_data_test = X_test[features_to_consider]\n",
    "    num_data_test = scaler.transform(num_data_test.values)\n",
    "    cat_data_test = X_test['stock_id']\n",
    "\n",
    "    \n",
    "    train_dataset = myDataset(cat_data,num_data, y_train)\n",
    "    test_dataset =  myDataset(cat_data_test, num_data_test, y_test)\n",
    "    \n",
    "    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "    n_epochs = 100\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = 512,\n",
    "                                           num_workers = 0)\n",
    "    test_loader  = torch.utils.data.DataLoader(test_dataset , batch_size = 512,\n",
    "                                           num_workers = 0)\n",
    "\n",
    "\n",
    "    stock_shape = len(cat_data)\n",
    "    num_input   = len(num_data)\n",
    "    n_f = len(features_to_consider)\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = NNet(n_f,n_cata, stock_embedding_size)\n",
    "    model.to(device)\n",
    "    criterion = My_loss().to(device)\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr = 0.006)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "\n",
    "    for epoch in range (n_epochs):\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for data,target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x_cat, x_num = data\n",
    "            x_cat = x_cat[0].cuda()\n",
    "            x_num = x_num.cuda()\n",
    "            target = target[0].cuda()\n",
    "            output = model(x_cat,x_num)\n",
    "            loss = criterion(output[:,0], target)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()*x_cat.size(0)\n",
    "        train_loss = train_loss / len(train_loader.dataset)\n",
    "        print('Epoch:  {}  \\tTraining Loss: {:.6f}'.format(epoch + 1, train_loss))\n",
    "        preds = model(torch.LongTensor(list(cat_data_test)).cuda(), torch.FloatTensor(num_data_test).cuda()\n",
    "                 ).reshape(1,-1)[0]\n",
    "        val_loss = criterion(preds, torch.Tensor(y_test.values).cuda())\n",
    "        scheduler.step(val_loss)\n",
    "        early_stopping(val_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    preds = model(torch.LongTensor(list(cat_data_test)).cuda(), torch.FloatTensor(num_data_test).cuda()\n",
    "                 ).reshape(1,-1)[0]\n",
    "    \n",
    "    score = round(rmspe(y_true = y_test.values, y_pred = preds.cpu().detach().numpy()),5)\n",
    "    print('Fold {} {}: {}'.format(counter, model_name, score))\n",
    "    scores_folds[model_name].append(score)\n",
    "\n",
    "    tt =scaler.transform(test_nn[features_to_consider].values)\n",
    "    #test_nn[target_name] += model.predict([test_nn['stock_id'], tt]).reshape(1,-1)[0].clip(0,1e10)\n",
    "\n",
    "    pred_1 = model(torch.LongTensor(list(test_nn['stock_id'])).cuda(), torch.FloatTensor(tt).cuda()\n",
    "                 ).reshape(1,-1)[0]\n",
    "\n",
    "    test_predictions_nn += pred_1.cpu().detach().numpy()/n_folds\n",
    "    #test[target_name] += model.predict([test['stock_id'], test[features_to_consider]]).reshape(1,-1)[0].clip(0,1e10)\n",
    "\n",
    "    counter += 1\n",
    "    features_to_consider.append('stock_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0144529 , 0.00183588, 0.00183588])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_p = pd.read_csv('input/optiver-realized-volatility-prediction/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Oct 13 01:33:31 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.80       Driver Version: 460.80       CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX 108...  Off  | 00000000:01:00.0 Off |                  N/A |\n",
      "| 23%   33C    P8    17W / 250W |   3159MiB / 11177MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_consider = list(train_nn)\n",
    "\n",
    "features_to_consider.remove('time_id')\n",
    "features_to_consider.remove('target')\n",
    "try:\n",
    "    features_to_consider.remove('pred_NN')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV 1/2\n",
      "Loading using 0.0s\n",
      "Epoch:  1  \tTraining Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RUISAV~1\\AppData\\Local\\Temp/ipykernel_6400/2378050135.py:104: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  dtime = np.array(dtime).mean()\n",
      "C:\\Users\\RUISAV~1\\AppData\\Local\\Temp/ipykernel_6400/2378050135.py:104: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  dtime = np.array(dtime).mean()\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (1024) must match the size of tensor b (900) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\RUISAV~1\\AppData\\Local\\Temp/ipykernel_6400/2378050135.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[0mt1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[1;32mdel\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m         \u001b[0mdtime\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtime\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m         \u001b[0mPtime\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPtime\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'This Epoch spend: '\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mepoch_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m's'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Users\\RuiSavior\\anaconda3\\envs\\optiver_kaggle\\lib\\site-packages\\numpy\\core\\_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m    177\u001b[0m             \u001b[0mis_float16_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m     \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         ret = um.true_divide(\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (1024) must match the size of tensor b (900) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "scores_folds = []\n",
    "n_folds = 2\n",
    "kfold = KFold(n_splits = 2, random_state = 2021, shuffle = True)\n",
    "for fold, (trn_ind, val_ind) in enumerate(kfold.split(train_nn)):\n",
    "    print('CV {}/{}'.format(fold+1, n_folds))\n",
    "    \n",
    "    \n",
    "    X_train = train_nn.iloc[trn_ind]\n",
    "    y_train = train_nn.iloc[trn_ind]\n",
    "    X_test = train_nn.iloc[val_ind]\n",
    "    y_test = train_nn.iloc[val_ind]\n",
    "    \n",
    "    X_train = train_nn.loc[:,features_to_consider]\n",
    "    y_train = train_nn.loc[:,target_name]\n",
    "    X_test = train_nn.loc[:,features_to_consider]\n",
    "    y_test = train_nn.loc[:,target_name]\n",
    "    \n",
    "    #############################################################################################\n",
    "    # NN\n",
    "    #############################################################################################\n",
    "    \n",
    "    try:\n",
    "        features_to_consider.remove('stock_id')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    num_data = X_train[features_to_consider]\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))         \n",
    "    num_data = scaler.fit_transform(num_data.values)    \n",
    "\n",
    "    cat_data = X_train['stock_id']    \n",
    "    target =  y_train\n",
    "\n",
    "    num_data_test = X_test[features_to_consider]\n",
    "    num_data_test = scaler.transform(num_data_test.values)\n",
    "    cat_data_test = X_test['stock_id']\n",
    "\n",
    "    \n",
    "    train_dataset = myDataset(cat_data,num_data, y_train)\n",
    "    test_dataset =  myDataset(cat_data_test, num_data_test, y_test)\n",
    "    \n",
    "    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "    n_epochs = 5\n",
    "    \n",
    "    loadtime = time.time()\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = 1024 * 1,\n",
    "                                           num_workers = 0)\n",
    "    test_loader  = torch.utils.data.DataLoader(test_dataset , batch_size = 1024 * 1,\n",
    "                                           num_workers = 0)\n",
    "    print(\"Loading using \"+str(time.time()-loadtime)+'s')\n",
    "\n",
    "    stock_shape = len(cat_data)\n",
    "    num_input   = len(num_data)\n",
    "    n_f = len(features_to_consider)\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = NNet(n_f,n_cata, stock_embedding_size)\n",
    "    model.to(device)\n",
    "    criterion = My_loss().to(device)\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr = 0.006)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "\n",
    "    for epoch in range (n_epochs):\n",
    "        train_loss = 0.0\n",
    "        epoch_time = time.time()\n",
    "        dtime = []\n",
    "        Ptime = []\n",
    "        counter = 0\n",
    "\n",
    "        for data,target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            x_cat, x_num = data\n",
    "            \n",
    "            \n",
    "            dtime.append(time.time()-datatime)\n",
    "            predtime=time.time()\n",
    "            \n",
    "            output = model(x_cat,x_num)\n",
    "            loss = criterion(output[:,0], target)\n",
    "            loss.backward()\n",
    "            Ptime.append(time.time()-predtime)\n",
    "\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()*x_cat.size(0)\n",
    "            \n",
    "            del x_cat\n",
    "            torch.cuda.empty_cache()\n",
    "            counter += 1\n",
    "        train_loss = train_loss / len(train_loader.dataset)\n",
    "        print('Epoch:  {}  \\tTraining Loss: {:.6f}'.format(epoch + 1, train_loss))\n",
    "        preds = model(torch.LongTensor(list(cat_data_test)).cuda(), torch.FloatTensor(num_data_test).cuda()\n",
    "                 ).reshape(1,-1)[0]\n",
    "        val_loss = criterion(preds, torch.Tensor(y_test.values).cuda())\n",
    "        scheduler.step(val_loss)\n",
    "        early_stopping(val_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        t1 = time.time()\n",
    "        del val_loss, preds, train_loss, data,target\n",
    "        dtime = np.array(dtime).mean()\n",
    "        Ptime = np.array(Ptime).mean()\n",
    "        print('This Epoch spend: '+str(time.time()-epoch_time)+'s')\n",
    "        print('dtime'+str(dtime)+'s',' Proportion:',dtime*counter/(time.time()-epoch_time))\n",
    "        print('Predtime'+str(Ptime)+'s',' Proportion:',Ptime*counter/(time.time()-epoch_time))\n",
    "        print('Del using'+str(t1-time.time())+'s')\n",
    "        \n",
    "    preds = model(torch.LongTensor(list(cat_data_test)).cuda(), torch.FloatTensor(num_data_test).cuda()\n",
    "                 ).reshape(1,-1)[0]\n",
    "    \n",
    "    score = round(rmspe(y_true = y_test.values, y_pred = preds.cpu().detach().numpy()),5)\n",
    "    print('Fold {} {}: {}'.format(fold, model_name, score))\n",
    "    scores_folds.append(score)\n",
    "    features_to_consider.append('stock_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09,\n",
       "        1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09, 1.6343e+09],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datatime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_folds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:optiver_kaggle]",
   "language": "python",
   "name": "conda-env-optiver_kaggle-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
