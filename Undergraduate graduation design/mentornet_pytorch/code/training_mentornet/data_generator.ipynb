{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Generates training data for learning/updating MentorNet.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = ''\n",
    "vstar_fn = ''\n",
    "sample_size = 100000\n",
    "tr_rate = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pretrain_defined(vstar_fn, outdir, sample_size):\n",
    "    \"\"\"Generates a trainable dataset given a vstar_fn.\n",
    "\n",
    "    Args:\n",
    "        vstar_fn: the name of the variable star function to use.\n",
    "        outdir: directory to save the training data.\n",
    "        sample_size: size of the sample.\n",
    "    \"\"\"\n",
    "    batch_l = np.concatenate((np.arange(0, 10, 0.1), np.arange(10, 30, 1)))\n",
    "    batch_diff = np.arange(-5, 5, 0.1)\n",
    "    batch_y = np.array([0])\n",
    "    batch_e = np.arange(0,100,1)\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for t in itertools.product(batch_l, batch_diff, batch_y, batch_e):\n",
    "        # 类似笛卡尔积，四个都遍历\n",
    "        data.append(t)\n",
    "    data = np.array(data)\n",
    "\n",
    "    v = vstar_fn(data)\n",
    "    v = v.reshape([-1, 1])\n",
    "    data = np.hstack((data, v))\n",
    "\n",
    "    perm = np.arange(data.shape[0])\n",
    "    np.random.shuffle(perm)\n",
    "    data = data[perm[0:min(sample_size, len(perm))], ]\n",
    "\n",
    "    tr_size = int(data.shape[0] * tr_rate)\n",
    "    tr = data[0:tr_size]\n",
    "    ts = data[(tr_size+1):data.shape[0]]\n",
    "\n",
    "    if not os.path.exists(outdir):\n",
    "        os.makedirs(outdir)\n",
    "\n",
    "    print('training_shape={} test_shape={}'.format(tr.shape, ts.shape))\n",
    "\n",
    "    with open(os.path.join(outdir, 'tr.p'), 'wb') as outfile:\n",
    "        pickle.dump(tr, outfile)\n",
    "\n",
    "    with open(os.path.join(outdir, 'ts.p'), 'wb') as outfile:\n",
    "        pickle.dump(ts, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1200000, 4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_data_driven(input_csv_filename,\n",
    "                         outdir,\n",
    "                         percentile_range='40,50,60,75,80,90'):\n",
    "    \"\"\"Generates a data-driven trainable dataset, given a CSV.\n",
    "\n",
    "  Refer to README.md for details on how to format the CSV.\n",
    "\n",
    "  Args:\n",
    "    input_csv_filename: the path of the CSV file. The csv file format\n",
    "      0: epoch_percentage\n",
    "      1: noisy label\n",
    "      2: clean label\n",
    "      3: loss\n",
    "    outdir: directory to save the training data.\n",
    "    percentile_range: the percentiles used to compute the moving average.\n",
    "    \"\"\"\n",
    "    raw = read_from_csv(input_csv_filename)\n",
    "\n",
    "    raw = np.array(raw.values())\n",
    "    dataset_name = os.path.splitext(os.path.basename(input_csv_filename))[0]\n",
    "\n",
    "    percentile_range = percentile_range.split(',')\n",
    "    percentile_range = [int(x) for x in percentile_range]\n",
    "\n",
    "    for percentile in percentile_range:\n",
    "        percentile = int(percentile)\n",
    "        p_perncentile = np.percentile(raw[:, 3], percentile)\n",
    "\n",
    "        v_star = np.float32(raw[:, 1] == raw[:, 2])\n",
    "\n",
    "        l = raw[:, 3]\n",
    "        diff = raw[:, 3] - p_perncentile\n",
    "        # label not used in the current version.\n",
    "        y = np.array([0] * len(v_star))\n",
    "        epoch_percentage = raw[:, 0]\n",
    "\n",
    "        data = np.vstack((l, diff, y, epoch_percentage, v_star))\n",
    "        data = np.transpose(data)\n",
    "\n",
    "        perm = np.arange(data.shape[0])\n",
    "        np.random.shuffle(perm)\n",
    "        data = data[perm,]\n",
    "\n",
    "        tr_size = int(data.shape[0] * 0.8)\n",
    "\n",
    "        tr = data[0:tr_size]\n",
    "        ts = data[(tr_size + 1):data.shape[0]]\n",
    "\n",
    "        cur_outdir = os.path.join(\n",
    "            outdir, '{}_percentile_{}'.format(dataset_name, percentile))\n",
    "        if not os.path.exists(cur_outdir):\n",
    "            os.makedirs(cur_outdir)\n",
    "\n",
    "        print('training_shape={} test_shape={}'.format(tr.shape, ts.shape))\n",
    "        print(cur_outdir)\n",
    "        with open(os.path.join(cur_outdir, 'tr.p'), 'wb') as outfile:\n",
    "            pickle.dump(tr, outfile)\n",
    "\n",
    "        with open(os.path.join(cur_outdir, 'ts.p'), 'wb') as outfile:\n",
    "            pickle.dump(ts, outfile)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d6b7a2cca450d9e5272a88beb247fd04f9ffc9667cef683458d0420589cf7e4e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
